{
  "0": {
    "pdf_path": "data/pdfs/machine learning_paper_66.pdf",
    "text_excerpt": "Machine Learning and Computational Mathematics\nWeinan Ei\nPrinceton University and Beijing Institute of Big Data Research\nIn memory of Professor Feng Kang (1920-1993)\nContents\n1 Introduction 2\n2 Machine learning-based algorithms for problems in computational sci-\nence 3\n2.1 Nonlinear multi-grid method and protein folding . . . . . . . . . . . . . . . 4\n2.2 Molecular dynamics with ab initio accuracy . . . . . . . . . . . . . . . . . 6\n3 Machine learning-based algorithms for high dimensional problems in\nscienti\fc computing 9\n3.1 Stochastic control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n3.2 Nonlinear parabolic PDEs . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3.3 Moment closure for kinetic equations modeling gas dynamics . . . . . . . . 13\n4 Mathematical theory of machine learning 15\n4.1 An introduction to neural network-based supervised learning . . . . . . . . 15\n4.2 Approximation theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n4.3 Estimation error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n4.4 A priori estimates for regularized models . . . . . . . . . . . . . . . . . . . 21\n5 Machine learning from a continuous viewpoint 22\n5.1 Representation of functions . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n5.2 The stochastic optimization problem . . . . . . . . . . . . . . . . . . . . . 23\n5.3 Optimization: Gradient \rows . . . . . . . . . . . . . . . . . . . . . . . . . 24\n5.4 Discretizing the gradient \rows . . . . . . . . . . . . . . . . . . . . . . . . . 24\n5.5 The optimal control problem for \row-based representation . . . . . . . . . 25\n6 Concluding remarks 28\nAbstract\nNeural network-based machine learning is capable of approximating functions in\nvery high dimension with unprecedented e\u000eciency and accuracy. This has opened up\nmany exciting new possibilities, not just in traditional areas of arti\fcial intelligence,\nbut also in scienti\fc computing and computational science. At the ",
    "title": "Machine Learning and Computational Mathematics",
    "abstract": "Neural network-based machine learning is capable of approximating functions\nin very high dimension with unprecedented efficiency and accuracy. This has\nopened up many exciting new possibilities, not just in traditional areas of\nartificial intelligence, but also in scientific computing and computational\nscience. At the same time, machine learning has also acquired the reputation of\nbeing a set of \"black box\" type of tricks, without fundamental principles. This\nhas been a real obstacle for making further progress in machine learning. In\nthis article, we try to address the following two very important questions: (1)\nHow machine learning has already impacted and will further impact computational\nmathematics, scientific computing and computational science? (2) How\ncomputational mathematics, particularly numerical analysis, {can} impact\nmachine learning? We describe some of the most important progress that has been\nmade on these issues. Our hope is to put things into a perspective that will\nhelp to integrate machine learning with computational mathematics.",
    "link": "http://arxiv.org/abs/2009.14596v1",
    "published": "2020-09-23T23:16:46Z"
  },
  "1": {
    "pdf_path": "data/pdfs/machine learning_paper_141.pdf",
    "text_excerpt": "Educational data mining: prediction \nof students’ academic performance using \nmachine learning algorithms\nMustafa Yağcı*  \nIntroduction\nThe application of data mining methods in the field of education has attracted great \nattention in recent years. Data Mining (DM) is the discovery of data. It is the field of \ndiscovering new and potentially useful information or meaningful results from big data \n(Witten et al., 2011). It also aims to obtain new trends and new patterns from large data -\nsets by using different classification algorithms (Baker & Inventado, 2014).\nEducational data mining (EDM) is the use of traditional DM methods to solve prob -\nlems related to education (Baker & Yacef, 2009; cited in Fernandes et al., 2019). EDM \nis the use of DM methods on educational data such as student information, edu -\ncational records, exam results, student participation in class, and the frequency of Abstract \nEducational data mining has become an effective tool for exploring the hidden rela-\ntionships in educational data and predicting students’ academic achievements. This \nstudy proposes a new model based on machine learning algorithms to predict the \nfinal exam grades of undergraduate students, taking their midterm exam grades as \nthe source data. The performances of the random forests, nearest neighbour, support \nvector machines, logistic regression, Naïve Bayes, and k-nearest neighbour algorithms, \nwhich are among the machine learning algorithms, were calculated and compared to \npredict the final exam grades of the students. The dataset consisted of the academic \nachievement grades of 1854 students who took the Turkish Language-I course in a \nstate University in Turkey during the fall semester of 2019–2020. The results show that \nthe proposed model achieved a classification accuracy of 70–75%. The predictions \nwere made using only three types of parameters; midterm exam grades, Depart -\nment data and Faculty data. Such data-driven studies are very important in terms of \n",
    "title": "Educational data mining: prediction of students' academic performance using machine learning algorithms",
    "abstract": "Educational data mining has become an effective tool for exploring the hidden relationships in educational data and predicting students' academic achievements. This study proposes a new model based on machine learning algorithms to predict the final exam grades of undergraduate students, taking their midterm exam grades as the source data. The performances of the random forests, nearest neighbour, support vector machines, logistic regression, Naïve Bayes, and k-nearest neighbour algorithms, which are among the machine learning algorithms, were calculated and compared to predict the final exam grades of the students. The dataset consisted of the academic achievement grades of 1854 students who took the Turkish Language-I course in a state University in Turkey during the fall semester of 2019–2020. The results show that the proposed model achieved a classification accuracy of 70–75%. The predictions were made using only three types of parameters; midterm exam grades, Department data and Faculty data. Such data-driven studies are very important in terms of establishing a learning analysis framework in higher education and contributing to the decision-making processes. Finally, this study presents a contribution to the early prediction of students at high risk of failure and determines the most effective machine learning methods.",
    "link": "https://www.semanticscholar.org/paper/0ad4189bdddfa32ecf7b1c9122eba57c8d8bbc7f",
    "published": "2022-03-03"
  },
  "2": {
    "pdf_path": "data/pdfs/machine learning_paper_611.pdf",
    "text_excerpt": "Machin e Learnin g 4, 41-65 , 1989\nc 1989 Kluwe r Academi c Publishers—Manufacture d in The Netherland s\nSemi-Supervise d Learnin g\nRAYMON D BOAR D (board@cs.uiuc.edu )\nLEONAR D PIT T (pitt@cs.uiuc.edu )\nDepartment  of Computer  Science,  University  of Illinois  at Urbana-Champaign,  1304  W. Springfield  Ave.,\nUrbana,  IL 61801 . USA\nEditor : Davi d Haussle r\nKeywords : concep t learning , classification , pac-learning , Boolea n formulas , polynomial-tim e\nidentification .\nAbstract . Th e distribution-independen t mode l of (supervised ) concep t learnin g due to Valian t (1984 )\nis extende d to that of semi-supervised  learning  (ss-learning) , in whic h a collectio n of disjoin t concept s\nis to be simultaneousl y learne d with only partia l informatio n concernin g concep t membershi p availabl e\nto the learnin g algorithm . It is show n that man y learnabl e concep t classe s are also ss-learnable . A new\ntechniqu e of learning , usin g an intermediate  oracle,  is introduced . Sufficien t condition s for a collectio n\nof concep t classe s to be ss-learnabl e are given .\n1. Introductio n\nTheoretica l result s in concep t learnin g have receive d muc h attentio n recentl y withi n\nthe mode l of learnabilit y introduce d by Valian t (1984) . This mode l (calle d pac-\nlearnability,  for \"probabl y approximatel y correc t learning \" (Angluin , 1988a) ) as-\nsume s that ther e is a teacher , or oracle , that present s the learne r with randoml y\ngenerate d example s (and counterexamples)  of the concep t to be learned . The effect s\nof providin g additiona l informatio n to the learner , such as querie s and hints , have\nalso been studie d (Valiant , 1984 ; Angluin , 1987 , 1988a , 1988b , 1988c , 1988d ; Ber-\nman & Roos , 1987) .\nIn this pape r we ask whethe r it is possibl e to learn with less information—withou t\na teache r labelin g example s of each concep t to be learne d as positiv e or negative .\nFurther , we conside r the proble m of simultaneousl y learni",
    "title": "Semi-Supervised Learning",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022653227824",
    "published": "2003-04-04T16:55:36Z"
  },
  "3": {
    "pdf_path": "data/pdfs/machine learning_paper_380.pdf",
    "text_excerpt": "Machine Learning 1: 243-248, 1986\n© 1986 Kluwer Academic Publishers, Boston - Manufactured in The Netherlands\nEditorial: Human and Machine Learning\nThe goals of machine learning\nOne can identify a number of different themes within the machine learning\ncommunity, each corresponding to central goals of its parent field, artificial\nintelligence. For instance, many AI researchers are concerned with implementing\nknowledge-intensive systems, but these often take man-years to construct. Machine\nlearning may provide methods for automating this process, promising consider-\nable savings in time and effort. Similarly, many AI researchers view artificial\nintelligence as a scientific discipline rather than an engineering one, and hope to\nformulate general principles of intelligent behavior that hold across a variety of\ndomains. Since machine learning focuses on the acquisition of domain-specific\nknowledge rather than the knowledge itself, it holds considerable potential for such\ngeneral principles.\nStill other AI researchers are concerned with explaining human behavior,\nviewing AI techniques as an ideal tool for stating complex theories of the human\ninformation processing system. Since learning is a central phenomenon in human\ncognition, these researchers evaluate machine learning methods in terms of their\nability to explain human learning. Although this is a minority opinion within\nmachine learning, below we present some reasons why more members of the field\nshould take this approach seriously.\nScience as search\nOne of the central insights of AI is that intelligence involves search, and that\neffective search is constrained by domain-specific knowledge. This framework can\nbe applied to problem solving, language understanding, and learning from\nexperience. One can even apply this search metaphor to machine learning as a field\nof scientific study. In this framework, machine learning researchers are exploring a\nvast space of possible learning methods, searching for techniques with ",
    "title": "Editorial: Human and Machine Learning",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022854429410",
    "published": "2003-04-04T16:57:10Z"
  },
  "4": {
    "pdf_path": "data/pdfs/machine learning_paper_61.pdf",
    "text_excerpt": "DriveML: An R Package for Driverless Machine\nLearning\nSayan Putatunda *\nSenior Manager- AA & DS\nEnterprise Data and Analytics (EDA)\nVMware Software India Pvt. Ltd.\nBangalore, India\nsayanp@iima.ac.inDayananda Ubrangala\nSenior Business Analyst- AA & DS\nEnterprise Data and Analytics (EDA)\nVMware Software India Pvt. Ltd.\nBangalore, India\ndaya6489@gmail.com\nKiran R\nSenior Director- AA & DS\nEnterprise Data and Analytics (EDA)\nVMware Software India Pvt. Ltd.\nBangalore, India\nefpm04013@iiml.ac.inRavi Prasad Kondapalli\nSenior Manager- AA & DS\nEnterprise Data and Analytics (EDA)\nVMware Software India Pvt. Ltd.\nBangalore, India\nrpkondapalli@yahoo.com\nAbstract\nIn recent years, the concept of automated machine learn-\ning has become very popular. Automated Machine Learn-\ning (AutoML) mainly refers to the automated methods for\nmodel selection and hyper-parameter optimization of vari-\nous algorithms such as random forests, gradient boosting,\nneural networks, etc. In this paper, we introduce a new pack-\nage i.e. DriveML for automated machine learning. DriveML\nhelps in implementing some of the pillars of an automated\nmachine learning pipeline such as automated data prepara-\ntion, feature engineering, model building and model expla-\nnation by running the function instead of writing lengthy\nR codes. The DriveML package is available in CRAN. We\ncompare the DriveML package with other relevant packages\nin CRAN/Github by applying them on multiple datasets of\ndifferent dimensions. We find that DriveML performs the best\ntaking into consideration both the prediction accuracy and the\nexecution time. We also provide an illustration by applying\nthe DriveML package with default configuration on a real\nworld dataset. Overall, the main benefits of DriveML are in\ndevelopment time savings, reduce developer’s errors, optimal\ntuning of machine learning models and reproducibility.\nCCS Concepts: •Computing methodologies →Supervised\nlearning by classification .\nKeywords: AutoML, Feature Engineering, Machi",
    "title": "DriveML: An R Package for Driverless Machine Learning",
    "abstract": "In recent years, the concept of automated machine learning has become very\npopular. Automated Machine Learning (AutoML) mainly refers to the automated\nmethods for model selection and hyper-parameter optimization of various\nalgorithms such as random forests, gradient boosting, neural networks, etc. In\nthis paper, we introduce a new package i.e. DriveML for automated machine\nlearning. DriveML helps in implementing some of the pillars of an automated\nmachine learning pipeline such as automated data preparation, feature\nengineering, model building and model explanation by running the function\ninstead of writing lengthy R codes. The DriveML package is available in CRAN.\nWe compare the DriveML package with other relevant packages in CRAN/Github and\nfind that DriveML performs the best across different parameters. We also\nprovide an illustration by applying the DriveML package with default\nconfiguration on a real world dataset. Overall, the main benefits of DriveML\nare in development time savings, reduce developer's errors, optimal tuning of\nmachine learning models and reproducibility.",
    "link": "http://arxiv.org/abs/2005.00478v3",
    "published": "2020-05-01T16:40:25Z"
  },
  "5": {
    "pdf_path": "data/pdfs/machine learning_paper_582.pdf",
    "text_excerpt": "Machine Learning, 5, 239-266 (1990)\n© 1990 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\nLearning Logical Definitions from Relations\nJ.R. QUINLAN\nBasser Department of Computer Science, University of Sydney, Sydney NSW Australia 2006\nEditor: Jack Mostow\nAbstract. This paper describes FOIL, a system that learns Horn clauses from data expressed as relations. FOIL\nis based on ideas that have proved effective in attribute-value learning systems, but extends them to a first-order\nformalism. This new system has been applied successfully to several tasks taken from the machine learning literature.\nKeywords. Induction, first-order rules, relational data, empirical learning\n1. Introduction\nConcept learning, which Hunt, Marin, and Stone (1966) describe succinctly as \"[the] capac-\nity to develop classification rules from experience,\" has long been a principal area of machine\nlearning research. Supervised concept learning systems are supplied with information about\nseveral entities whose class membership is known and produce from this a characterization\nof each class.\nOne major dimension along which to differentiate concept learning systems is the com-\nplexity of the input and output languages that they employ. At one extreme are learning\nsystems that use a prepositional attribute-value language for describing entities and classifica-\ntion rules. The simplicity of this formalism allows such systems to deal with large volumes\nof data and thus to exploit statistical properties of collections of examples and counter-\nexamples of a concept. At the other end of the spectrum, logical inference systems accept\ndescriptions of complex, structured entities and generate classification rules expressed in\nfirst-order logic. These typically have access to background knowledge pertinent to the\ndomain and so require fewer entity descriptions. FOIL, the system described in this paper,\nbuilds on ideas from both groups. Objects are described using relations and from these\nFO",
    "title": "Learning Logical Definitions from Relations",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022699322624",
    "published": "2003-04-04T16:55:36Z"
  },
  "6": {
    "pdf_path": "data/pdfs/machine learning_paper_456.pdf",
    "text_excerpt": "Machin e Learning , 4, 251-25 4 (1989 )\n© 1989 Kluwe r Academi c Publishers , Boston . Manufacture d in The Netherlands .\nCan Machin e Learnin g Offe r Anythin g\nto Exper t Systems ?\nBRUC E G. BUCHANA N\nProfessor  of Computer  Science,  Medicine,  and Philosophy,  University  of Pittsburgh,  Pittsburgh,  PA 15260\nToday' s exper t system s have no abilit y to learn from experience . This commonl y hear d crit-\nicism , unfortunately , is largel y true. Excep t for simpl e classificatio n systems , exper t system s\ndo not emplo y a learnin g componen t to construc t part s of their knowledg e base s from\nlibrarie s of previousl y solve d cases . And none that I know of couple s learnin g into closed -\nloop modificatio n base d on experience , althoug h the SOA R architectur e [Rosenbloo m and\nNewel l 1985 ] come s the closes t to bein g the sort of integrate d syste m neede d for continuou s\nlearning . Learnin g capabilitie s are neede d for intelligen t system s that can remai n usefu l\nin the face of changin g environment s or changin g standard s of expertise . Why are the learn -\ning method s we know how to implemen t not bein g used to build or maintai n exper t system s\nin the commercia l world ?\nPart of the answe r lies in the syntacti c view we have traditionall y take n towar d learning .\nStatistica l technique s such as linea r regression , for example , are \"knowledge-poor \" proce -\ndures that are unabl e to use knowledg e we may bring to the learnin g task. However , learnin g\nis a problem-solvin g activity . As such , we can and shoul d analyz e the requirement s and\nfunctiona l specifications , the inpu t and output , and the assumption s and limitations . If there\nis one thin g we have learne d in AI it is that complex , i.e., combinatoriall y explosiv e and\nill-structured , problem s ofte n can be tame d by introducin g knowledg e into an otherwis e\nsyntacti c procedure .\nThere is not a singl e mode l for learnin g effectively , as is confirme d by the ",
    "title": "Can Machine Learning Offer Anything to Expert Systems?",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022646520981",
    "published": "2003-04-04T16:55:36Z"
  },
  "7": {
    "pdf_path": "data/pdfs/machine learning_paper_39.pdf",
    "text_excerpt": "Springer Nature 2021 L ATEX template\nA Review on Machine Unlearning\nHaibo ZHANG1*, Toru NAKAMURA2, Takamasa\nISOHARA2and Kouichi SAKURAI3\n1*Department of Information Science and Technology, Graduate\nSchool of Information Science and Electrical Engineering, Kyushu\nUniversity, Japan, 819-0395.\n2KDDI Research Inc., Japan, 356-8502, .\n3Department of Information Science and Technology, Faculty of\nInformation Science and Electrical Engineering, Kyushu\nUniversity, Japan, 819-0395.\n*Corresponding author(s). E-mail(s):\nzhang.haibo892@s.kyushu-u.ac.jp;\nContributing authors: tr-nakamura@kddi-research.jp;\nta-isohara@kddi-research.jp; sakurai@inf.kyushu-u.ac.jp;\nAbstract\nRecently, an increasing number of laws have governed the useability of\nusers’ privacy. For example, Article 17 of the General Data Protection\nRegulation (GDPR), the right to be forgotten , requires machine learning\napplications to remove a portion of data from a dataset and retrain it\nif the user makes such a request. Furthermore, from the security per-\nspective, training data for machine learning models, i.e., data that may\ncontain user privacy, should be effectively protected, including appro-\npriate erasure. Therefore, researchers propose various privacy-preserving\nmethods to deal with such issues as machine unlearning. This paper\nprovides an in-depth review of the security and privacy concerns in\nmachine learning models. First, we present how machine learning can\nuse users’ private data in daily life and the role that the GDPR plays\nin this problem. Then, we introduce the concept of machine unlearn-\ning by describing the security threats in machine learning models and\nhow to protect users’ privacy from being violated using machine learning\nplatforms. As the core content of the paper, we introduce and ana-\nlyze current machine unlearning approaches and several representative\n1arXiv:2411.11315v1  [cs.LG]  18 Nov 2024Springer Nature 2021 L ATEX template\n2 A Review on Machine Unlearning\nresearch results and discu",
    "title": "A Review on Machine Unlearning",
    "abstract": "Recently, an increasing number of laws have governed the useability of users'\nprivacy. For example, Article 17 of the General Data Protection Regulation\n(GDPR), the right to be forgotten, requires machine learning applications to\nremove a portion of data from a dataset and retrain it if the user makes such a\nrequest. Furthermore, from the security perspective, training data for machine\nlearning models, i.e., data that may contain user privacy, should be\neffectively protected, including appropriate erasure. Therefore, researchers\npropose various privacy-preserving methods to deal with such issues as machine\nunlearning. This paper provides an in-depth review of the security and privacy\nconcerns in machine learning models. First, we present how machine learning can\nuse users' private data in daily life and the role that the GDPR plays in this\nproblem. Then, we introduce the concept of machine unlearning by describing the\nsecurity threats in machine learning models and how to protect users' privacy\nfrom being violated using machine learning platforms. As the core content of\nthe paper, we introduce and analyze current machine unlearning approaches and\nseveral representative research results and discuss them in the context of the\ndata lineage. Furthermore, we also discuss the future research challenges in\nthis field.",
    "link": "http://arxiv.org/abs/2411.11315v1",
    "published": "2024-11-18T06:18:13Z"
  },
  "8": {
    "pdf_path": "data/pdfs/machine learning_paper_118.pdf",
    "text_excerpt": "arXiv:cs/0110053v1  [cs.IR]  26 Oct 2001Machine Learning in Automated Text Categorization\nFabrizio Sebastiani\nConsiglio Nazionale delle Ricerche, Italy\nThe automated categorization (or classiﬁcation) of texts i nto predeﬁned categories has witnessed a\nbooming interest in the last ten years, due to the increased a vailability of documents in digital form\nand the ensuing need to organize them. In the research commun ity the dominant approach to this\nproblem is based on machine learning techniques: a general i nductive process automatically builds\na classiﬁer by learning, from a set of preclassiﬁed document s, the characteristics of the categories.\nThe advantages of this approach over the knowledge engineer ing approach (consisting in the\nmanual deﬁnition of a classiﬁer by domain experts) are a very good eﬀectiveness, considerable\nsavings in terms of expert manpower, and straightforward po rtability to diﬀerent domains. This\nsurvey discusses the main approaches to text categorizatio n that fall within the machine learning\nparadigm. We will discuss in detail issues pertaining to thr ee diﬀerent problems, namely document\nrepresentation, classiﬁer construction, and classiﬁer ev aluation.\nCategories and Subject Descriptors: H.3.1 [ Information storage and retrieval ]: Content anal-\nysis and indexing— Indexing methods ; H.3.3 [ Information storage and retrieval ]: Informa-\ntion search and retrieval— Information ﬁltering ; H.3.3 [ Information storage and retrieval ]:\nSystems and software— Performance evaluation (eﬃciency and eﬀectiveness) ; I.2.3 [ Artiﬁcial\nIntelligence ]: Learning— Induction\nGeneral Terms: Algorithms, Experimentation, Theory\nAdditional Key Words and Phrases: Machine learning, text ca tegorization, text classiﬁcation\n1. INTRODUCTION\nIn the last ten years content-based document management tas ks (collectively known\nasinformation retrieval – IR) have gained a prominent status in the information\nsystems ﬁeld, due to the increased availability of document s in ",
    "title": "Machine learning in automated text categorization",
    "abstract": "The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.",
    "link": "https://www.semanticscholar.org/paper/6b20af22b0734757d9ead382b201a65f9dd637cc",
    "published": "2001-10-26"
  },
  "9": {
    "pdf_path": "data/pdfs/machine learning_paper_570.pdf",
    "text_excerpt": "Machin e Learnin g 3: 79-92 , 1988\n© 1988 Kluwe r Academi c Publishers , Bosto n - Manufacture d in The Netherland s\nA Revie w of Machin e Learnin g at AAAI-8 7\nRUSSEL L GREINE R (GREINER@AI.TORONTO.EDU )\nDepartment  of Computer  Science,  University  of Toronto,\nToronto,  Ontario  M5S  1A4,  CANADA\nBERNAR D SILVE R (SILVER@AI.AI.MIT.EDU )\nComputer  and Intelligent  Systems  Laboratory,  GTE  Laboratories  Incorporated,\n40 Sylvan  Road,  Waltham  MA 02254,  V.S.A\nSUE BECKE R (BECKER@AI.TORONTO.EDU )\nMICHAE L GRUNINGE R (GRUNINGER@AI.TORONTO.EDU )\nDepartment  of Computer  Science,  University  of Toronto,\nToronto,  Ontario  M5S  1A4,  CANADA\n1. Introductio n\nSome of us can remembe r the first AAA I conferenc e in 1980 - a cozy gath -\nering of 400 AI researcher s tucke d awa y in one corne r of a universit y campus .\nTher e wer e only two paralle l session s of papers , and they fille d one relativel y\nthin proceeding s volume . Ther e wer e no tutoria l session s and no exhibitio n\nhall. For bette r or worse , the field of artificia l intelligenc e has grow n consider -\nably in the subsequen t seve n years . The recen t Sixt h Nationa l Conferenc e on\nArtificia l Intelligence , AAAI-87 , involve d over 6,70 0 peopl e and require d the\nfull accommodation s of the Seattl e Cente r in Seattle , Washingto n (site of a\nforme r world' s fair) for its four paralle l technica l session s and exhibitio n show ;\nand it still neede d the Universit y of Washingto n campu s for its four paralle l\ntutoria l sessions .\nMachin e learnin g (ML ) has also develope d over thes e years . It has recentl y\nemerge d as the subfiel d of AI that deal s with technique s for improvin g the\nperformanc e of a computationa l system . It is now distinguishe d from studie s\nof huma n learnin g and from specifi c knowledg e acquisitio n (KA ) tools . In\nadditio n to severa l ML pape r sessions , ther e are now tutorial s on the topi c\nand even ML program s and book s on displa y in the trad e show .",
    "title": "A Review of Machine Learning at AAAI-87",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022637632387",
    "published": "2003-04-04T16:55:36Z"
  },
  "10": {
    "pdf_path": "data/pdfs/machine learning_paper_551.pdf",
    "text_excerpt": "P1: KCU\nMachine Learning KL634-01-Erratum July 7, 1998 11:20\nMachine Learning 32, 77 (1998)\nc°1998 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\nErratum\nAnerrorappearedonpage256ofthepaperbyPaulFinnetal.,“PharmacophoreDiscovery\nUsing the Inductive Logic Programming System PROGOL,” which appeared in volume30,numbers2/3of MachineLearning . Thepublisherregretsthatintheuppermoleculeof\nFigure 4 the letter C was omitted. The correct version of Figure 4 appears below, alongwith its caption.\nFigure4. ACEinhibitornumbers1(top)and10withhighlighted4-pointpharmacophore. Molecularstructures\nhave been simpliﬁed by the removal of hydrogens.",
    "title": "Erratum",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1017155901277",
    "published": "2002-12-29T15:19:29Z"
  },
  "11": {
    "pdf_path": "data/pdfs/machine learning_paper_177.pdf",
    "text_excerpt": "PERSPECTIVE\nPerspectives in machine learning for wildlife\nconservation\nDevis Tuia1,17✉, Benjamin Kellenberger1,17, Sara Beery2,17,\nBlair R. Costelloe3,4,5,17, Silvia Zuf ﬁ6, Benjamin Risse7,\nAlexander Mathis8, Mackenzie W. Mathis8, Frank van Langevelde9,\nTilo Burghardt10, Roland Kays11,12, Holger Klinck13, Martin Wikelski3,4,\nIain D. Couzin3,4,5, Grant van Horn13, Margaret C. Crofoot3,4,5,\nCharles V. Stewart14& Tanya Berger-Wolf15,16\nInexpensive and accessible sensors are accelerating data acquisition in animal ecology. These\ntechnologies hold great potential for large-scale ecological understanding, but are limited bycurrent processing approaches which inef ﬁciently distill data into relevant information. We\nargue that animal ecologists can capitalize on large datasets generated by modern sensors bycombining machine learning approaches with domain knowledge. Incorporating machinelearning into ecological work ﬂows could improve inputs for ecological models and lead to\nintegrated hybrid modeling tools. This approach will require close interdisciplinary colla-boration to ensure the quality of novel approaches and train a new generation of datascientists in ecology and conservation.\nAnimal diversity is declining at an unprecedented rate1. This loss comprises not only\ngenetic, but also ecological and behavioral diversity, and is currently not well understood:\nout of more than 120,000 species monitored by the IUCN Red List of Threatened Species,\nup to 17,000 have a ‘Data de ﬁcient ’status2. We urgently need tools for rapid assessment of\nwildlife diversity and population dynamics at large scale and high spatiotemporal resolution,\nfrom individual animals to global densities. In this Perspective, we aim to build bridges across\necology and machine learning to highlight how relevant advances in technology can be leveraged\nto rise to this urgent challenge in animal conservation.https://doi.org/10.1038/s41467-022-27980-y OPEN\n1School of Architecture, Civil and Environmental Eng",
    "title": "Perspectives in machine learning for wildlife conservation",
    "abstract": "Inexpensive and accessible sensors are accelerating data acquisition in animal ecology. These technologies hold great potential for large-scale ecological understanding, but are limited by current processing approaches which inefficiently distill data into relevant information. We argue that animal ecologists can capitalize on large datasets generated by modern sensors by combining machine learning approaches with domain knowledge. Incorporating machine learning into ecological workflows could improve inputs for ecological models and lead to integrated hybrid modeling tools. This approach will require close interdisciplinary collaboration to ensure the quality of novel approaches and train a new generation of data scientists in ecology and conservation. Animal ecologists are increasingly limited by constraints in data processing. Here, Tuia and colleagues discuss how collaboration between ecologists and data scientists can harness machine learning to capitalize on the data generated from technological advances and lead to novel modeling approaches.",
    "link": "https://www.semanticscholar.org/paper/d9b34c6b616f75485856794478bfbeab1ea93b81",
    "published": "2021-10-25"
  },
  "12": {
    "pdf_path": "data/pdfs/machine learning_paper_317.pdf",
    "text_excerpt": "arXiv:1906.06821v2  [cs.LG]  23 Oct 20191\nA Survey of Optimization Methods from\na Machine Learning Perspective\nShiliang Sun, Zehui Cao, Han Zhu, and Jing Zhao\nAbstract —Machine learning develops rapidly, which has made\nmany theoretical breakthroughs and is widely applied in var ious\nﬁelds. Optimization, as an important part of machine learni ng,\nhas attracted much attention of researchers. With the expon ential\ngrowth of data amount and the increase of model complexity,\noptimization methods in machine learning face more and more\nchallenges. A lot of work on solving optimization problems o r\nimproving optimization methods in machine learning has bee n\nproposed successively. The systematic retrospect and summ ary\nof the optimization methods from the perspective of machine\nlearning are of great signiﬁcance, which can offer guidance\nfor both developments of optimization and machine learning\nresearch. In this paper, we ﬁrst describe the optimization\nproblems in machine learning. Then, we introduce the princi ples\nand progresses of commonly used optimization methods. Next ,\nwe summarize the applications and developments of optimiza tion\nmethods in some popular machine learning ﬁelds. Finally, we\nexplore and give some challenges and open problems for the\noptimization in machine learning.\nIndex Terms —Machine learning, optimization method, deep\nneural network, reinforcement learning, approximate Baye sian\ninference.\nI. I NTRODUCTION\nRECENTLY, machine learning has grown at a remarkable\nrate, attracting a great number of researchers and\npractitioners. It has become one of the most popular researc h\ndirections and plays a signiﬁcant role in many ﬁelds, such\nas machine translation, speech recognition, image recogni tion,\nrecommendation system, etc. Optimization is one of the core\ncomponents of machine learning. The essence of most machine\nlearning algorithms is to build an optimization model and le arn\nthe parameters in the objective function from the given data .\nIn the era of",
    "title": "A Survey of Optimization Methods From a Machine Learning Perspective",
    "abstract": "Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this article, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Finally, we explore and give some challenges and open problems for the optimization in machine learning.",
    "link": "https://www.semanticscholar.org/paper/3119ea9c7ad7a5e044dc7c267329a4bbf00d0158",
    "published": "2019-06-17"
  },
  "13": {
    "pdf_path": "data/pdfs/machine learning_paper_158.pdf",
    "text_excerpt": "ARTICLE\nPower of data in quantum machine learning\nHsin-Yuan Huang1,2,3, Michael Broughton1, Masoud Mohseni1, Ryan Babbush1, Sergio Boixo1,\nHartmut Neven1& Jarrod R. McClean1✉\nThe use of quantum computing for machine learning is among the most exciting prospective\napplications of quantum technologies. However, machine learning tasks where data is pro-vided can be considerably different than commonly studied computational tasks. In this work,we show that some problems that are classically hard to compute can be easily predicted byclassical machines learning from data. Using rigorous prediction error bounds as a founda-tion, we develop a methodology for assessing potential quantum advantage in learning tasks.The bounds are tight asymptotically and empirically predictive for a wide range of learningmodels. These constructions explain numerical results showing that with the help of data,classical machine learning models can be competitive with quantum models even if they aretailored to quantum problems. We then propose a projected quantum model that provides asimple and rigorous quantum speed-up for a learning problem in the fault-tolerant regime. Fornear-term implementations, we demonstrate a signi ﬁcant prediction advantage over some\nclassical models on engineered data sets designed to demonstrate a maximal quantum\nadvantage in one of the largest numerical tests for gate-based quantum machine learning todate, up to 30 qubits.https://doi.org/10.1038/s41467-021-22539-9 OPEN\n1Google Quantum AI, Venice, CA, USA.2Institute for Quantum Information and Matter, Caltech, Pasadena, CA, USA.3Department of Computing and\nMathematical Sciences, Caltech, Pasadena, CA, USA.✉email: jmcclean@google.com\nNATURE COMMUNICATIONS |         (2021) 12:2631 | https://doi.org/10.1038/s41467-021-22539-9 | www.nature.com/naturecommunications 11234567890():,;As quantum technologies continue to rapidly advance, it\nbecomes increasingly important to understand which\napplications can bene ﬁt from the po",
    "title": "Power of data in quantum machine learning",
    "abstract": "The use of quantum computing for machine learning is among the most exciting prospective applications of quantum technologies. However, machine learning tasks where data is provided can be considerably different than commonly studied computational tasks. In this work, we show that some problems that are classically hard to compute can be easily predicted by classical machines learning from data. Using rigorous prediction error bounds as a foundation, we develop a methodology for assessing potential quantum advantage in learning tasks. The bounds are tight asymptotically and empirically predictive for a wide range of learning models. These constructions explain numerical results showing that with the help of data, classical machine learning models can be competitive with quantum models even if they are tailored to quantum problems. We then propose a projected quantum model that provides a simple and rigorous quantum speed-up for a learning problem in the fault-tolerant regime. For near-term implementations, we demonstrate a significant prediction advantage over some classical models on engineered data sets designed to demonstrate a maximal quantum advantage in one of the largest numerical tests for gate-based quantum machine learning to date, up to 30 qubits. Expectations for quantum machine learning are high, but there is currently a lack of rigorous results on which scenarios would actually exhibit a quantum advantage. Here, the authors show how to tell, for a given dataset, whether a quantum model would give any prediction advantage over a classical one.",
    "link": "https://www.semanticscholar.org/paper/57e6cca1479a4642f867e69b4dee93d14259dc3d",
    "published": "2020-11-03"
  },
  "14": {
    "pdf_path": "data/pdfs/machine learning_paper_154.pdf",
    "text_excerpt": "Predicting the Future — Big Data, Machine Learning, and \nClinical Medicine\nZiad Obermeyer, M.D., M.Phil.  and\nDepartment of Emergency Medicine, Harvard Medical School and Brigham and Women’s \nHospital, and the Department of Health Care Policy, Harvard Medical School, Boston\nEzekiel J. Emanuel, M.D., Ph.D.\nDepartment of Medical Ethics and Health Policy, Perelman School of Medicine, and the \nDepartment of Health Care Management, the Wharton School, University of Pennsylvania, \nPhiladelphia\nBy now, it’s almost old news: big data will transform medicine. It’s essential to remember, \nhowever, that data by themselves are useless. To be useful, data must be analyzed, \ninterpreted, and acted on. Thus it is algorithms — not data sets — that will prove \ntransformative. We believe attention therefore has to shift to new statistical tools from the \nfield of machine learning that will be critical for anyone practicing medicine in the 21st \ncentury.\nFirst, it’s important to understand what machine learning is not. Most computer-based \nalgorithms in medicine are “expert systems” — rule sets encoding knowledge on a given \ntopic, which are applied to draw conclusions about specific clinical scenarios, such as \ndetecting drug interactions or judging the appropriateness of obtaining radiologic imaging. \nExpert systems work the way an ideal medical student would: they take general principles \nabout medicine and apply them to new patients.\nMachine learning, conversely, approaches problems as a doctor progressing through \nresidency might: by learning rules from data. Starting with patient-level observations, \nalgorithms sift through vast numbers of variables, looking for combinations that reliably \npredict outcomes. In one sense, this process is similar to that of traditional regression \nmodels: there is an outcome, covariates, and a statistical function linking the two. But where \nmachine learning shines is in handling enormous numbers of predictors — sometimes, \nremarkably, more predic",
    "title": "Predicting the Future - Big Data, Machine Learning, and Clinical Medicine.",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/4d1fdd81f033cd58f3723bfc61e7d12079647a7a",
    "published": "2016-09-28"
  },
  "15": {
    "pdf_path": "data/pdfs/machine learning_paper_295.pdf",
    "text_excerpt": "Sidey-GibbonsandSidey-Gibbons BMCMedicalResearchMethodology           (2019) 19:64 \nhttps://doi.org/10.1186/s12874-019-0681-4\nRESEARCH ARTICLE OpenAccess\nMachinelearninginmedicine:a\npracticalintroduction\nJenniA.M.Sidey-Gibbons1andChrisJ.Sidey-Gibbons2,3,4*\nAbstract\nBackground: Following visiblesuccessesonawiderangeofpredictivetasks,machinelearningtechniquesare\nattractingsubstantialinterestfrommedicalresearchersandclinicians.Weaddresstheneedforcapacitydevelopment\ninthisareabyprovidingaconceptualintroductiontomachinelearningalongsideapracticalguidetodevelopingand\nevaluatingpredictivealgorithmsusingfreely-availableopensourcesoftwareandpublicdomaindata.\nMethods: Wedemonstratetheuseofmachinelearningtechniquesbydevelopingthreepredictivemodelsfor\ncancerdiagnosisusingdescriptionsofnucleisampledfrombreastmasses.Thesealgorithmsincluderegularized\nGeneralLinearModelregression(GLMs),SupportVectorMachines(SVMs)witharadialbasisfunctionkernel,and\nsingle-layerArtificialNeuralNetworks.Thepublicly-availabledatasetdescribingthebreastmasssamples (N=683)\nwasrandomlysplitintoevaluation (n=456)andvalidation (n=227)samples.\nWetrainedalgorithmsondatafromtheevaluationsamplebeforetheywereusedtopredictthediagnosticoutcome\ninthevalidationdataset.Wecomparedthepredictionsmadeonthevalidationdatasetswiththereal-world\ndiagnosticdecisionstocalculatetheaccuracy,sensitivity,andspecificityofthethreemodels.Weexploredtheuseof\naveragingandvotingensemblestoimprovepredictiveperformance.Weprovideastep-by-stepguidetodeveloping\nalgorithmsusingtheopen-source Rstatisticalprogrammingenvironment.\nResults: Thetrainedalgorithmswereabletoclassifycellnucleiwithhighaccuracy (.94-.96 ),sensitivity (.97-.99 ),\nandspecificity (.85-.94 ).Maximumaccuracy (.96)andareaunderthecurve (.97)wasachievedusingtheSVM\nalgorithm.Predictionperformanceincreasedmarginally(accuracy =.97,sensitivity =.99,specificity =.95)when\nalgorithmswerearrangedintoavotingensemble.\nConclusions: Weuseastraightforwardexampletodemonstratethetheoryandpracticeo",
    "title": "Machine learning in medicine: a practical introduction",
    "abstract": "BackgroundFollowing visible successes on a wide range of predictive tasks, machine learning techniques are attracting substantial interest from medical researchers and clinicians. We address the need for capacity development in this area by providing a conceptual introduction to machine learning alongside a practical guide to developing and evaluating predictive algorithms using freely-available open source software and public domain data.MethodsWe demonstrate the use of machine learning techniques by developing three predictive models for cancer diagnosis using descriptions of nuclei sampled from breast masses. These algorithms include regularized General Linear Model regression (GLMs), Support Vector Machines (SVMs) with a radial basis function kernel, and single-layer Artificial Neural Networks. The publicly-available dataset describing the breast mass samples (N=683) was randomly split into evaluation (n=456) and validation (n=227) samples.We trained algorithms on data from the evaluation sample before they were used to predict the diagnostic outcome in the validation dataset. We compared the predictions made on the validation datasets with the real-world diagnostic decisions to calculate the accuracy, sensitivity, and specificity of the three models. We explored the use of averaging and voting ensembles to improve predictive performance. We provide a step-by-step guide to developing algorithms using the open-source R statistical programming environment.ResultsThe trained algorithms were able to classify cell nuclei with high accuracy (.94 -.96), sensitivity (.97 -.99), and specificity (.85 -.94). Maximum accuracy (.96) and area under the curve (.97) was achieved using the SVM algorithm. Prediction performance increased marginally (accuracy =.97, sensitivity =.99, specificity =.95) when algorithms were arranged into a voting ensemble.ConclusionsWe use a straightforward example to demonstrate the theory and practice of machine learning for clinicians and medical researchers. The principals which we demonstrate here can be readily applied to other complex tasks including natural language processing and image recognition.",
    "link": "https://www.semanticscholar.org/paper/5d093bd376ba63495ea442241bc8bc2f0ff30c2b",
    "published": "2019-03-19"
  },
  "16": {
    "pdf_path": "data/pdfs/machine learning_paper_43.pdf",
    "text_excerpt": "Theoretical Robopsychology: Samu Has Learned\nTuring Machines\nNorbert B\u0013 atfai\nbatfai.norbert@inf.unideb.hu\nDepartment of Information Technology\nUniversity of Debrecen\nOctober 25, 2018\nAbstract\nFrom the point of view of a programmer, the robopsychology is a syn-\nonym for the activity is done by developers to implement their machine\nlearning applications. This robopsychological approach raises some fun-\ndamental theoretical questions of machine learning. Our discussion of\nthese questions is constrained to Turing machines. Alan Turing had given\nan algorithm (aka the Turing Machine) to describe algorithms. If it has\nbeen applied to describe itself then this brings us to Turing's notion of\nthe universal machine. In the present paper, we investigate algorithms\nto write algorithms. From a pedagogy point of view, this way of writ-\ning programs can be considered as a combination of learning by listening\nand learning by doing due to it is based on applying agent technology\nand machine learning. As the main result we introduce the problem of\nlearning and then we show that it cannot easily be handled in reality\ntherefore it is reasonable to use machine learning algorithm for learning\nTuring machines.\n1 Introduction\nSamu is a disembodied developmental robotic experiment to develop a family\nchatterbot agent who will be able to talk in a natural language like humans do\n[B\u0013 at15a]. At this moment it is only an utopian idea of the project Samu. The\npractical purpose of Samu projects is to develop computational mental organs\nthat can support software agents to acquire higher-order knowledge from their\ninput [BB16]. The activities have been conducted during the development of\nsuch mental organs may be considered as \frst e\u000borts to create on demand the\nAsimovian profession called robopsychology1[B\u0013 at16a].\nThe roots of this paper lie in the two new software experiments Samu Turing\n[B\u0013 at16c] and Samu C. Turing [B\u0013 at16b]. These are very simpli\fed versions of\nthe former habituation-sensi",
    "title": "Theoretical Robopsychology: Samu Has Learned Turing Machines",
    "abstract": "From the point of view of a programmer, the robopsychology is a synonym for\nthe activity is done by developers to implement their machine learning\napplications. This robopsychological approach raises some fundamental\ntheoretical questions of machine learning. Our discussion of these questions is\nconstrained to Turing machines. Alan Turing had given an algorithm (aka the\nTuring Machine) to describe algorithms. If it has been applied to describe\nitself then this brings us to Turing's notion of the universal machine. In the\npresent paper, we investigate algorithms to write algorithms. From a pedagogy\npoint of view, this way of writing programs can be considered as a combination\nof learning by listening and learning by doing due to it is based on applying\nagent technology and machine learning. As the main result we introduce the\nproblem of learning and then we show that it cannot easily be handled in\nreality therefore it is reasonable to use machine learning algorithm for\nlearning Turing machines.",
    "link": "http://arxiv.org/abs/1606.02767v2",
    "published": "2016-06-08T21:46:20Z"
  },
  "17": {
    "pdf_path": "data/pdfs/machine learning_paper_333.pdf",
    "text_excerpt": "Techniques for Interpretable Machine Learning\nMengnan Du, Ninghao Liu, Xia Hu\nDepartment of Computer Science and Engineering, Texas A&M University\n{dumengnan,nhliu43,xiahu}@tamu.edu\nABSTRACT\nInterpretable machine learning tackles the important prob-\nlem that humans cannot understand the behaviors of com-\nplex machine learning models and how these models arrive at\na particular decision. Although many approaches have been\nproposed, a comprehensive understanding of the achieve-\nments and challenges is still lacking. We provide a survey\ncovering existing techniques to increase the interpretability\nof machine learning models. We also discuss crucial issues\nthat the community should consider in future work such as\ndesigning user-friendly explanations and developing compre-\nhensive evaluation metrics to further push forward the area\nof interpretable machine learning.\n1. INTRODUCTION\nMachine learning is progressing at an astounding rate,\npowered by complex models such as ensemble models and\ndeep neural networks (DNNs). These models have a wide\nrange of real-world applications, such as movie recommenda-\ntions of Net\rix, neural machine translation of Google, speech\nrecognition of Amazon Alexa. Despite the successes, ma-\nchine learning has its own limitations and drawbacks. The\nmost signi\fcant one is the lack of transparency behind their\nbehaviors, which leaves users with little understanding of\nhow particular decisions are made by these models. Con-\nsider, for instance, an advanced self-driving car equipped\nwith various machine learning algorithms doesn't brake or\ndecelerate when confronting a stopped \fretruck. This un-\nexpected behavior may frustrate and confuse users, making\nthem wonder why. Even worse, the wrong decisions could\ncause severe consequences if the car is driving at highway\nspeeds and might \fnally crash the \fretruck. The concerns\nabout the black-box nature of complex models have ham-\npered their further applications in our society, especially in\nthose critical ",
    "title": "Techniques for interpretable machine learning",
    "abstract": "Uncovering the mysterious ways machine learning models make decisions.",
    "link": "https://www.semanticscholar.org/paper/3df952d4a724655f7520ff95d4b2cef90fff0cae",
    "published": "2018-07-31"
  },
  "18": {
    "pdf_path": "data/pdfs/machine learning_paper_38.pdf",
    "text_excerpt": "arXiv:2005.09428v2  [cs.LG]  14 Aug 2024Quantum-Classical Machine learning by Hybrid Tensor Netwo rks\nDing Liu,1,∗Jiaqi Yao,1Zekun Yao,1and Quan Zhang1\n1School of Computer Science and Technology, Tiangong Univer sity, Tianjin 300387, China\nTensor networks (TN) have found a wide use in machine learnin g, and in particular, TN and deep\nlearning bear striking similarities. In this work, we propo se the quantum-classical hybrid tensor\nnetworks (HTN) which combine tensor networks with classica l neural networks in a uniform deep\nlearning framework to overcome the limitations of regular t ensor networks in machine learning.\nWe ﬁrst analyze the limitations of regular tensor networks i n the applications of machine learning\ninvolving the representation power and architecture scala bility. We conclude that in fact the regular\ntensor networks are not competent to be the basic building bl ocks of deep learning. Then, we\ndiscuss the performance of HTN which overcome all the deﬁcie ncy of regular tensor networks for\nmachine learning. In this sense, we are able to train HTN in th e deep learning way which is the\nstandard combination of algorithms such as Back Propagatio n and Stochastic Gradient Descent. We\nﬁnally provide two applicable cases to show the potential ap plications of HTN, including quantum\nstates classiﬁcation and quantum-classical autoencoder. These cases also demonstrate the great\npotentiality to design various HTN in deep learning way.\nI. INTRODUCTION.\nInrecentyears,tensornetworks(TN) havedrawnmore\nattention as one of the most powerful numerical tools for\nstudying quantum many-body systems [ 1–4]. Further-\nmore, TN have been recently applied to many research\nareas of machine learning [ 5–8], such as image classiﬁ-\ncation [9–11], dimensionality reduction [ 12,13], gener-\native model [ 10,14], data compression [ 15], improving\ndeep neural network [ 16], probabilistic graph model [ 17],\nquantum compressed sensing [ 18], even the promising\nway to implement quantum ci",
    "title": "Quantum-Classical Machine learning by Hybrid Tensor Networks",
    "abstract": "Tensor networks (TN) have found a wide use in machine learning, and in\nparticular, TN and deep learning bear striking similarities. In this work, we\npropose the quantum-classical hybrid tensor networks (HTN) which combine tensor\nnetworks with classical neural networks in a uniform deep learning framework to\novercome the limitations of regular tensor networks in machine learning. We\nfirst analyze the limitations of regular tensor networks in the applications of\nmachine learning involving the representation power and architecture\nscalability. We conclude that in fact the regular tensor networks are not\ncompetent to be the basic building blocks of deep learning. Then, we discuss\nthe performance of HTN which overcome all the deficiency of regular tensor\nnetworks for machine learning. In this sense, we are able to train HTN in the\ndeep learning way which is the standard combination of algorithms such as Back\nPropagation and Stochastic Gradient Descent. We finally provide two applicable\ncases to show the potential applications of HTN, including quantum states\nclassification and quantum-classical autoencoder. These cases also demonstrate\nthe great potentiality to design various HTN in deep learning way.",
    "link": "http://arxiv.org/abs/2005.09428v2",
    "published": "2020-05-15T10:20:35Z"
  },
  "19": {
    "pdf_path": "data/pdfs/machine learning_paper_651.pdf",
    "text_excerpt": "Machine Learning, 41, 197–215, 2000\nc°2000 Kluwer Academic Publishers. Manufactured in The Netherlands.\nAdaptive Versus Nonadaptive Attribute-Efﬁcient\nLearning⁄\nPETER DAMASCHKE Peter.Damaschke@fernuni-hagen.de\nFern Universit ¨at, Theoretische Informatik II, 58084 Hagen, Germany\nEditor:Lisa Hellerstein\nAbstract. We study the complexity of learning arbitrary Boolean functions of nvariables by membership\nqueries, if at most rvariables are relevant. Problems of this type have important applications in fault searching,\ne.g. logical circuit testing and generalized group testing. Previous literature concentrates on special classes ofsuchBooleanfunctionsandconsidersonlyadaptivestrategies.FirstwegiveastraightforwardadaptivealgorithmusingO.r2\nrlogn/queries, but actually, most queries are asked nonadaptively. This leads to the problem of\npurely nonadaptive learning. We give a graph-theoretic characterization of nonadaptive learning families, called\nr-wise bipartite connected families. By the probabilistic method we show the existence of such families of size\nO.r2rlognCr22r/. This implies that nonadaptive attribute-efﬁcient learning is not essentially more expensive\nthan adaptive learning. We also sketch an explicit pseudopolynomial construction, though with a slightly worsebound. It uses the common derandomization technique of small-biased k-independent sample spaces. For the\nspecial case rD2, we get roughly 2 :275lognadaptive queries, which is fairly close to the obvious lower bound\nof 2logn. For the class of monotone functions, we prove that the optimal query number O.2\nrCrlogn/can be\nalready achieved in O.r/stages. On the other hand, ˜.2rlogn/is a lower bound on nonadaptive queries.\nKeywords: membership queries, relevant variables, nonadaptive learning, probabilistic method, group testing,\nmonotone Boolean functions\n1. Introduction\nThis paper addresses the problem of exact learning of Boolean functions by membership\nqueries, provided that at most rof thenattributes (variabl",
    "title": "Adaptive Versus Nonadaptive Attribute-Efficient Learning",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1007616604496",
    "published": "2002-12-22T05:54:50Z"
  },
  "20": {
    "pdf_path": "data/pdfs/machine learning_paper_278.pdf",
    "text_excerpt": "Machine Learning and Applications in Ultrafast Photonics  1 \n 2 \nGoëry Genty,1 Lauri Salmela,1 John M. Dudley,2 Daniel Brunner,2 3 \nAlexey Kokhanovskiy,3 Sergei Kobtsev,3 and Sergei K. Turitsyn3, 4 4 \n1Laboratory of Photonics, Tampere Un iversity, FI-33101 Tampere, Finland  5 \n2Institut FEMTO-ST, Université Bourgogne Franche-Comté 6 \nCNRS UMR 6174, 25000 Besançon, France  7 \n3Division of Laser Physics and Innovative Technologies,  8 \nNovosibirsk State University, Pirogov a str., 2, Novosibirsk, 630090, Russia.  9 \n4Aston Institute of Photonic Technologies,  10 \nAston University, B4 7ET, Birmingham, United Kingdom.  11 \n 12 \n 13 \nAbstract 14 \nRecent years have seen the rapid growth and development of the field of smart photonics, 15 \nwhere machine learning algorithms are bein g matched to optical systems to add new 16 \nfunctionalities and to enhance performance. An area where mach ine learning shows particular 17 \npotential to accelerate technol ogy is the field of ultrafas t photonics – the generation and 18 \ncharacterization of light pulses,  the study of light-matter inter actions on short timescales, and 19 \nhigh-speed optical measurements. Our aim here is  to highlight a number  of specific areas 20 \nwhere the promise of machine learning in u ltrafast photonics has already been realized, 21 \nincluding the design and operati on of pulsed lasers, and the ch aracterization and control of 22 \nultrafast propagation dynamics. We also consider challenges a nd future areas of research. 23 \n  24 Machine learning is an umbrella term describing  the use of statistical techniques and numerical 25 \nalgorithms to carry out tasks without explicit programmed a nd procedural instructions. 26 \nMachine learning algorithms are widely used in many areas of engineering and science, with 27 \nparticular strengths in classification, patte rn recognition, predictio n, system parameter 28 \noptimization, and the construc tion of models of complex dynamics from observed data. 29 \nMachine le",
    "title": "Machine learning and applications in ultrafast photonics",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/e8aa24d9c64f1215a97e8118905fad0189a53b97",
    "published": "2020-11-30"
  },
  "21": {
    "pdf_path": "data/pdfs/machine learning_paper_634.pdf",
    "text_excerpt": "Machin e Learning , 13, 145-14 9 (1993 )\n© 1993 Kluwe r Academi c Publishers , Boston . Manufacture d in The Netherlands .\nBook  Review\nMachin e Learning : A Theoretica l Approac h\nby Bala s K. Natarajan . Morga n Kaufman n Publishers ,\nInc., 1991 .\nLISA HELLERSTEI N\nDepartment  of Electrical  Engineering  and Computer  Science,  Northwestern  University,  Evanston,  IL 60208-3118\nEditor : Albert o Segr e\n1. Overvie w\nThe PAC (probabl y approximatel y correct ) mode l of learnin g was introduce d in 1984 by\nL. Valian t as a forma l framewor k for studyin g the computationa l complexit y of machin e\nlearning . In Machine  Learning,  a Theoretical  Approach,  Bala s K. Nataraja n provide s an\nintroductio n to researc h in the PAC model , and mor e generally , to the field of computa -\ntiona l learnin g theory . The book was writte n for use as a text in an introductor y graduat e\ncourse . It is also advertise d as a referenc e for AI researchers . Reader s are assume d to be\nfamilia r with topic s such as polynomial-tim e algorithms , NP-completeness , and deterministi c\nfinite automata , althoug h mor e advance d topic s such as randomize d complexit y classe s\nand cryptographi c scheme s are covere d briefl y at an introductor y level .\nNataraja n contrast s the goal s and method s of researc h in computationa l learnin g theor y\nto the goal s and method s of two other approache s to learning—A I and inductiv e inference .\nAI researc h is different , he says , becaus e it tend s to be less forma l and more experimental .\nHe migh t also have mentione d that the problem s in AI are often mor e genera l and less\nwell define d than problem s in computationa l learnin g theory : for example , learnin g to under -\nstand Englis h stories , rathe r than learnin g classe s of Boolea n monomials . This differenc e\nis certainl y eviden t in his book . Nataraja n also point s out that althoug h ther e are man y\nsimilaritie s betwee n inductiv e inferenc e and computationa l lea",
    "title": "Book Review Machine Learning: A Theoretical Approach by Balas K. Natarajan. Morgan Kaufmann Publishers, Inc., 1991",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022691730976",
    "published": "2003-04-04T16:55:36Z"
  },
  "22": {
    "pdf_path": "data/pdfs/machine learning_paper_383.pdf",
    "text_excerpt": "Machin e Learning , 14, 305-31 2 (1994 )\n© 199 4 Kluwe r Academi c Publishers , Boston . Manufacture d in The Netherlands .\nExtended  Abstract\nMachin e Learnin g and Qualitativ e Reasonin g\nIVAN BRATK O\nLjubljana  University,  Faculty  of Electrical  Engineering  and  Computer  Science  and  J. Stefan  Institute,  Ljubl-\njana,  Slovenia\nEditor : D. Sleema n\nDate Submitted : 17 Novembe r 1992\n1. Qualitativ e representation s and ILP\nQualitativ e modelin g and reasonin g is a mos t interestin g area for applyin g and experi -\nmentin g with machin e learnin g techniques . Qualitativ e reasonin g task s of interes t wher e\nmachin e learnin g can be applie d includ e modeling , diagnosis , control , discovery , design ,\nand knowledg e compilation . This pape r review s example s of recen t researc h into som e\nof thes e application s areas . In particular , the example s give n illustrat e how Inductiv e\nLogic Programmin g (ILP ; Muggleto n 1990 , 1992 ) applie s naturall y to thes e task s whe n\nqualitativ e representation s are used .\nLet us first conside r the natur e of description s that we typicall y encounte r in qualitativ e\nreasoning . Conside r the proces s of fillin g a containe r with water . Tabl e 1 show s som e\nexample s of quantitativ e description s and their correspondin g qualitativ e abstractions . In\nrow (a) of the table , t1 and t2 denot e som e time point s that we kno w exist , but the\nexact time s are not given , zero  and top correspon d to two levels , 0 and the top of the\ncontainer , wher e we kno w that zero  and top, but the exac t valu e of top is not know n\nor given . In row (b) of the table , the qualitativ e descriptio n is read as \"Amount  is\nmonotonically  increasing  functio n of Level.\"\nTable  I. Quantitativ e and qualitativ e description s\n(a)\n(b)Quantitativ e\nTime\n3 sec.\n4 sec.\n5 sec.Level\n0.01 m\n0.23 m\n0.31 m\nAmount  = 2.5 * Level  + 0.7 * Level 2Qualitativ e\nLevel(between(t 1,t2)) =\n(between(zero,  top),  increasing)\nAmount  = M+",
    "title": "Machine Learning and Qualitative Reasoning",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022661713654",
    "published": "2003-04-04T16:55:36Z"
  },
  "23": {
    "pdf_path": "data/pdfs/machine learning_paper_72.pdf",
    "text_excerpt": "arXiv:2205.00210v1  [cs.SE]  30 Apr 2022Software Testing for Machine Learning\nDusica Marijan, Arnaud Gotlieb\nSimula Research Laboratory, Norway\ndusica@simula.no, arnaud@simula.no\nAbstract\nMachine learning has become prevalent across a wide va-\nriety of applications. Unfortunately, machine learning ha s\nalso shown to be susceptible to deception, leading to errors ,\nand even fatal failures. This circumstance calls into ques-\ntion the widespread use of machine learning, especially in\nsafety-critical applications, unless we are able to assure its\ncorrectness and trustworthiness properties. Software ver iﬁ-\ncation and testing are established technique for assuring s uch\nproperties, for example by detecting errors. However, soft -\nware testing challenges for machine learning are vast and pr o-\nfuse - yet critical to address. This summary talk discusses t he\ncurrent state-of-the-art of software testing for machine l earn-\ning. More speciﬁcally, it discusses six key challenge areas\nfor software testing of machine learning systems, examines\ncurrent approaches to these challenges and highlights thei r\nlimitations. The paper provides a research agenda with elab -\norated directions for making progress toward advancing the\nstate-of-the-art on testing of machine learning.\nIndex terms— testing challenges, machine learning, ma-\nchine learning testing, testing ML, testing AI\n1 Introduction\nApplications of machine learning (ML) technology have be-\ncome vital in many innovative domains. At the same time,\nthe vulnerability of ML has become evident, sometimes\nleading to catastrophic failures1. This entails that compre-\nhensive testing of ML needs to be performed, to ensure the\ncorrectness and trustworthiness of ML-enabled systems.\nSoftware testing of ML systems is susceptible to a num-\nber of challenges compared to testing of traditional softwa re\nsystems. In this paper, by traditional systems we mean\nsoftware systems not integrating ML, and by ML systems\nwe mean software systems contai",
    "title": "Software Testing for Machine Learning",
    "abstract": "Machine learning has become prevalent across a wide variety of applications.\nUnfortunately, machine learning has also shown to be susceptible to deception,\nleading to errors, and even fatal failures. This circumstance calls into\nquestion the widespread use of machine learning, especially in safety-critical\napplications, unless we are able to assure its correctness and trustworthiness\nproperties. Software verification and testing are established technique for\nassuring such properties, for example by detecting errors. However, software\ntesting challenges for machine learning are vast and profuse - yet critical to\naddress. This summary talk discusses the current state-of-the-art of software\ntesting for machine learning. More specifically, it discusses six key challenge\nareas for software testing of machine learning systems, examines current\napproaches to these challenges and highlights their limitations. The paper\nprovides a research agenda with elaborated directions for making progress\ntoward advancing the state-of-the-art on testing of machine learning.",
    "link": "http://arxiv.org/abs/2205.00210v1",
    "published": "2022-04-30T08:47:10Z"
  },
  "24": {
    "pdf_path": "data/pdfs/machine learning_paper_34.pdf",
    "text_excerpt": "Elements of effective machine learning datasets in\nastronomy\nBernie Boscoe*\nDepartment of Computer Science\nOccidental College\nLos Angeles, CA 90041\nboscoe@oxy.eduTuan Do\nDepartment of Physics and Astronomy\nUCLA\nLos Angeles, CA 90025\ntdo@astro.ucla.edu\nEvan Jones\nDepartment of Physics and Astronomy\nUCLA\nLos Angeles, CA 90025\nevan.jones@astro.ucla.eduYunqi Li\nDepartment of Physics and Astronomy\nUCLA\nLos Angeles, CA 90025\nKevin Alfaro\nDepartment of Physics and Astronomy\nUCLA\nLos Angeles, CA 90025Christy Ma\nDepartment of Physics and Astronomy\nUCLA\nLos Angeles, CA 90025\nAbstract\nIn this work, we identify elements of effective machine learning datasets in as-\ntronomy and present suggestions for their design and creation. Machine learning\nhas become an increasingly important tool for analyzing and understanding the\nlarge-scale ﬂood of data in astronomy. To take advantage of these tools, datasets\nare required for training and testing. However, building machine learning datasets\nfor astronomy can be challenging. Astronomical data is collected from instruments\nbuilt to explore science questions in a traditional fashion rather than to conduct\nmachine learning. Thus, it is often the case that raw data, or even downstream\nprocessed data is not in a form amenable to machine learning. We explore the\nconstruction of machine learning datasets and we ask: what elements deﬁne ef-\nfective machine learning datasets? We deﬁne effective machine learning datasets\nin astronomy to be formed with well-deﬁned data points, structure, and metadata.\nWe discuss why these elements are important for astronomical applications and\nways to put them in practice. We posit that these qualities not only make the\ndata suitable for machine learning, they also help to foster usable, reusable, and\nreplicable science practices.\n1 Introduction\nIn recent years astronomy has seen a wide application of machine learning (ML) in numerous\nsubﬁelds, from exoplanets and stellar astrophysics to extragalactic and cosmolog",
    "title": "Elements of effective machine learning datasets in astronomy",
    "abstract": "In this work, we identify elements of effective machine learning datasets in\nastronomy and present suggestions for their design and creation. Machine\nlearning has become an increasingly important tool for analyzing and\nunderstanding the large-scale flood of data in astronomy. To take advantage of\nthese tools, datasets are required for training and testing. However, building\nmachine learning datasets for astronomy can be challenging. Astronomical data\nis collected from instruments built to explore science questions in a\ntraditional fashion rather than to conduct machine learning. Thus, it is often\nthe case that raw data, or even downstream processed data is not in a form\namenable to machine learning. We explore the construction of machine learning\ndatasets and we ask: what elements define effective machine learning datasets?\nWe define effective machine learning datasets in astronomy to be formed with\nwell-defined data points, structure, and metadata. We discuss why these\nelements are important for astronomical applications and ways to put them in\npractice. We posit that these qualities not only make the data suitable for\nmachine learning, they also help to foster usable, reusable, and replicable\nscience practices.",
    "link": "http://arxiv.org/abs/2211.14401v2",
    "published": "2022-11-25T23:37:24Z"
  },
  "25": {
    "pdf_path": "data/pdfs/machine learning_paper_465.pdf",
    "text_excerpt": "Machin e Learnin g 4, 247-24 9 (1989 )\n© 1989 Kluwe r Academi c Publishers , Boston . Manufacture d in The Netherlands .\nIntroduction : A Sample r in Knowledg e Acquisitio n for\nthe Machin e Learnin g Communit y\nThis specia l issue is devote d to invite d editorial s and technica l paper s on knowledg e acqui -\nsition . In the past , specia l issue s have been devote d to recognize d subfield s of machin e\nlearning , wher e a subfiel d migh t be characterize d by a particula r metho d of machin e learn -\ning, such as geneti c algorithms . The relationshi p betwee n machin e learnin g and knowledg e\nacquisitio n is not so clearcu t as the field-subfiel d one. Neithe r are the method s of knowledg e\nacquisitio n so homogeneou s and easil y characterize d as for geneti c algorithms .\nJust as with machin e learnin g as a whole , peopl e who conside r themselve s to be workin g\nin the field of knowledg e acquisitio n are identifie d mor e by the goal of their wor k than\nby any particula r methodolog y they apply . I think most of us in knowledg e acquisitio n woul d\nagree that our goal is to mak e exper t system s easie r to build and maintai n and, alon g the\nway, to mak e the exper t system s built more explainable , mor e robust , and so on. This aim\nhas give n the knowledg e acquisitio n field an engineerin g flavor . Our tools and methodologie s\nare judge d by their suitabilit y for their user communit y and the environmen t in whic h they\nare used , and by the performanc e of the knowledg e bases they produc e and the exper t system s\nthey support . We tend to plac e a heav y emphasi s on fieldin g system s and evaluatin g the\npracticalit y of our approaches .\nIn the ques t for this goal , knowledg e acquisitio n worker s have employe d a diversit y of\nmethods . Som e wor k in knowledg e acquisition , includin g som e of the earlies t work , was\ndone usin g traditiona l machin e learnin g technique s (e.g. [Michalsk i and Chilausky , 1980 ;\nQuinlan , 1986]) , but o",
    "title": "Introduction: A Sampler in Knowledge Acquisition for the Machine Learning Community",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022694404143",
    "published": "2003-04-04T16:55:36Z"
  },
  "26": {
    "pdf_path": "data/pdfs/machine learning_paper_399.pdf",
    "text_excerpt": "Machin e Learnin g 3: 253-259 , 1989\n© 1989 Kluwe r Academi c Publisher s - Manufacture d in The Netherland s\nEDITORIA L\nTowar d a Unifie d Scienc e of Machin e Learnin g\nDiversificatio n and unificatio n\nMachin e learnin g is a divers e disciplin e that acts as host to a variet y of re-\nsearc h goals , learnin g techniques , and methodologica l approaches . Researcher s\nare makin g continua l progres s on all of these front s - tacklin g new problems ,\nformulatin g innovativ e solution s to thos e problems , and devisin g new ways to\nevaluat e their solutions . Suc h variet y is the sign of a health y and growin g field .\nHowever , diversificatio n also has its dangers . Subdiscipline s can emerg e\nthat focu s on one goal or evaluatio n schem e to the exclusio n of others , and\nsimilaritie s amon g method s can be obscure d by differen t notation s and termi -\nnology . Thus , it is equall y importan t to searc h for basic principle s that unif y\nthe differen t paradigm s withi n a field . Just as the twin force s of gravit y and\npressur e hold a star in dynami c equilibriu m whil e generatin g energy , so the\njoint processe s of diversificatio n and unificatio n can hold a scienc e togethe r\nwhile fosterin g progress .\nIn this editorial , I examin e seve n dichotomie s that have emerge d in recen t\nyears to partitio n the field of machin e learning . I begi n with thre e issue s\nrelate d to researc h goal s and evaluatio n methodologies , then turn to four mor e\nsubstantiv e issue s abou t learnin g method s themselves . In each case , I argu e\nthat long-ter m progres s will occu r only if we can find way s to unif y thes e\napparentl y competin g view s into a coheren t whole .\nAccurac y and efficienc y\nLearnin g involve s some chang e in performance, 1 and one of the main goal s\nof machin e learnin g is to develo p algorithm s that improv e their performanc e\nover time . However , ther e are man y differen t aspect s of performance . For\ninstance , early work on e",
    "title": "Toward a Unified Science of Machine Learning",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022689616458",
    "published": "2003-04-04T16:55:36Z"
  },
  "27": {
    "pdf_path": "data/pdfs/machine learning_paper_11.pdf",
    "text_excerpt": "© 20 18 IEEE. Personal use of this material is permitted. Permission from IEEE must be  obtained for all other uses, in any current or future media, including  \nreprinting/republishing this material for advertising or promotional purposes, creating new  collective works, for resale or redistribution to servers or lists, or reuse \nof any copyrighted  component of this work in other w orks.  A Unified Analytical Framework for Trustable Machine \nLearning and Automation Running with Blockchain  \nTao Wang  \nSAS Institute Inc.  \nCary, USA  \nt.wang@sas.com  \nAbstract—Traditional machine learning algorithm s use data \nfrom database s that are mutable , and ther efore the data cannot \nbe fully trust ed. Also, the machine learning process  is difficult to \nautomate . This paper proposes build ing a trustable machine \nlearning system by using  blockchain technology , which can store \ndata in a permanent and immutable way. In addition , smart \ncontract s are used to automate the machine learning process. \nThis paper makes  three contributions. First, it establishes a link \nbetween machine learning technology and blockchain technology. \nPreviously, machine learning and blockchain have been \nconsidered two independent technologies without an obvious link. \nSecond, it proposes a unified analytical framework for trustable \nmachine learning by using  blockchain technology. Th is unified \nframework solves both the trust ability  and automation issue s in \nmachine learning. Third, it enables  a computer to translate core \nmachine learning impl ementation from a single  thread  on a  \nsingle  machine to multi ple thread s on multi ple machine s running  \nwith blockchain by using a unified approach.  The paper uses \nassociation rule mining  as an example  to demonstrate how \ntrustable machine learning can be implemented  with blockchain, \nand it shows how this approach can be used to analyze  opioid \nprescription s to help combat the opioid crisis.   \nKeywords —machine learning, tru st,",
    "title": "A Unified Analytical Framework for Trustable Machine Learning and\n  Automation Running with Blockchain",
    "abstract": "Traditional machine learning algorithms use data from databases that are\nmutable, and therefore the data cannot be fully trusted. Also, the machine\nlearning process is difficult to automate. This paper proposes building a\ntrustable machine learning system by using blockchain technology, which can\nstore data in a permanent and immutable way. In addition, smart contracts are\nused to automate the machine learning process. This paper makes three\ncontributions. First, it establishes a link between machine learning technology\nand blockchain technology. Previously, machine learning and blockchain have\nbeen considered two independent technologies without an obvious link. Second,\nit proposes a unified analytical framework for trustable machine learning by\nusing blockchain technology. This unified framework solves both the\ntrustability and automation issues in machine learning. Third, it enables a\ncomputer to translate core machine learning implementation from a single thread\non a single machine to multiple threads on multiple machines running with\nblockchain by using a unified approach. The paper uses association rule mining\nas an example to demonstrate how trustable machine learning can be implemented\nwith blockchain, and it shows how this approach can be used to analyze opioid\nprescriptions to help combat the opioid crisis.",
    "link": "http://arxiv.org/abs/1903.08801v1",
    "published": "2019-03-21T02:17:08Z"
  },
  "28": {
    "pdf_path": "data/pdfs/machine learning_paper_271.pdf",
    "text_excerpt": "RESEARCH ARTICLE Open Access\nComparing different supervised machine\nlearning algorithms for disease prediction\nShahadat Uddin1*, Arif Khan1,2, Md Ekramul Hossain1and Mohammad Ali Moni3\nAbstract\nBackground: Supervised machine learning algorithms have been a dominant method in the data mining field.\nDisease prediction using health data has recently shown a potential application area for these methods. This study\naims to identify the key trends among different types of supervised machine learning algorithms, and their\nperformance and usage for disease risk prediction.\nMethods: In this study, extensive research efforts were made to identify those studies that applied more than one\nsupervised machine learning algorithm on single disease prediction. Two databases (i.e., Scopus and PubMed) were\nsearched for different types of search items. Thus, we selected 48 articles in total for the comparison among\nvariants supervised machine learning algorithms for disease prediction.\nResults: We found that the Support Vector Machine (SVM) algorithm is applied most frequently (in 29 studies)\nfollowed by the Naïve Bayes algorithm (in 23 studies). However, the Random Forest (RF) algorithm showed superior\naccuracy comparatively. Of the 17 studies where it was applied, RF showed the highest accuracy in 9 of them, i.e.,\n53%. This was followed by SVM which topped in 41% of the studies it was considered.\nConclusion: This study provides a wide overview of the relative performance of different variants of supervised\nmachine learning algorithms for disease prediction. This important information of relative performance can be used\nto aid researchers in the selection of an appropriate supervised machine learning algorithm for their studies.\nKeywords: Machine learning, Supervised machine learning algorithm, Medical data, Disease prediction\nBackground\nMachine learning algorithms employ a variety of statis-\ntical, probabilistic and optimisation methods to learn\nfrom past experience and detect useful",
    "title": "Comparing different supervised machine learning algorithms for disease prediction",
    "abstract": "BackgroundSupervised machine learning algorithms have been a dominant method in the data mining field. Disease prediction using health data has recently shown a potential application area for these methods. This study ai7ms to identify the key trends among different types of supervised machine learning algorithms, and their performance and usage for disease risk prediction.MethodsIn this study, extensive research efforts were made to identify those studies that applied more than one supervised machine learning algorithm on single disease prediction. Two databases (i.e., Scopus and PubMed) were searched for different types of search items. Thus, we selected 48 articles in total for the comparison among variants supervised machine learning algorithms for disease prediction.ResultsWe found that the Support Vector Machine (SVM) algorithm is applied most frequently (in 29 studies) followed by the Naïve Bayes algorithm (in 23 studies). However, the Random Forest (RF) algorithm showed superior accuracy comparatively. Of the 17 studies where it was applied, RF showed the highest accuracy in 9 of them, i.e., 53%. This was followed by SVM which topped in 41% of the studies it was considered.ConclusionThis study provides a wide overview of the relative performance of different variants of supervised machine learning algorithms for disease prediction. This important information of relative performance can be used to aid researchers in the selection of an appropriate supervised machine learning algorithm for their studies.",
    "link": "https://www.semanticscholar.org/paper/c8ac6060d34179871b81ecd19621c63360347f8e",
    "published": "2019-12-01"
  },
  "29": {
    "pdf_path": "data/pdfs/machine learning_paper_55.pdf",
    "text_excerpt": "TOWARDS IDENTIFYING AND MANAGING SOURCES \nOF UNCERTAINTY IN AI AND MACHINE LEARNING \nMODELS – AN OVERVIEWFRAUNHOFER INSTITUTE FOR EXPERIMENTAL SOFTWARE ENGINEERING IESE\nMICHAEL KLÄSUNCERTAINTY IN AI AND MACHINE LEARNING – AN OVERVIEW\n2As a consequence, the functional behavior expected from \ndata-driven components can only be specified in part on their \nintended domain, and we cannot assure that they will behave \nas expected in all cases. Moreover, their processing structure is \nusually difficult to trace and validate by humans because this \nstructure rarely follows human intuition but is generated to pro -\nvide the algorithmically generalized input-output relationship in \nan effective manner. Prominent representatives of models used \nby data-driven components are artificial neural networks and \nsupport vector machines (Russell & Norvig, 2016).\nSince data-driven models are an important source of uncertain -\nty in embedded systems that collaborate in an open context, \nthe uncertainty they introduce has to be appropriately under -\nstood and managed during design time and runtime.\nPrevious work (Kläs & Vollmer, 2018) proposes separating the \nsources of uncertainty in data-driven components into three \nmajor classes, distinguishing between uncertainty caused by \nlimitations in terms of model fit, data quality, and scope com -\npliance. Whereas model fit focuses on the inherent uncertainty \nin data-driven models, data quality covers the additional uncer -\ntainty caused by their application to input data obtained in \nsuboptimal conditions and scope compliance covers situations \nwhere the model is likely applied outside the scope for which it \nwas trained and validated.Motivation\nData-driven models (Solomatine & Ostfeld, 2008), (Solomatine, \nSee, & Abrahart, 2009), such as those provided by the applica -\ntion of AI and machine learning, are becoming components of \nincreasing importance for complex software-intensive systems. \nIn particular, embedded systems that collaborate ",
    "title": "Towards Identifying and Managing Sources of Uncertainty in AI and\n  Machine Learning Models - An Overview",
    "abstract": "Quantifying and managing uncertainties that occur when data-driven models\nsuch as those provided by AI and machine learning methods are applied is\ncrucial. This whitepaper provides a brief motivation and first overview of the\nstate of the art in identifying and quantifying sources of uncertainty for\ndata-driven components as well as means for analyzing their impact.",
    "link": "http://arxiv.org/abs/1811.11669v1",
    "published": "2018-11-28T16:49:37Z"
  },
  "30": {
    "pdf_path": "data/pdfs/machine learning_paper_279.pdf",
    "text_excerpt": "78    communications of the acm   |   october 2012  |   vol. 55  |   no. 10review articles\nMachine learning syste Ms automatically learn \nprograms from data. This is often a very attractive \nalternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k .a. \ndata mining or predictive analytics) will be the driver of the next big wave of innovation.\n15 Several fine \ntextbooks are available to interested practitioners and researchers (for example, Mitchell\n16 and Witten et \nal.24). However, much of the “folk knowledge” that is needed to successfully develop \nmachine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article.doi:10.1145/2347736.2347755\nTapping into the “folk knowledge” needed to \nadvance machine learning applications.\nby Pedro domingos\na few u seful \nt\nhings to \nKnow \na\nbout \nm\nachine \nLearning\n key insights\n    machine learning algorithms can figure \nout how to perform impor\ntant tasks \nby generalizing from examples. \nt\nhis is \noften feasible and cost-effective where \nmanual programming is not. \na\ns more \ndata becomes available, more ambitious problems can be tackled.\n    machine learning is widely used in \ncomputer science and other fields. \nh\nowever, developing successful \nmachine learning applications requires a substantial amount of “black art” that is difficult to find in textbooks.\n    this article summarizes 12 key lessons \nthat machine learning researchers and pr\nactitioners have l",
    "title": "A few useful things to know about machine learning",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/1696cbf7da0ee845c50591843993e6605adec177",
    "published": "2012-10-01"
  },
  "31": {
    "pdf_path": "data/pdfs/machine learning_paper_523.pdf",
    "text_excerpt": "Machine Learning, 19, 133-152(1995)\n© 1995 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\nComprehension Grammars Generated from\nMachine Learning of Natural Languages*\nPATRICK SUPPES suppes@csli.stanford.edu\nCSLI, Ventura Hall, Stanford University, Stanford CA 94305-4115\nMICHAEL BOTTNER boettner@ockham.stanford.edu\nMax-Planck-lnstitutfiir Psycholinguistik, 6526 XD Nijmegen, The Netherlands\nLIN LIANG liang@csli.stanford.edu\nCSL1, Ventura Hall, Stanford University, Stanford CA 94305-4115\nEditor: Jaime Carbonell\nAbstract. We are developing a theory of probabilistic language learning in the context of robotic instruction in\nelementary assembly actions. We describe the process of machine learning in terms of the various events that\nhappen on a given trial, including the crucial association of words with internal representations of their meaning.\nOf central importance in learning is the generalization from utterances to grammatical forms. Our system derives\na comprehension grammar for a superset of a natural language from pairs of verbal stimuli like Go to the screw!\nand corresponding internal representations of coerced actions. For the derivation of a grammar no knowledge of\nthe language to be learned is assumed but only knowledge of an internal language.\nWe present grammars for English, Chinese, and German generated from a finite sample of about 500 commands\nthat are roughly equivalent across the three languages. All of the three grammars, which are context-free in form,\naccept an infinite set of commands in the given language.\nKeywords: learning, natural language, grammatical generalization, robotics\n1. Introduction and Overview\nWe are developing a theory of machine language learning in the context of robotic in-\nstruction of elementary assembly actions. More specifically, our situations of language\nlearning concern actions like moving to objects, picking up objects and moving objects in\nenvironments. Typical objects are screws, nuts, washers, and ",
    "title": "Comprehension Grammars Generated from Machine Learning of Natural Languages",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022663208628",
    "published": "2003-04-04T16:55:36Z"
  },
  "32": {
    "pdf_path": "data/pdfs/machine learning_paper_47.pdf",
    "text_excerpt": "On conditional parity as a\nnotion of non-discrimination in\nmachine learning\nYa'acov Ritov, Yuekai Sun, and Ruofei Zhao\nUniversity of Michigan\nAbstract. We identify conditional parity as a general notion of non-\ndiscrimination in machine learning. In fact, several recently proposed\nnotions of non-discrimination, including a few counterfactual notions,\nare instances of conditional parity. We show that conditional parity is\namenable to statistical analysis by studying randomization as a general\nmechanism for achieving conditional parity and a kernel-based test of\nconditional parity.\n1. NON-DISCRIMINATION IN MACHINE LEARNING\nAs automated decision systems permeate our world, the problem of implicit\nbiases in these systems have become more serious. Machine learning algorithms\nare routinely used to make decisions in credit, criminal justice, and education, all\nof which are domains protected by anti-discrimination law. Although automated\ndecision systems seem to eliminate the biases of a human decision maker, they\nmay perpetuate or even exacerbate biases in the data.\nFor example, consider an advertising platform which uses demographic infor-\nmation of visitors to a website to decide which credit card o\u000bers to show \frst-time\nvisitors. If the system is trained on historical data where minority visitors were\ngiven less advantageous o\u000bers, the system may steer similar visitors to less ad-\nvantageous o\u000bers, which is illegal (Steel and Angwin, 2010).\nIn response, the scienti\fc community has proposed several formal de\fnitions\nof non-discrimination and various approaches to ensure algorithms are non-dis-\ncriminatory. Unfortunately, the myriad of de\fnitions and approaches hinders the\nadoption of this work by practitioners: they must choose from the growing list of\nde\fnitions and approaches, and there is often no clear choice.\nIn light of this plethora of de\fnitions, we identify a general notion of non-\ndiscrimination in Section 2 that not only includes many recently proposed de\fni-\n",
    "title": "On conditional parity as a notion of non-discrimination in machine\n  learning",
    "abstract": "We identify conditional parity as a general notion of non-discrimination in\nmachine learning. In fact, several recently proposed notions of\nnon-discrimination, including a few counterfactual notions, are instances of\nconditional parity. We show that conditional parity is amenable to statistical\nanalysis by studying randomization as a general mechanism for achieving\nconditional parity and a kernel-based test of conditional parity.",
    "link": "http://arxiv.org/abs/1706.08519v1",
    "published": "2017-06-26T17:41:20Z"
  },
  "33": {
    "pdf_path": "data/pdfs/machine learning_paper_518.pdf",
    "text_excerpt": "□ □□             \nMachine Learning, 25, 151–193 (1996)\nc°1996 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\nClassicLearning\nMICHAEL FRAZIER * frazier@cs.acu.edu\nComputer Science Department, Abilene Christian University, Abilene, TX 79699\nLEONARD PITT ** pitt@cs.uiuc.edu\nDepartment of Computer Science, University of Illinois at Urbana-Champaign\n1304 W. Springﬁeld Avenue, Urbana, IL 61801\nEditor:Thomas Hancock\nAbstract. Descriptionlogics ,alsocalled terminologicallogics ,arecommonlyusedinknowledge-basedsystems\ntodescribeobjectsandtheirrelationships. Weinvestigatethelearnabilityofatypicaldescriptionlogic, Classic,\nandshowthat Classic sentencesarelearnableinpolynomialtimeintheexactlearningmodelusingequivalence\nqueries and membership queries (which are in essence, “subsumption queries”—we show a prediction hardnessresult for the more traditional membership queries that convey information about speciﬁc individuals).\nWe show that membership queries alone are insufﬁcient for polynomial time learning of Classic sentences.\nCombinedwithearliernegativeresults(Cohen & Hirsh, 1994a)showingthat,givenstandardcomplexitytheoreticassumptions, equivalence queries alone are insufﬁcient (or random examples alone in the PAC setting are insuf-ﬁcient), this shows that both sources of information are necessary for efﬁcient learning in that neither type aloneis sufﬁcient. In addition, we show that a modiﬁcation of the algorithm deals robustly with persistent malicioustwo-sidedclassiﬁcationnoiseinthemembershipquerieswiththeprobabilityofamisclassiﬁcationboundedbelow1/2. Other extensions are considered.\nKeywords: Descriptionlogic,polynomial-timelearning, Classic,subsumption,queries,knowledgeacquisition\n1. Introduction\nWeaddresstheproblemofefﬁcientknowledgeacquisitionfromthevantagepointofcom-\nputational learning theory. Traditionally, computational learning theory has focused onpropositional domains. We investigate learning in the ﬁrst-order domain of description\nlogicsorte",
    "title": "Classic Learning",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1026443024002",
    "published": "2003-11-06T11:45:40Z"
  },
  "34": {
    "pdf_path": "data/pdfs/machine learning_paper_206.pdf",
    "text_excerpt": "Privacy Risk in Machine Learning:\nAnalyzing the Connection to Over\ftting\u0003\nSamuel YeomyIrene GiacomellizMatt FredriksonySomesh Jhaz\nyCarnegie Mellon University,zUniversity of Wisconsin{Madison\nfsyeom,mfredrikg@cs.cmu.edu,figiacomelli,jhag@cs.wisc.edu\nAbstract\nMachine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A\ngrowing body of prior work demonstrates that models produced by these algorithms may leak speci\fc\nprivate information in the training data to an attacker, either through the models' structure or their\nobservable behavior. However, the underlying cause of this privacy risk is not well understood beyond a\nhandful of anecdotal accounts that suggest over\ftting and in\ruence might play a role.\nThis paper examines the e\u000bect that over\ftting and in\ruence have on the ability of an attacker to learn\ninformation about the training data from machine learning models, either through training set member-\nship inference orattribute inference attacks. Using both formal and empirical analyses, we illustrate\na clear relationship between these factors and the privacy risk that arises in several popular machine\nlearning algorithms. We \fnd that over\ftting is su\u000ecient to allow an attacker to perform membership\ninference and, when the target attribute meets certain conditions about its in\ruence, attribute inference\nattacks. Interestingly, our formal analysis also shows that over\ftting is not necessary for these attacks\nand begins to shed light on what other factors may be in play. Finally, we explore the connection between\nmembership inference and attribute inference, showing that there are deep connections between the two\nthat lead to e\u000bective new attacks.\n1 Introduction\nMachine learning has emerged as an important technology, enabling a wide range of applications including\ncomputer vision, machine translation, health analytics, and advertising, among others. The fact that many\ncompelling applications of this technology involve the collect",
    "title": "Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting",
    "abstract": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",
    "link": "https://www.semanticscholar.org/paper/c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7",
    "published": "2017-09-05"
  },
  "35": {
    "pdf_path": "data/pdfs/machine learning_paper_60.pdf",
    "text_excerpt": " \nThis paper is submitted to IEEE for possible publication  Analysis of Software Engineering for Agile \nMachine Learning Projects  \n \n \nKushal Singla, Joy Bose , Chetan Naik  \nSamsung R&D Institute  \nBangalore, India  \nkushal.s@samsung.com\n \n  \n  \n \nAbstract— The number of machine learning, artificial \nintell igence or data scie nce related software engineering \nprojects using Agile methodology is increasing. However, there \nare very few studies on how such projects work in practice. In \nthis p aper, we analyze project issues tracking data taken from \nScrum (a popular tool for Agile) for several  machine learning \nprojects. We compare this data with corresponding data from \nnon-machine learning projects, in an attempt to analyze how \nmachine learning projects are executed differently from \nnormal software engineering projects. On analysis, w e find \nthat machine learning project issues use different kinds of \nwords to describe issues, have higher number of exploratory or \nresearch oriented tasks as compared to impleme ntation tasks, \nand have a higher number of issues in the product backlog \nafter e ach sprint, deno ting that it is more difficult to estimate \nthe duration of machine learning pr oject related tasks in \nadvance. After analyzing this data, we pr opose a few ways in \nwhich Agile machine learning projects can be better logged \nand executed , given  their differences with normal software \nengineering pr ojects.  \nKeywords — scrum, machine learning project, software \nengineerin g, agile methodology  \nI. INTRODUCTION  \nThe number of software engineering projects having \nsome artificial intelligence (AI), machine le arning (ML) or \ndata sc ience (DS) related component is growing. One study \nforecast that the spending on machine learning projects \nwould grow from $12 bi llion in 2017 to $58 Billion by 2021 \n[1].  \nAgile methodology [2] has been very popular recently as \na mean s for software development, because of advantages \nsuch as flexibility and rapid pr",
    "title": "Analysis of Software Engineering for Agile Machine Learning Projects",
    "abstract": "The number of machine learning, artificial intelligence or data science\nrelated software engineering projects using Agile methodology is increasing.\nHowever, there are very few studies on how such projects work in practice. In\nthis paper, we analyze project issues tracking data taken from Scrum (a popular\ntool for Agile) for several machine learning projects. We compare this data\nwith corresponding data from non-machine learning projects, in an attempt to\nanalyze how machine learning projects are executed differently from normal\nsoftware engineering projects. On analysis, we find that machine learning\nproject issues use different kinds of words to describe issues, have higher\nnumber of exploratory or research oriented tasks as compared to implementation\ntasks, and have a higher number of issues in the product backlog after each\nsprint, denoting that it is more difficult to estimate the duration of machine\nlearning project related tasks in advance. After analyzing this data, we\npropose a few ways in which Agile machine learning projects can be better\nlogged and executed, given their differences with normal software engineering\nprojects.",
    "link": "http://arxiv.org/abs/1912.07323v1",
    "published": "2019-12-16T12:40:26Z"
  },
  "36": {
    "pdf_path": "data/pdfs/machine learning_paper_392.pdf",
    "text_excerpt": "Machine Learning 5, 233-237 (1990)\n© 1990 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\nEditorial\nAdvice to Machine Learning Authors\nThis editorial contains suggestions to authors of papers in the area of machine learning,\nalthough much of it applies to the broader field of artificial intelligence. I have distilled\nthese comments from my five-year experience as an editor of Machine Learning, focusing\non problems that tended to recur in different papers. Many comments are slanted toward\npapers that describe running systems, but others will be useful for different types of papers.\nAuthors should focus on those suggestions relevant to their own research emphasis.\nI have divided the suggestions into a number of categories, which should be self-explanatory.\nI expect most readers will agree with many of the points, but undoubtedly some will be\nmore controversial. Despite this, I believe that listing them explicitly in this manner will\nat least encourage authors to think about the issues before drafting their papers, and thus\nreduce the need for revisions at later dates.\nContent\n• State the goals of your research and the criteria by which readers should evaluate your\napproach. Categorize the paper in terms of some familiar class; e.g., a theoretical analysis,\na comparative experimental study, a description of some new learning algorithm, or a\ncomputational model of human learning.\n• Specify the performance and learning tasks that are the focus of your research, clearly\ndistinguishing between the two aspects. If there is no performance system, propose some\nother means of evaluating the learning behavior.\n• Describe the representation and organization of your system's knowledge, along with\nthe representation of training data. Include examples of each in the paper.\n• Explain both the performance and learning components of your system in enough detail\nthat readers can reimplement them. If possible, include both a pseudocode description\nand an extended ex",
    "title": "Editorial: Advice to Machine Learning Authors",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022647305786",
    "published": "2003-04-04T16:55:36Z"
  },
  "37": {
    "pdf_path": "data/pdfs/machine learning_paper_436.pdf",
    "text_excerpt": "Machin e Learnin g 3: 95-99 , 1988\n© 1988 Kluwe r Academi c Publisher s - Manufacture d in The Netherland s\nGUES T EDITORIA L\nGeneti c Algorithm s and Machin e Learnin g\nMetaphor s for learnin g\nTher e is no a priori  reaso n why machin e learnin g mus t borro w from nature .\nA field coul d exist , complet e wit h well-define d algorithms , dat a structures ,\nand theorie s of learning , withou t once referrin g to organisms , cognitiv e or\ngeneti c structures , and psychologica l or evolutionar y theories . Yet at the end\nof the day, with the positio n paper s written , the computer s plugge d in, and\nthe program s debugged , a learnin g edific e devoi d of natura l metapho r woul d\nlack something . It woul d ignor e the fact that all thes e creation s hav e becom e\npossibl e only afte r thre e billio n year s of evolutio n on this planet . It woul d\nmiss the poin t tha t the very idea s of adaptatio n and learnin g are concept s\ninvente d by the mos t recen t representative s of the specie s Homo  sapiens  from\nthe carefu l observatio n of themselve s and life aroun d them . It woul d miss the\npoint that natura l example s of learnin g and adaptatio n are treasur e trove s of\nrobus t procedure s and structures .\nFortunately , the field of machin e learnin g does rely upo n nature' s bount y\nfor bot h inspiratio n and mechanism . Man y machin e learnin g system s now\nborro w heavil y from curren t thinkin g in cognitiv e science , and rekindle d in-\nteres t in neura l network s and connectionis m is evidenc e of seriou s mechanisti c\nand philosophica l current s runnin g throug h the field . Anothe r area wher e nat-\nural exampl e has bee n tappe d is in wor k on genetic  algorithms  (GAs ) and\ngenetics-base d machin e learning . Roote d in the earl y cybernetic s movemen t\n(Holland , 1962) , progres s has been mad e in both theor y (Holland , 1975 ; Hol-\nland, Holyoak , Nisbett , & Thagard , 1986 ) and applicatio n (Goldberg , 1989 ;\nGrefenstette , 1985 , 1987 ) to",
    "title": "Genetic Algorithms and Machine Learning",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022602019183",
    "published": "2003-04-04T16:55:36Z"
  },
  "38": {
    "pdf_path": "data/pdfs/machine learning_paper_393.pdf",
    "text_excerpt": "Machine Learning 1: 141-144, 1986\n© 1986 Kluwer Academic Publishers, Boston - Manufactured in The Netherlands\nEditorial: The Terminology of Machine Learning\nScience is a communal endeavor, and communication is essential to its effective\noperation. Journals play an important role in this process, and the primary goal of\nMachine Learning is to improve communication between researchers in our emerging\nfield. One way to maximize this communication is to encourage clear and consistent\nterminology, and this is especially important in new fields like machine learning. An\nauthor's choice of words can have a major impact on his paper's ability to com-\nmunicate, and this makes terminology an important concern for both the authors and\nthe editors of any journal.\nThe fields of artificial intelligence and cognitive science have a long history of ter-\nminological disputes. Researchers in both fields have been criticized (both from\nwithin and without) for using semantically loaded terms. The pages of journals and\nproceedings abound with statements that an AI system understands natural\nlanguage, that it reasons about some problem, or even that it learns new concepts\nor discovers scientific laws.\nHowever, we should not forget that AI and cognitive science are unique fields.\nUnlike physics and chemistry, they study processes that can be observed through in-\ntrospection (at least to some extent) by all humans. As a result, our everyday\nlanguages already contain terms for most of the mechanisms that we are interested\nin explaining. Thus, it is natural for us to describe our systems as \"understanding\"\nor \"learning\", even when we realize that our current models of these processes only\nshadow the human versions. In contrast, \"hard\" scientists are free to invent entirely\nnew names for concepts, or to use existing terms (such as charm and color) in new\nsenses without fear of confusion.\nBut despite the criticisms, I would argue that AI and related fields are in little\ndanger from their use o",
    "title": "Editorial: The Terminology of Machine Learning",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022840627593",
    "published": "2003-04-04T16:57:10Z"
  },
  "39": {
    "pdf_path": "data/pdfs/machine learning_paper_24.pdf",
    "text_excerpt": " \nTuning Learning Rates with the Cumulative-Learning Constant \n By Nathan Faraj   \nAbstract This paper introduces a novel method for optimizing learning rates in machine learning. A previously unrecognized proportionality between learning rates and dataset sizes is discovered, providing valuable insights into how dataset scale inﬂuences training dynamics. Additionally, a cumulative learning constant is identiﬁed, oﬀering a framework for designing and optimizing advanced learning rate schedules. These ﬁndings have the potential to enhance training eﬃciency and performance across a \nwide\n \nrange\n \nof\n \nmachine\n \nlearning\n \napplications. \n1. Introduction \nThe learning rate is a deep learning hyperparameter that is very important for training models eﬃciently. There has been a lot of research done on optimizing learning rates including sophisticated methods for adaptive learning rates as well as alternating learning rates (Wu et al., 2019). However, the learning rate selection is arbitrary - An optimal learning rate is found without understanding due to the belief that the learning rate is too sensitive to changes in other hyperparameters within the model. However, this paper will present that the learning rate follows some strict proportionality laws. Previous work has been done to compare proportionality of learning rates in reference to batch size (Diego et al., 2020). Therefore, this paper will not be exploring how batch size aﬀects the optimal learning rate. \n1.1 Main Contributions \nIn this paper, we demonstrate that the optimal learning rate is inversely proportional to the size of the dataset. Notably, we show that this proportionality holds regardless of whether the dataset is expanded with new data or repeated data; for instance, doubling the number of epochs and doubling the dataset size are equivalent in this context. Additionally, we introduce the concept of a \"cumulative learning constant,\" a value speciﬁc to a given model architecture. This constant can be",
    "title": "Tuning Learning Rates with the Cumulative-Learning Constant",
    "abstract": "This paper introduces a novel method for optimizing learning rates in machine\nlearning. A previously unrecognized proportionality between learning rates and\ndataset sizes is discovered, providing valuable insights into how dataset scale\ninfluences training dynamics. Additionally, a cumulative learning constant is\nidentified, offering a framework for designing and optimizing advanced learning\nrate schedules. These findings have the potential to enhance training\nefficiency and performance across a wide range of machine learning\napplications.",
    "link": "http://arxiv.org/abs/2505.13457v1",
    "published": "2025-04-30T00:07:48Z"
  },
  "40": {
    "pdf_path": "data/pdfs/machine learning_paper_8.pdf",
    "text_excerpt": "Generated using the oﬃcial AMS L ATEX template v6.1 two-column layout. This work has been submitted for\npublication. Copyright in this work may be transferred without further notice, and this version may no longer be\naccessible.\nA Machine Learning Tutorial for Operational Meteorology, Part I: Traditional Machine\nLearning\nR/a.pc/n.pc/d.pc/y.pc J. C/h.pc/a.pc/s.pc/e.pca,b,c, D/a.pc/v.pc/i.pc/d.pc R. H/a.pc/r.pc/r.pc/i.pc/s.pc/o.pc/n.pcb,d,e, A/m.pc/a.pc/n.pc/d.pc/a.pc B/u.pc/r.pc/k.pc/e.pcb,c, G/a.pc/r.pc/y.pc M. L/a.pc/c.pc/k.pc/m.pc/a.pc/n.pc/n.pcf/a.pc/n.pc/d.pc A/m.pc/y.pc M/c.pcG/o.pc/v.pc/e.pc/r.pc/n.pca,b,c\naSchool of Computer Science, University of Oklahoma, Norman OK USA\nbSchool of Meteorology, University of Oklahoma, Norman OK USA\ncNSF AI Institute for Research on Trustworthy AI in Weather, Climate, and Coastal Oceanography, University of Oklahoma, Norman OK\nUSA\ndCooperative Institute for Severe and High-Impact Weather Research and Operations, University of Oklahoma, Norman OK USA\neNOAA/NWS/Storm Prediction Center, Norman, Oklahoma\nfDepartment of Marine, Earth, and Atmospheric Sciences, North Carolina State University, Raleigh, North Carolina\nABSTRACT: Recently, theuseofmachinelearninginmeteorologyhas increasedgreatly. Whilemany machinelearningmethodsarenot\nnew,universityclassesonmachinelearningarelargelyunavailabletometeorologystudentsandarenotrequiredtobecomeameteorologist.\nThelackofformalinstructionhascontributedtoperceptionthatmachinelearningmethodsare’blackboxes’andthusend-usersarehesitant\nto apply the machine learning methods in their every day workﬂow. To reduce the opaqueness of machine learning methods and lower\nhesitancytowardsmachinelearninginmeteorology,thispaperprovidesasurveyofsomeofthemostcommonmachinelearningmethods.\nA familiar meteorological example is used to contextualize the machine learning methods while also discussing machine learning topics\nusing plain language. The following machine learning methods are demonstrated: linear regressio",
    "title": "A Machine Learning Tutorial for Operational Meteorology, Part I:\n  Traditional Machine Learning",
    "abstract": "Recently, the use of machine learning in meteorology has increased greatly.\nWhile many machine learning methods are not new, university classes on machine\nlearning are largely unavailable to meteorology students and are not required\nto become a meteorologist. The lack of formal instruction has contributed to\nperception that machine learning methods are 'black boxes' and thus end-users\nare hesitant to apply the machine learning methods in their every day workflow.\nTo reduce the opaqueness of machine learning methods and lower hesitancy\ntowards machine learning in meteorology, this paper provides a survey of some\nof the most common machine learning methods. A familiar meteorological example\nis used to contextualize the machine learning methods while also discussing\nmachine learning topics using plain language. The following machine learning\nmethods are demonstrated: linear regression; logistic regression; decision\ntrees; random forest; gradient boosted decision trees; naive Bayes; and support\nvector machines. Beyond discussing the different methods, the paper also\ncontains discussions on the general machine learning process as well as best\npractices to enable readers to apply machine learning to their own datasets.\nFurthermore, all code (in the form of Jupyter notebooks and Google Colaboratory\nnotebooks) used to make the examples in the paper is provided in an effort to\ncatalyse the use of machine learning in meteorology.",
    "link": "http://arxiv.org/abs/2204.07492v2",
    "published": "2022-04-15T14:48:04Z"
  },
  "41": {
    "pdf_path": "data/pdfs/machine learning_paper_20.pdf",
    "text_excerpt": "Mathematical Perspective of Machine Learning\nYarema Boryshchak, PhD\nJuly 6, 2020\nAbstract\nWe take a closer look at some theoretical challenges of Machine Learning as a func-\ntion approximation, gradient descent as the default optimization algorithm, limitations of\nﬁxed length and width networks and a diﬀerent approach to RNNs from a mathematical\nperspective.\n1 Introduction\nIn the nutshell the idea of training a neural network ( NN) is equivalent to the problem\napproximation of a given function, f, with the domain, D, and codomain, C,\nf:D!C (1)\nwhich depends on some data of size N2Nofk-dimensional input vectors, ~ xj2D\u0012 Rk\nandl-dimensional output (label) vectors, ~ yj2C\u0012 Rl, by a composition of functions of the\nform\n~Pi(~ zi\u00001\u0001~ wi) = (Pi(~ zi\u00001\u0001~ wi);:::;Pi(~ zi\u00001\u0001~ wi)); (2)\nwhere Piis called an activation function of layer i,~ zi\u00001is an output vector of the layer\ni\u00001, and ~ wiis called the weight vector of layer i. Once the size of each layer and the\nchoice of each activation function is made, one usually uses, so called, back propagation\nalgorithm, adjusting the values of each weight vector according to some type of gradient\ndescent rule. In other words, one is trying to solve an optimization problem, minimizing\nthe \"diﬀerence norm\"\nC:=\r\r\rf\u0000~f\r\r\r\nd;where ~f=~Pr\u0010\n~Pr\u00001(:::~P1)\u0011\n(3)\nandr2Nis the number of layers of the neural network.\nSo apriori, we are making a choice of the function ~fof weights~ w1;:::;~ wr. Note that\nthe dimension of each vector ~ wiis the size of the layer i. To simplify the problem we may\n1arXiv:2007.01503v1  [cs.LG]  3 Jul 2020always ﬁnd the maximum, m, of the size layers, and assume that each ~ wi2Rm. We are\nimplicitly assuming that for each i= 1;:::;r,~Pi(~0) =~0.1\n2 Existence of function fand the toll of cost function C\nFirst thing to consider, given a labeled data set f(~ xj;~ yj) :j= 1;:::;Ng, if there is a\nrepresentation function fsuch that f(~ xj) =~ yjfor allj= 1;:::;N. This important step is\noften overlooked in practice. In theory, ",
    "title": "Mathematical Perspective of Machine Learning",
    "abstract": "We take a closer look at some theoretical challenges of Machine Learning as a\nfunction approximation, gradient descent as the default optimization algorithm,\nlimitations of fixed length and width networks and a different approach to RNNs\nfrom a mathematical perspective.",
    "link": "http://arxiv.org/abs/2007.01503v1",
    "published": "2020-07-03T05:26:02Z"
  },
  "42": {
    "pdf_path": "data/pdfs/machine learning_paper_360.pdf",
    "text_excerpt": "Ensuring Fairness in Machine Learning to Advance Health \nEquity\nAlvin Rajkomar, MD*,\nGoogle, Mountain View, and University of California, San Francisco, San Francisco, California\nMichaela Hardt, PhD*,\nGoogle, Mountain View, California\nMichael D. Howell, MD, MPH ,\nGoogle, Mountain View, California\nGreg Corrado, PhD , and\nGoogle, Mountain View, California\nMarshall H. Chin, MD, MPH\nUniversity of Chicago, Chicago, Illinois\nAbstract\nMachine learning is used increasingly in clinical care to improve diagnosis, treatment selection, \nand health system efficiency. Because machine-learning models learn from historically collected \ndata, populations that have experienced human and structural biases in the past—called protected \ngroups —are vulnerable to harm by incorrect predictions or withholding of resources. This article \ndescribes how model design, biases in data, and the interactions of model predictions with \nclinicians and patients may exacerbate health care disparities. Rather than simply guarding against \nthese harms passively, machine-learning systems should be used proactively to advance health \nequity. For that goal to be achieved, principles of distributive justice must be incorporated into \nmodel design, deployment, and evaluation. The article describes several technical implementations \nof distributive justice—specifically those that ensure equality in patient outcomes, performance, \nand resource allocation—and guides clinicians as to when they should prioritize each principle. \nMachine learning is providing increasingly sophisticated decision support and population-level \nmonitoring, and it should encode principles of justice to ensure that models benefit all patients.\nCorresponding Author:  Alvin Rajkomar, MD, Google LLC, 1600 Amphitheatre Way, Mountain View, CA 94043; \nalvinrajkomar@google.com.*Drs. Rajkomar and Hardt contributed equally to this work.\nCurrent author addresses and author contributions are available at Annals.org .\nCurrent Author Addresses:  Drs",
    "title": "Ensuring Fairness in Machine Learning to Advance Health Equity",
    "abstract": "Machine learning can identify the statistical patterns of data generated by tens of thousands of physicians and billions of patients to train computers to perform specific tasks with sometimes superhuman ability, such as detecting diabetic eye disease better than retinal specialists (1). However, historical data also capture patterns of health care disparities, and machine-learning models trained on these data may perpetuate these inequities. This concern is not just academic. In a model used to predict future crime on the basis of historical arrest records, African American defendants who did not reoffend were classified as high risk at a substantially higher rate than white defendants who did not reoffend (2, 3). Similar biases have been observed in predictive policing (4) and identifying which calls to a child protective services agency required an in-person investigation (5, 6). The implications for health care led the American Medical Association to pass policy recommendations to promote development of thoughtfully designed, high-quality, clinically validated health care AI [artificial or augmented intelligence, such as machine learning] that . . . identifies and takes steps to address bias and avoids introducing or exacerbating health care disparities including when testing or deploying new AI tools on vulnerable populations (7). We argue that health care organizations and policymakers should go beyond the American Medical Association's position of doing no harm and instead proactively design and use machine-learning systems to advance health equity. Whereas much health disparities work has focused on discriminatory decision making and implicit biases by clinicians, policymakers, organizational leaders, and researchers are increasingly focusing on the ill health effects of structural racism and classismhow systems are shaped in ways that harm the health of disempowered, marginalized populations (8). For example, the United States has a shameful history of purposive decisions by government and private businesses to segregate housing. Zoning laws, discrimination in mortgage lending, prejudicial practices by real estate agents, and the ghettoization of public housing all contributed to the concentration of urban African Americans in inferior housing that has led to poor health (9, 10). Even when the goal of decision makers is not outright discrimination against disadvantaged groups, actions may lead to inequities. For example, if the goal of a machine-learning system is to maximize efficiency, that might come at the expense of disadvantaged populations. As a society, we value health equity. For example, the Healthy People 2020 vision statement aims for a society in which all people live long, healthy lives, and one of the mission's goals is to achieve health equity, eliminate disparities, and improve the health of all groups (11). The 4 classic principles of Western clinical medical ethics are justice, autonomy, beneficence, and nonmaleficence. However, health equity will not be attained unless we purposely design our health and social systems, which increasingly will be infused with machine learning (12), to achieve this goal. To ensure fairness in machine learning, we recommend a participatory process that involves key stakeholders, including frequently marginalized populations, and considers distributive justice within specific clinical and organizational contexts. Different technical approaches can configure the mathematical properties of machine-learning models to render predictions that are equitable in various ways. The existence of mathematical levers must be supplemented with criteria for when and why they should be usedeach tool comes with tradeoffs that require ethical reasoning to decide what is best for a given application. We propose incorporating fairness into the design, deployment, and evaluation of machine-learning models. We discuss 2 clinical applications in which machine learning might harm protected groups by being inaccurate, diverting resources, or worsening outcomes, especially if the models are built without consideration for these patients. We then describe the mechanisms by which a model's design, data, and deployment may lead to disparities; explain how different approaches to distributive justice in machine learning can advance health equity; and explore what contexts are more appropriate for different equity approaches in machine learning. Case Study 1: Intensive Care Unit Monitoring A common area of predictive modeling research focuses on creating a monitoring systemfor example, to warn a rapid response team about inpatients at high risk for deterioration (1315), requiring their transfer to an intensive care unit within 6 hours. How might such a system inadvertently result in harm to a protected group? In this thought experiment, we consider African Americans as a protected group. To build the model, our hypothetical researchers collected historical records of patients who had clinical deterioration and those who did not. The model acts like a diagnostic test of risk for intensive care unit transfer. However, if too few African American patients were included in the training datathe data used to construct the modelthe model might be inaccurate for them. For example, it might have a lower sensitivity and miss more patients at risk for deterioration. African American patients might be harmed if clinical teams started relying on alerts to identify at-risk patients without realizing that the prediction system underdetects patients in that group (automation bias) (16). If the model had a lower positive predictive value for African Americans, it might also disproportionately harm them through dismissal biasa generalization of alert fatigue in which clinicians may learn to discount or dismiss alerts for African Americans because they are more likely to be false-positive (17). Case Study 2: Reducing Length of Stay Imagine that a hospital created a model with clinical and social variables to predict which inpatients might be discharged earliest so that it could direct limited case management resources to them to prevent delays. If residence in ZIP codes of socioeconomically depressed or predominantly African American neighborhoods predicted greater lengths of stay (18), this model might disproportionately allocate case management resources to patients from richer, predominantly white neighborhoods and away from African Americans in poorer ones. What Is Machine Learning? Traditionally, computer systems map inputs to outputs according to manually specified ifthen rules. With increasingly complex tasks, such as language translation, manually specifying rules becomes infeasible, and instead the mapping (or model) is learned by the system given only input examples represented through a set of features together with their desired output, referred to as labels. The quality of a model is assessed by computing evaluation metrics on data not used to build the model, such as sensitivity, specificity, or the c-statistic, which measures the ability of a model to distinguish patients with a condition from those without it (19, 20). Once the model's quality is deemed satisfactory, it can be deployed to make predictions on new examples for which the label is unknown when the prediction is made. The quality of the models on retrospective data must be followed with tests of clinical effectiveness, safety, and comparison with current practice, which may require clinical trials (21). Traditionally, statistical models for prediction, such as the pooled-cohort equation (22), have used few variables to predict clinical outcomes, such as cardiovascular risk (23). Modern machine-learning techniques, however, can consider many more features. For example, a recent model to predict hospital readmissions examined hundreds of thousands of pieces of information, including the free text of clinical notes (24). Complex data and models can drive more personalized and accurate predictions but may also make algorithms hard to understand and trust (25). What Can Cause a Machine-Learning System to Be Unfair? The Glossary lists key biases in the design, data, and deployment of a machine-learning model that may perpetuate or exacerbate health care disparities if left unchecked. The Figure reveals how the various biases relate to one another and how the interactions of model predictions with clinicians and patients may exacerbate health care disparities. Biases may arise during the design of a model. For example, if the label is marred by health care disparities, such as predicting the onset of clinical depression in environments where protected groups have been systematically misdiagnosed, then the model will learn to perpetuate this disparity. This represents a generalization of test-referral bias (26) that we refer to as label bias. Moreover, the data on which the model is developed may be biased. Data on patients in the protected group might be distributed differently from those in the nonprotected group because of biological or nonbiological variation (9, 27). For example, the data may not contain enough examples from a group to properly tailor the predictions to them (minority bias) (28), or the data set of the protected group may be less informative because features are missing not at random as a result of more fragmented care (29, 30). Glossary Figure. Conceptual framework of how various biases relate to one another. During model development, differences in the distribution of features used to predict a label between the protected and nonprotected groups may bias a model to be less accurate for protected groups. Moreover, the data used to develop a model may not generalize to the data used during model deployment (trainingserving skew). Biases in model design and data affect patient outcomes through the model's interaction with clinicians and patients. The immediate effect of these differences is that the model may ",
    "link": "https://www.semanticscholar.org/paper/82a5a84528a7ca0409f75e2211a3b33a217e9bac",
    "published": "2018-12-04"
  },
  "43": {
    "pdf_path": "data/pdfs/machine learning_paper_96.pdf",
    "text_excerpt": "Optimization Methods for Large-Scale Machine Learning\nL\u0013 eon Bottou\u0003Frank E. CurtisyJorge Nocedalz\nFebruary 12, 2018\nAbstract\nThis paper provides a review and commentary on the past, present, and future of numerical\noptimization algorithms in the context of machine learning applications. Through case studies\non text classi\fcation and the training of deep neural networks, we discuss how optimization\nproblems arise in machine learning and what makes them challenging. A major theme of our\nstudy is that large-scale machine learning represents a distinctive setting in which the stochastic\ngradient (SG) method has traditionally played a central role while conventional gradient-based\nnonlinear optimization techniques typically falter. Based on this viewpoint, we present a com-\nprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior,\nand highlight opportunities for designing algorithms with improved performance. This leads to\na discussion about the next generation of optimization methods for large-scale machine learning,\nincluding an investigation of two main streams of research on techniques that diminish noise in\nthe stochastic directions and methods that make use of second-order derivative approximations.\nContents\n1 Introduction 3\n2 Machine Learning Case Studies 4\n2.1 Text Classi\fcation via Convex Optimization . . . . . . . . . . . . . . . . . . . . . . . 4\n2.2 Perceptual Tasks via Deep Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 6\n2.3 Formal Machine Learning Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n3 Overview of Optimization Methods 13\n3.1 Formal Optimization Problem Statements . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.2 Stochastic vs. Batch Optimization Methods . . . . . . . . . . . . . . . . . . . . . . . 15\n3.3 Motivation for Stochastic Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.4 Beyond SG: Noise Reduction and Second-Order Methods . . . . . . . ",
    "title": "Optimization Methods for Large-Scale Machine Learning",
    "abstract": "This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.",
    "link": "https://www.semanticscholar.org/paper/d21703674ae562bae4a849a75847cdd9ead417df",
    "published": "2016-06-15"
  },
  "44": {
    "pdf_path": "data/pdfs/machine learning_paper_627.pdf",
    "text_excerpt": "Machine Learning, 51, 137–163, 2003\nc/circlecopyrt2003 Kluwer Academic Publishers. Manufactured in The Netherlands.\nLearning from Different Teachers\nDANA ANGLUIN dana.angluin@yale.edu\nComputer Science Department, Yale University, P .O. Box 208285, New Haven, CT 06520-8285, USA\nM¯ARTIN ¸ˇS KRIK ¸ IS martins.krikis@intel.com\nIntel Corporation, 75 Reed Rd., Hudson, MA 01749, USA\nEditor: Lisa Hellerstein\nAbstract. We introduce a new model of a learner learning an unknown concept from examples with a teacher’s\nhelp. In such models, “outright coding” refers to a situation in which the teacher sends the learner a representationof the concept, either directly or encoded via the examples. Previous models have used adversarial learners oradversarial teachers to try to prevent outright coding. Our model is an attempt to reﬂect more directly some of thereasons that outright coding is not a common mode of human learning.\nWe model the learner as a Turing machine with oracle access to another programming system, called its “function\nbox.” The programming system in its function box is initially unknown to the learner. The target concept is a partialrecursive function and the goal of the learner is to ﬁnd in its function box a function that is equal to or extendsthe target concept. We exhibit a class of learner/teacher pairs in which the learner can learn any partial recursivefunction, provided that the learner’s function box is “not too much slower” than the teacher’s. This result is shownnot to hold if the learner’s function box can contain an arbitrary acceptable programming system.\nKeywords: concept learning, teacher, programming system, hint, complexity bound, oracle Turing machine\n1. Introduction\nIn this paper we introduce and explore a new model for the situation of a teacher attempting\nto teach a concept to a learner. Several different formal models of teaching have beenstudied, addressing different aspects of the problem, for example, the work of Goldman andKearns (1995) an",
    "title": "Learning from Different Teachers",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022854802097",
    "published": "2003-04-04T16:57:10Z"
  },
  "45": {
    "pdf_path": "data/pdfs/machine learning_paper_367.pdf",
    "text_excerpt": "Supporting Information \nMol2vec: Unsupervised Machine Learning Approach  \nwith Chemical Intuition \n \nSabrina Jaeger, 1 Simone Fulle, 1* Samo Turk 1*  \n1 BioMed X Innovation Center, Im Neuenheimer Feld 51 5, 69120 Heidelberg, Germany. \n* Corresponding authors: fulle@bio.mx,  turk@bio.mx \n \n Table S1:  Evaluation of different Word2vec hyperparameters. \nMethod  Architecture  Window  Dimension  AUC  Sensitivity  Specificity  \nRF  CBOW  5 100  0.83 ± 0.05  0.20 ± 0.14  1.00 ± 0.01  \nGBM  CBOW  5 100  0.81 ± 0.05  0.35 ± 0.14  0.98 ± 0.02  \nDNN  CBOW  5 100  0.81 ± 0.05  0.31 ± 0.15  0.98 ± 0.02  \nRF  CBOW  5 300  0.83 ± 0.05  0.21 ± 0.14  1.00 ± 0.01  \nGBM  CBOW  5 300  0.82 ± 0.05  0.36 ± 0.14  0.98 ± 0.02  \nDNN  CBOW  5 300  0.82 ± 0.05  0.28 ± 0.15  0.98 ± 0.02  \nRF  CBOW  10  100  0.83 ± 0.05  0.19 ± 0.14  1.00 ± 0.01  \nGBM  CBOW  10  100  0.80 ± 0.05  0.34 ± 0.14  0.98 ± 0.02  \nDNN  CBOW  10  100  0.81 ± 0.04  0.30 ± 0.14  0.98 ± 0.02  \nRF  CBOW  10  300  0.83 ± 0.05  0.19 ± 0.14  1.00 ± 0.01  \nGBM  CBOW  10  300  0.82 ± 0.05  0.36 ± 0.14  0.98 ± 0.02  \nDNN  CBOW  10  300  0.82 ± 0.05  0.30 ±0.15  0.98 ± 0.02  \nRF  Skip -gram  10  100  0.84 ± 0.05  0.20 ± 0.14  1.00 ± 0.01  \nGBM  Skip -gram  10  100  0.82 ± 0.05  0.36 ± 0.15  0.98 ± 0.02  \nDNN  Skip -gram  10  100  0.82 ± 0.05  0.31 ± 0.15  0.98 ± 0.02  \nRF  Skip -gram  10  300  0.85 ± 0.05  0.20 ± 0.14  1.00 ± 0.01  \nGBM  Skip -gram  10  300  0.84 ± 0.05  0.38 ± 0.14  0.98 ± 0.02  \nDNN  Skip -gram  10  300  0.83 ± 0.04  0.33 ± 0.15  0.98 ± 0.02  \nRF  Skip -gram  20  100  0.84 ± 0.04  0.19 ± 0.14  1.00 ± 0.01  \nGBM  Skip -gram  20  100  0.82 ± 0.05  0.37 ± 0.14  0.98 ± 0.02  \nDNN  Skip -gram  20  100  0.82 ± 0.05  0.29 ± 0.17  0.98 ± 0.02  \nRF  Skip -gram  20  300  0.85 ± 0.05  0.20 ± 0.14  1.00 ± 0.01  \nGBM  Skip -gram  20  300  0.84 ± 0.05  0.38 ± 0.13  0.98 ± 0.02  \nDNN  Skip -gram  20  300  0.82 ± 0.05  0.31 ± 0.18  0.98 ± 0.03  \n \n Table S2:  Pairwise cosine distance of Mol2vec vectors of ami no acids. \n TRP PH",
    "title": "Mol2vec: Unsupervised Machine Learning Approach with Chemical Intuition",
    "abstract": "Inspired by natural language processing techniques, we here introduce Mol2vec, which is an unsupervised machine learning approach to learn vector representations of molecular substructures. Like the Word2vec models, where vectors of closely related words are in close proximity in the vector space, Mol2vec learns vector representations of molecular substructures that point in similar directions for chemically related substructures. Compounds can finally be encoded as vectors by summing the vectors of the individual substructures and, for instance, be fed into supervised machine learning approaches to predict compound properties. The underlying substructure vector embeddings are obtained by training an unsupervised machine learning approach on a so-called corpus of compounds that consists of all available chemical matter. The resulting Mol2vec model is pretrained once, yields dense vector representations, and overcomes drawbacks of common compound feature representations such as sparseness and bit collisions. The prediction capabilities are demonstrated on several compound property and bioactivity data sets and compared with results obtained for Morgan fingerprints as a reference compound representation. Mol2vec can be easily combined with ProtVec, which employs the same Word2vec concept on protein sequences, resulting in a proteochemometric approach that is alignment-independent and thus can also be easily used for proteins with low sequence similarities.",
    "link": "https://www.semanticscholar.org/paper/88a99980f1f7eeac5f36be2e4601898988bdf937",
    "published": "2018-01-10"
  },
  "46": {
    "pdf_path": "data/pdfs/machine learning_paper_32.pdf",
    "text_excerpt": "Mixed integer programming formulation of\nunsupervised learning\nArturo Berrones-Santos\nUniversidad Aut\u0013 onoma de Nuevo Le\u0013 on\nFacultad de Ingenier\u0013 \u0010a Mec\u0013 anica y El\u0013 ectrica\nPosgrado en Ingenier\u0013 \u0010a de Sistemas\nFacultad de Ciencias F\u0013 \u0010sico Matem\u0013 aticas\nPosgrado en Ciencias con Orientaci\u0013 on en Matem\u0013 aticas\nAP 126, Cd. Universitaria, San Nicol\u0013 as de los Garza, NL 66450, M\u0013 exico\narturo.berronessn@uanl.edu.mx\nJanuary 22, 2020\nAbstract\nA novel formulation and training procedure for full Boltzmann ma-\nchines in terms of a mixed binary quadratic feasibility problem is given.\nAs a proof of concept, the theory is analytically and numerically tested\non XOR patterns.\nKeywords: Boltzamnn machines; Mixed integer programming; Unsu-\npervised learning.\n1 Introduction\nA central open question in machine learning is the e\u000bective handling of unlabeled\ndata [1, 2]. The construction of balanced representative datasets for supervised\nmachine learning for the most part still requires a very close and time consuming\nhuman direction, so the development of e\u000ecient learning from data algorithms\nin an unsupervised fashion is a very active area of research [1, 2]. A general\nframework to deal with unlabeled data is the Boltzmann machine paradigm, in\nwhich is attempted to learn a probability distribution for the patterns in the\ndata without any previous identi\fcation of input and output variables. In its\nmost general setups however, the training of Blotzmann machines is compu-\ntationally intractable [2, 3, 4]. In this contribution is established a relation,\nwhich to the best of my knowledge was previously unknown, between Mixed\nInteger Programing (MIP) and the full Boltzmann machine in binary variables.\nIs hoped that this novel formulation opens the road to more e\u000ecient learning\nalgorithms by taking advantage of the great variety of techniques available for\nMIP.\n1arXiv:2001.07278v1  [cs.LG]  20 Jan 20202 Full Boltzmann machine with data as con-\nstraints\nConsider a network of units with bina",
    "title": "Mixed integer programming formulation of unsupervised learning",
    "abstract": "A novel formulation and training procedure for full Boltzmann machines in\nterms of a mixed binary quadratic feasibility problem is given. As a proof of\nconcept, the theory is analytically and numerically tested on XOR patterns.",
    "link": "http://arxiv.org/abs/2001.07278v1",
    "published": "2020-01-20T23:09:32Z"
  },
  "47": {
    "pdf_path": "data/pdfs/machine learning_paper_176.pdf",
    "text_excerpt": "A survey on missing data in machine \nlearning\nTlamelo Emmanuel* , Thabiso Maupong, Dimane Mpoeleng, Thabo Semong, Banyatsang Mphago and \nOteng Tabona \nIntroduction\nMissing values are usually attributed to: human error when processing data, machine \nerror due to the malfunctioning of equipment, respondents refusal to answer certain \nquestions, drop-out in studies and merging unrelated data [1, 2]. The missing values \nproblem is usually common in all domains that deal with data and causes different issues \nlike performance degradation, data analysis problems and biased outcomes lead by the \ndifferences in missing and complete values [3]. Moreover, the seriousness of missing val -\nues depend in part on how much data is missing, the pattern of missing data, and the \nmechanism underlying the missingness of the data [4]. Missing values can be handled \nby certain techniques including, deletion of instances and replacement with potential \nor estimated values [5–7], a technique denoted as imputation [8]. Several traditional Abstract \nMachine learning has been the corner stone in analysing and extracting information \nfrom data and often a problem of missing values is encountered. Missing values occur \nbecause of various factors like missing completely at random, missing at random or \nmissing not at random. All these may result from system malfunction during data col-\nlection or human error during data pre-processing. Nevertheless, it is important to deal \nwith missing values before analysing data since ignoring or omitting missing values \nmay result in biased or misinformed analysis. In literature there have been several \nproposals for handling missing values. In this paper, we aggregate some of the litera-\nture on missing data particularly focusing on machine learning techniques. We also \ngive insight on how the machine learning approaches work by highlighting the key \nfeatures of missing values imputation techniques, how they perform, their limitations \nand the kind of data",
    "title": "A survey on missing data in machine learning",
    "abstract": "Machine learning has been the corner stone in analysing and extracting information from data and often a problem of missing values is encountered. Missing values occur because of various factors like missing completely at random, missing at random or missing not at random. All these may result from system malfunction during data collection or human error during data pre-processing. Nevertheless, it is important to deal with missing values before analysing data since ignoring or omitting missing values may result in biased or misinformed analysis. In literature there have been several proposals for handling missing values. In this paper, we aggregate some of the literature on missing data particularly focusing on machine learning techniques. We also give insight on how the machine learning approaches work by highlighting the key features of missing values imputation techniques, how they perform, their limitations and the kind of data they are most suitable for. We propose and evaluate two methods, the k nearest neighbor and an iterative imputation method (missForest) based on the random forest algorithm. Evaluation is performed on the Iris and novel power plant fan data with induced missing values at missingness rate of 5% to 20%. We show that both missForest and the k nearest neighbor can successfully handle missing values and offer some possible future research direction.",
    "link": "https://www.semanticscholar.org/paper/e2e8ea9bcdb83deb2787ce89db1b51f7ccb1bd1e",
    "published": "2021-06-17"
  },
  "48": {
    "pdf_path": "data/pdfs/machine learning_paper_64.pdf",
    "text_excerpt": "POWER CONSUMPTION VARIATION OVER ACTIVATION\nFUNCTIONS\nLeon Derczynski\nDepartment of Computer Science\nIT University of Copenhagen\n2300 Denmark\nleod@itu.dk\nABSTRACT\nThe power machine learning models consume when making predictions can be\naffected by a model’s architecture. This paper presents various estimates of power\nconsumption for a range of different activation functions, a core factor in neural\nnetwork model architecture design. Substantial differences in hardware perfor-\nmance exist between activation functions. This difference informs how power\nconsumption in machine learning models can be reduced.\n1 I NTRODUCTION\nThe ﬁeld of deep neural networks has reported strong progress in many problem areas, including\nnatural language processing (NLP), image recognition, and game playing. Many of the advances\nin these areas have been the fruit of using larger and thus more computationally demanding neural\nnetworks. Amodei & Hernandez (2018) ﬁnd that the cost of training doubled every few months be-\ntween the releases of AlexNet (Krizhevsky et al., 2012) and AlphaZero Silver et al. (2018). In NLP,\npower consumption has also risen: Strubell et al. (2019) determine the carbon footprint of a con-\ntemporary machine translation architecure search to be in the order of hundreds of intercontinental\nﬂights, for models that offer only marginal performance improvement.\nThis paper examines activation functions, a core part of neural networks. The activation function is\nthe non-linearity at the core of each network node. It is applied over the input and bias parameters\nat a given node for each inference that a model makes. This makes for a potentially large num-\nber of computation being required to make predictions, predicated on network structure and size.\nWhen it comes to individual calculations, there is also broad variance. The complexity of low-level\ninstructions for each these functions also varies widely, from the simple rectiﬁed linear unit to the\ntranscendental hyperbolic ta",
    "title": "Power Consumption Variation over Activation Functions",
    "abstract": "The power that machine learning models consume when making predictions can be\naffected by a model's architecture. This paper presents various estimates of\npower consumption for a range of different activation functions, a core factor\nin neural network model architecture design. Substantial differences in\nhardware performance exist between activation functions. This difference\ninforms how power consumption in machine learning models can be reduced.",
    "link": "http://arxiv.org/abs/2006.07237v1",
    "published": "2020-06-12T14:40:46Z"
  },
  "49": {
    "pdf_path": "data/pdfs/machine learning_paper_265.pdf",
    "text_excerpt": "When Machine Learning Meets Privacy: A Survey and\nOutlook\nBO LIU∗,University of Technology Sydney, Australia\nMING DING, Data61, CSIRO, Australia\nSINA SHAHAM, The University of Sydney, Australia\nWENNY RAHAYU, La Trobe University, Australia\nFARHAD FAROKHI, The University of Melbourne, Australia\nZIHUAI LIN, The University of Sydney, Australia\nThe newly emerged machine learning (e.g. deep learning) methods have become a strong driving force to\nrevolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance\nsystems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence\nera. It is important to note that the problem of privacy preservation in the context of machine learning is\nquite different from that in traditional data privacy protection, as machine learning can act as both friend and\nfoe. Currently, the work on the preservation of privacy and machine learning (ML) is still in an infancy stage,\nas most existing solutions only focus on privacy problems during the machine learning process. Therefore,\na comprehensive study on the privacy preservation problems and machine learning is required. This paper\nsurveys the state of the art in privacy issues and solutions for machine learning. The survey covers three\ncategories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine\nlearning aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection\nschemes. The current research progress in each category is reviewed and the key challenges are identified.\nFinally, based on our in-depth analysis of the area of privacy and machine learning, we point out future\nresearch directions in this field.\nCCS Concepts: •Security and privacy →Privacy protections ;Social network security and privacy .\nAdditional Key Words and Phrases: machine learning, privacy, deep learning, differential privacy\nACM Reference Format:\nBo Li",
    "title": "When Machine Learning Meets Privacy",
    "abstract": "The newly emerged machine learning (e.g., deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning are still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This article surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning-aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.",
    "link": "https://www.semanticscholar.org/paper/b293e4659e20815bcf0b6d31ce46b8bd9437c1fa",
    "published": "2020-11-24"
  },
  "50": {
    "pdf_path": "data/pdfs/machine learning_paper_21.pdf",
    "text_excerpt": "PRIVATE MACHINE LEARNING VIA RANDOMISED RE-\nSPONSE\nDavid Barber\nDepartment of Computer Science\nUniversity College London, UK\nABSTRACT\nWe introduce a general learning framework for private machine learning based on\nrandomised response. Our assumption is that all actors are potentially adversarial\nand as such we trust only to release a single noisy version of an individual’s\ndatapoint. Our approach forms a consistent way to estimate the true underlying\nmachine learning model and we demonstrate this in the case of logistic regression.\n1 P RIVATE MACHINE LEARNING\nOur desire is to develop a strategy for machine learning driven by the requirement that private data\nshould be shared as little as possible and that no-one can be trusted with an individual’s data, neither\na data collector/aggregator, nor the machine learner that tries to ﬁt a model.\nRandomised Response, see for example Warner (1965), is relevant in this context in which a datapoint\nxnis replaced with a randomised ‘noisy’ version ~xn. A classical example is voting in an election in\nwhich an individual voter votes for one of two candidates AorBand is asked to lie (with probability\np) about whom they voted for . This results in noisy data and estimating the fraction fAof voters that\nvoted for candidate Abased on this noisy data\n~fA=1\nNNX\nn=1I(~xn=A) (1)\ncan give a potentially signiﬁcantly incorrect estimate. As Warner (1965) showed, since we know\nthe probabilistic mechanism that generated the noisy data, a better estimate of the fraction of voters\nvoting for candidate Ais given by\nfA=~fA+p\n1\u00002p(2)\nIn a machine learning context, the kind of scenario we envisage is that users may have labelled face\nimages as “happy\" or “sad\" on their mobile phones and the company MugTome wishes to train a\n“happy/sad\" face classiﬁer; however, users do not wish to send the raw face images to MugTome and\nalso wish to be able to plausibly deny which label they gave any training image. To preserve privacy,\neach user will send to MugTome",
    "title": "Private Machine Learning via Randomised Response",
    "abstract": "We introduce a general learning framework for private machine learning based\non randomised response. Our assumption is that all actors are potentially\nadversarial and as such we trust only to release a single noisy version of an\nindividual's datapoint. We discuss a general approach that forms a consistent\nway to estimate the true underlying machine learning model and demonstrate this\nin the case of logistic regression.",
    "link": "http://arxiv.org/abs/2001.04942v2",
    "published": "2020-01-14T17:56:16Z"
  },
  "51": {
    "pdf_path": "data/pdfs/machine learning_paper_81.pdf",
    "text_excerpt": "Membership Inference Attacks Against\nMachine Learning Models\nReza Shokri\nCornell Tech\nshokri@cornell.eduMarco Stronati\u0003\nINRIA\nmarco@stronati.orgCongzheng Song\nCornell\ncs2296@cornell.eduVitaly Shmatikov\nCornell Tech\nshmat@cs.cornell.edu\nAbstract —We quantitatively investigate how machine learning\nmodels leak information about the individual data records on\nwhich they were trained. We focus on the basic membership\ninference attack: given a data record and black-box access to\na model, determine if the record was in the model’s training\ndataset. To perform membership inference against a target model,\nwe make adversarial use of machine learning and train our own\ninference model to recognize differences in the target model’s\npredictions on the inputs that it trained on versus the inputs\nthat it did not train on.\nWe empirically evaluate our inference techniques on classi-\nﬁcation models trained by commercial “machine learning as a\nservice” providers such as Google and Amazon. Using realistic\ndatasets and classiﬁcation tasks, including a hospital discharge\ndataset whose membership is sensitive from the privacy perspec-\ntive, we show that these models can be vulnerable to membership\ninference attacks. We then investigate the factors that inﬂuence\nthis leakage and evaluate mitigation strategies.\nI. I NTRODUCTION\nMachine learning is the foundation of popular Internet\nservices such as image and speech recognition and natural lan-\nguage translation. Many companies also use machine learning\ninternally, to improve marketing and advertising, recommend\nproducts and services to users, or better understand the data\ngenerated by their operations. In all of these scenarios, ac-\ntivities of individual users—their purchases and preferences,\nhealth data, online and ofﬂine transactions, photos they take,\ncommands they speak into their mobile phones, locations they\ntravel to—are used as the training data.\nInternet giants such as Google and Amazon are already\noffering “machine learning as a s",
    "title": "Membership Inference Attacks Against Machine Learning Models",
    "abstract": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",
    "link": "https://www.semanticscholar.org/paper/f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d",
    "published": "2016-10-18"
  },
  "52": {
    "pdf_path": "data/pdfs/machine learning_paper_59.pdf",
    "text_excerpt": "Collaborative Machine Learning Markets with Data-Replication-Robust Payments\nOlga Ohrimenko Shruti Tople Sebastian Tschiatschek\nMicrosoft Research\nfoohrim,t-shtopl,setschia g@microsoft.com\nAbstract\nWe study the problem of collaborative machine learning mar-\nkets where multiple parties can achieve improved perfor-\nmance on their machine learning tasks by combining their\ntraining data. We discuss desired properties for these machine\nlearning markets in terms of fair revenue distribution and po-\ntential threats, including data replication. We then instantiate\na collaborative market for cases where parties share a com-\nmon machine learning task and where parties’ tasks are dif-\nferent. Our marketplace incentivizes parties to submit high\nquality training and true validation data. To this end, we in-\ntroduce a novel payment division function that is robust-to-\nreplication and customized output models that perform well\nonly on requested machine learning tasks. In experiments, we\nvalidate the assumptions underlying our theoretical analysis\nand show that these are approximately satisﬁed for commonly\nused machine learning models.\n1 Introduction\nOne of the main obstacles for training well-performing ma-\nchine learning models is the limited availability of suf-\nﬁciently diverse labeled training data. However, the data\nneeded to train good models often exists but is not easy to\nleverage as it is distributed and owned by multiple parties.\nFor instance, in the medical domain, important data about\npatients that could be used for learning diagnostic support\nsystems for cancer might be in possession of different hos-\npitals, each of which holding different data, (e.g., from a\nspeciﬁc geographical region with different demographics).\nTypically, by pooling the available data, the hospitals could\ntrain better machine learning models for their application\nthan they could using only their own data. As all hospi-\ntals would beneﬁt from a better machine learning model ob-\ntained through dat",
    "title": "Collaborative Machine Learning Markets with Data-Replication-Robust\n  Payments",
    "abstract": "We study the problem of collaborative machine learning markets where multiple\nparties can achieve improved performance on their machine learning tasks by\ncombining their training data. We discuss desired properties for these machine\nlearning markets in terms of fair revenue distribution and potential threats,\nincluding data replication. We then instantiate a collaborative market for\ncases where parties share a common machine learning task and where parties'\ntasks are different. Our marketplace incentivizes parties to submit high\nquality training and true validation data. To this end, we introduce a novel\npayment division function that is robust-to-replication and customized output\nmodels that perform well only on requested machine learning tasks. In\nexperiments, we validate the assumptions underlying our theoretical analysis\nand show that these are approximately satisfied for commonly used machine\nlearning models.",
    "link": "http://arxiv.org/abs/1911.09052v1",
    "published": "2019-11-08T13:58:31Z"
  },
  "53": {
    "pdf_path": "data/pdfs/machine learning_paper_511.pdf",
    "text_excerpt": "Machine Learning 2: 319-342, 1988\n© 1988 Kluwer Academic Publishers, Boston - Manufactured in The Netherlands\nQueries and Concept Learning\nDANA ANGLUIN (ANGLUIN@YALE.EDU)\nDepartment of Computer Science, Yale University, P.O. Box 2158,\nNew Haven, CT 06520, U.S.A.\n(Received: July 20, 1987)\n(Revised: January 26, 1988)\nKeywords: Concept learning, supervised learning, queries\nAbstract. We consider the problem of using queries to learn an unknown concept.\nSeveral types of queries are described and studied: membership, equivalence, subset, su-\nperset, disjointness, and exhaustiveness queries. Examples are given of efficient learning\nmethods using various subsets of these queries for formal domains, including the regular\nlanguages, restricted classes of context-free languages, the pattern languages, and re-\nstricted types of prepositional formulas. Some general lower bound techniques are given.\nEquivalence queries are compared with Valiant's criterion of probably approximately\ncorrect identification under random sampling.\n1. Introduction\nA successful learning component in an expert system will probably rely\nheavily on queries to its instructors. For example, Sammut and Banerji's\n(1986) system uses queries about specific examples as part of its strategy\nfor efficiently learning a target concept. Shapiro's (1981, 1982, 1983) Algo-\nrithmic Debugging System uses a variety of types of queries to the user to\npinpoint errors in Prolog programs. In this paper we use a formal frame-\nwork to study the power of several types of queries for concept-learning\ntasks.\nWe consider the problem of identifying an unknown set L* from some\nfinite or countable hypothesis space L1, L2,, ... of subsets of a universal set\nU. The usual assumption is that one is given an arbitrarily or stochastically\ngenerated sequence of elements of U, each classified as to whether it is in\nL*.1 Instead, we will assume that the learning algorithm has access to\na fixed set of oracles that will answer specific kinds of",
    "title": "Queries and Concept Learning",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022821128753",
    "published": "2003-04-04T16:57:10Z"
  },
  "54": {
    "pdf_path": "data/pdfs/machine learning_paper_384.pdf",
    "text_excerpt": "Machine Learning 2: 195 198, 1987\n© 1987 Kluwer Academic- Publishers, Boston Manufactured in The Netherlands\nEDITORIAL\nResearch Papers in Machine Learning\nA prototype for machine learning papers\nScientists do not work in isolation. Communication is central to the\nscientific enterprise, and much of the exchange occurs through research\npapers. Naturally, the content of these papers will vary with the field of\nstudy, and in this essay I discuss the information that machine learning\nresearchers should attempt to communicate in their articles. Readers may\ninterpret these comments as reflecting the current editorial policy of the\njournal Machine Learning, though this policy will undoubtedly evolve over\ntime, as even editors and editorial boards learn from experience.\nDespite the advantages of standardization, it can lead a new discipline\nto crystallize into a rigid pattern before its time. Rather than define an\ninflexible format that all papers in machine learning must follow, I provide\nbelow a more flexible, prototypical outline. Specific papers will diverge\nfrom this prototype, but it should provide a useful target, particularly\nfor authors new to the field. The outline assumes a paper that describes\na single, running machine learning system. Most papers that appear in\nMachine Learning will take this form, but later I will briefly consider some\nother possibilities.\n1. Goals of the research. Machine learning researchers have many different\nreasons for carrying out their work. Some are interested in general\nprinciples of intelligent behavior, others are concerned with modeling\nhuman learning, and still others are oriented towards applications. If\nan author hopes to communicate with his audience, it is essential that\nhe state his goals early in the paper, so that readers can decide for\nthemselves whether the work has made progress towards those goals.\n2. Description of the task. Learning always occurs in the context of some\nperformance task - such as classification, planni",
    "title": "Research Papers in Machine Learning",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022603230145",
    "published": "2003-04-04T16:55:36Z"
  },
  "55": {
    "pdf_path": "data/pdfs/machine learning_paper_120.pdf",
    "text_excerpt": "REVIEW ARTICLE OPEN\nSmall data machine learning in materials science\nPengcheng Xu1, Xiaobo Ji2, Minjie Li2✉and Wencong Lu1,2,3✉\nThis review discussed the dilemma of small data faced by materials machine learning. First, we analyzed the limitations brought by\nsmall data. Then, the work ﬂow of materials machine learning has been introduced. Next, the methods of dealing with small data\nwere introduced, including data extraction from publications, materials database construction, high-throughput computations and\nexperiments from the data source level; modeling algorithms for small data and imbalanced learning from the algorithm level;\nactive learning and transfer learning from the machine learning strategy level. Finally, the future directions for small data machinelearning in materials science were proposed.\nnpj Computational Materials            (2023) 9:42 ; https://doi.org/10.1038/s41524-023-01000-z\nINTRODUCTION\nAs an interdisciplinary subject covering computer science,\nmathematics, statistics and engineering, machine learning is\ndedicated to optimizing the performance of computer programs\nby using data or previous experience, which is also one of theimportant directions of arti ﬁcial intelligence development\n1,2.I n\nrecent years, machine learning has been widely used in manyﬁelds such as ﬁnance, medical care, industry, and biology\n3–10.I n\n2011, the concept of material genome initiative (MGI) wasproposed to shorten the material development cycle through\ncomputational tools, experimental facilities and digital data. Under\nthe leadership of the MGI, machine learning has also become oneof the important means for materials design and discovery\n11,12.\nThe core of machine learning-assisted materials design anddiscovery lies in the construction of machine learning modelswith good performance through algorithms and materials data toachieve the accurate prediction of target properties for undeter-\nmined samples\n13. The constructed model could be further used to\ndiscover and",
    "title": "Small data machine learning in materials science",
    "abstract": "This review discussed the dilemma of small data faced by materials machine learning. First, we analyzed the limitations brought by small data. Then, the workflow of materials machine learning has been introduced. Next, the methods of dealing with small data were introduced, including data extraction from publications, materials database construction, high-throughput computations and experiments from the data source level; modeling algorithms for small data and imbalanced learning from the algorithm level; active learning and transfer learning from the machine learning strategy level. Finally, the future directions for small data machine learning in materials science were proposed.",
    "link": "https://www.semanticscholar.org/paper/35b1d79993f0e4fbfcb3b86c5013c5e2a7e3117c",
    "published": "2023-03-25"
  },
  "56": {
    "pdf_path": "data/pdfs/machine learning_paper_218.pdf",
    "text_excerpt": "A SURVEY OF HUMAN-IN-THE-LOOP FOR MACHINE LEARNING\nXingjiao Wu1;2, Luwei Xiao2, Yixuan Sun3, Junhang Zhang2, Tianlong Ma1;2, Liang He1;2;\u0003\n1 Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University, Shanghai, China\n2 School of Computer Science and Technology, East China Normal University, Shanghai, China\n3 Fudan University, Shanghai, China\nfwuxingjiao2885, louisshaw008, madmaxkgb, junhangzhang68 g@gmail.com, ftlma, lhe g@cs.ecnu.edu.cn\nABSTRACT\nMachine learning has become the state-of-the-art technique\nfor many tasks including computer vision, natural language\nprocessing, speech processing tasks, etc.However, the unique\nchallenges posed by machine learning suggest that incorpo-\nrating user knowledge into the system can be beneﬁcial. The\npurpose of integrating human domain knowledge is also to\npromote the automation of machine learning. Human-in-the-\nloop is an area that we see as increasingly important in future\nresearch due to the knowledge learned by machine learning\ncannot win human domain knowledge. Human-in-the-loop\naims to train an accurate prediction model with minimum cost\nby integrating human knowledge and experience. Humans\ncan provide training data for machine learning applications\nand directly accomplish tasks that are hard for computers in\nthe pipeline with the help of machine-based approaches. In\nthis paper, we survey existing works on human-in-the-loop\nfrom a data perspective and classify them into three categories\nwith a progressive relationship: (1) the work of improving\nmodel performance from data processing, (2) the work of\nimproving model performance through interventional model\ntraining, and (3) the design of the system independent human-\nin-the-loop. Using the above categorization, we summarize\nthe major approaches in the ﬁeld; along with their technical\nstrengths/ weaknesses, we have a simple classiﬁcation and\ndiscussion in natural language processing, computer vision,\nand others. Besides, we provide som",
    "title": "A Survey of Human-in-the-loop for Machine Learning",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/102ebe229df18c8733ea1b8def56cd79996e2178",
    "published": "2021-08-02"
  },
  "57": {
    "pdf_path": "data/pdfs/machine learning_paper_62.pdf",
    "text_excerpt": "arXiv:2006.01387v2  [stat.ML]  4 Jun 2020A COMBINATORIAL CONJECTURE FROM PAC-BAYESIAN\nMACHINE LEARNING\nMALIK YOUNSI AND ALEXANDRE LACASSE\nAbstract. We present a proof of a combinatorial conjecture from the sec ond\nauthor’s Ph.D. thesis [9]. The proof relies on binomial and m ultinomial sums\nidentities. We also discuss the relevance of the conjecture in the context of\nPAC-Bayesian machine learning.\n1.PAC-Bayes bounds in machine learning.\nIn machine learning, ensemble methods are used to build stronger mo dels by\ncombining several base models. In the setting of classiﬁcation prob lems, the model,\ncalledclassiﬁer , can be combined using a weighted majority vote of the base clas-\nsiﬁers. Examples of learning algorithms built using such classiﬁers inclu deBagging\n[1] andBoosting [4].\nThe PAC-Bayes theorem, introduced by McAllester in the 1990’s ([1 3], [14],\n[15]), provides bounds on the expectation of the risk of the base cla ssiﬁers among a\nmajority vote classiﬁer. These bounds are computed from the emp irical risk of the\nmajority classiﬁer made during the training phase of a learning algorit hm, and can\nthen be used to provide guarantees on existing algorithms or to des ign new ones.\nThe version of the PAC-Bayes theorem in [7] involves a function ξ:N→R\ndeﬁned by\nξ(m) :=m/summationdisplay\nk=0/parenleftbiggm\nk/parenrightbigg/parenleftbiggk\nm/parenrightbiggk/parenleftbigg\n1−k\nm/parenrightbiggm−k\nand improves upon other versions, such as the ones in [11] and [12 ] for instance,\nwhich provide the bounds m+1 and 2√mrespectively using approximations. The\nversion in [7], on the other hand, is based on a direct calculation in ord er to obtain\nξ(m).\nBounds on the risk of the majority vote itself can be deduced from t he bounds\ngiven by the PAC-Bayes theorem. Such bounds, however, turn ou t to be greater\nthan the bounds on the base voters. In order to circumvent this is sue and obtain\nbetter bounds, a new version of the PAC-Bayes theorem was given in [10]. This\nnew theorem can b",
    "title": "A combinatorial conjecture from PAC-Bayesian machine learning",
    "abstract": "We present a proof of a combinatorial conjecture from the second author's\nPh.D. thesis. The proof relies on binomial and multinomial sums identities. We\nalso discuss the relevance of the conjecture in the context of PAC-Bayesian\nmachine learning.",
    "link": "http://arxiv.org/abs/2006.01387v2",
    "published": "2020-06-02T04:36:50Z"
  },
  "58": {
    "pdf_path": "data/pdfs/machine learning_paper_472.pdf",
    "text_excerpt": "Machin e Learning , 13, 151-15 2 (1993 )\n© 199 3 Kluwe r Academi c Publishers , Boston . Manufacture d in The Netherlands .\nA Repl y to Hellerstein' s Boo k Revie w of\nMachine  Learning:  A Theoretical  Approach\nB.K. NATARAJA N\nHewlett  Packard  Laboratories,  1501  Page  Mill  Road,  Palo  Alto,  CA 94304\nI wan t to than k Lisa Hellerstei n for her kind efforts , and Albert o Segr e for commission -\ning such .\nThe challeng e in writin g this book lay in presentin g technicall y difficul t materia l in a\nform that was accessibl e to the broade r audience , withou t compromisin g its integrity . A\ncompoundin g facto r was that this was to be the first text in the area , withou t prior leverag e\nin establishin g notationa l convention s or conceptua l organization . Such a challeng e demande d\nnumerou s editoria l and pedagogi c decisions , and I must confes s that man y of Hellerstein' s\npoint s bear directl y on thes e decisions . Whil e it is neithe r possibl e nor constructiv e for\nme to respon d to her revie w exhaustively , I woul d urge anyon e undertakin g a simila r work\nto pay her close attention . I will restric t my respons e to a few specifi c points .\nWhen I say that the PAC mode l appear s to be a good mode l of the natura l learnin g proc -\ness, I do not impl y that it model s the learnin g mechanism s of the livin g psyche . I mea n\nthat it is a good mode l of the input-outpu t behavio r of the natura l learnin g process—goo d\nbecaus e it is wort h studyin g in an aestheti c sense , and mor e importantly , good becaus e\nit promise s to be a step toward s explainin g huma n experience . As pointe d out by Valian t\n(1984) , both of the abov e qualitie s are desirabl e in a computationa l model . My statemen t\nis in the same spiri t as the statemen t that the Turin g machin e is a good mode l of the natura l\nnotio n of a computationa l algorithm .\nI am please d that Hellerstei n sees my book as a unifor m presentatio n of theoretica l results ,\nofferin g",
    "title": "A Reply to Hellerstein's Book Review of Machine Learning: A Theoretical Approach",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022643815046",
    "published": "2003-04-04T16:55:36Z"
  },
  "59": {
    "pdf_path": "data/pdfs/machine learning_paper_664.pdf",
    "text_excerpt": "Machine Learning, 12, 117-141 (1993)\n© 1993 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\nMachine Discovery of Effective Admissible Heuristics\nARMAND E. PRIEDITIS PRIEDITIS@cs.UCDAVIS.EDU\nDepartment of Computer Science, University of California, Davis, CA 95616\nAbstract. Admissible heuristics are an important class of heuristics worth discovering: they guarantee shortest\npath solutions in search algorithms such as A* and they guarantee less expensively produced, but boundedly longer\nsolutions in search algorithms such as dynamic weighting. Unfortunately, effective (accurate and cheap to com-\npute) admissible heuristics can take years for people to discover. Several researchers have suggested that certain\ntransformations of a problem can be used to generate admissible heuristics. This article defines a more general\nclass of transformations, called abstractions, that are guaranteed to generate only admissible heuristics. It also\ndescribes and evaluates an implemented program (Absolver II) that uses a means-ends analysis search control\nstrategy to discover abstracted problems that result in effective admissible heuristics. Absolver II discovered several\nwell-known and a few novel admissible heuristics, including the first known effective one for Rubik's Cube, thus\nconcretely demonstrating that effective admissible heuristics can be tractably discovered by a machine.\nKeywords. Machine discovery, admissible heuristics, search, abstraction.\n1. Introduction\nAdmissible (lower-bound) heuristics are an important class of heuristics worth discovering\nbecause they have several desirable properties in various search algorithms. For example,\nthey guarantee shortest path solutions in the A* algorithm. The computational complexity\nof generating a shortest path solution, which is NP-Complete for many problems, is some-\ntimes justified when path length corresponds to resource usage such as time or money or\nwhen the same solution is used frequently. Often, howev",
    "title": "Machine Discovery of Effective Admissible Heuristics",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022875501978",
    "published": "2003-04-04T16:57:10Z"
  },
  "60": {
    "pdf_path": "data/pdfs/machine learning_paper_69.pdf",
    "text_excerpt": "       Confronting Machine Learning With Financial Research   Kristof Lommers, Ouns El Harzli, Jack Kim1      Abstract This study aims to examine the challenges and applications of machine learning for financial research. Machine learning algorithms have been developed for certain data environments which substantially differ from the one we encounter in finance. Not only do difficulties arise due to some of the idiosyncrasies of financial markets, there is a fundamental tension between the underlying paradigm of machine learning and the research philosophy in financial economics. Given the peculiar features of financial markets and the empirical framework within social science, various adjustments have to be made to the conventional machine learning methodology. We discuss some of the main challenges of machine learning in finance and examine how these could be accounted for. Despite some of the challenges, we argue that machine learning could be unified with financial research to become a robust complement to the econometrician’s toolbox. Moreover, we discuss the various applications of machine learning in the research process such as estimation, empirical discovery, testing, causal inference and prediction.      Key words: Financial Machine Learning, Empirical Finance, Financial Econometrics JEL: G00, C10, C40  1 Kristof Lommers (University of Oxford Said Business School and Oxford-Man Institute of Quantitative Finance), Jack Kim (Data Capital Management), Ouns El Harzli (University of Oxford). Correspondence via Kristof Lommers (mail: Kristof.Lommers.DPHIL@said.oxford.edu). We would like to thank Viktor Smits, Stan Beckers, and Lina Thomas for their helpful comments on the paper. 1 1. Introduction   During the past decade, machine learning has gained substantial interest among scientists but its adoption in financial-economic academia has been rather limited. Given its success in other fields and the empirical nature of finance, it is appealing to use machine lea",
    "title": "Confronting Machine Learning With Financial Research",
    "abstract": "This study aims to examine the challenges and applications of machine\nlearning for financial research. Machine learning algorithms have been\ndeveloped for certain data environments which substantially differ from the one\nwe encounter in finance. Not only do difficulties arise due to some of the\nidiosyncrasies of financial markets, there is a fundamental tension between the\nunderlying paradigm of machine learning and the research philosophy in\nfinancial economics. Given the peculiar features of financial markets and the\nempirical framework within social science, various adjustments have to be made\nto the conventional machine learning methodology. We discuss some of the main\nchallenges of machine learning in finance and examine how these could be\naccounted for. Despite some of the challenges, we argue that machine learning\ncould be unified with financial research to become a robust complement to the\neconometrician's toolbox. Moreover, we discuss the various applications of\nmachine learning in the research process such as estimation, empirical\ndiscovery, testing, causal inference and prediction.",
    "link": "http://arxiv.org/abs/2103.00366v2",
    "published": "2021-02-28T01:10:09Z"
  },
  "61": {
    "pdf_path": "data/pdfs/machine learning_paper_652.pdf",
    "text_excerpt": "Machin e Learnin g 4, 67-97 , 1989\nc 1989 Kluwe r Academi c Publishers—Manufacture d in The Netherland s\nOn Learnin g Sets and Function s\nB. K. NATARAJA N (NAT@CS.CMU.EDU )\nThe Robotics  Institute,  Carnegie  Mellon  University,  Pittsburgh,  PA 15213,  U.S.A.\nEditor : Pat Langle y\nKeywords : Learnin g sets , learnin g functions , probabilisti c analysis , connectionis t networks .\nAbstract . Thi s pape r present s som e result s on the probabilisti c analysi s of learning , illustratin g the\napplicabilit y of thes e result s to setting s such as connectionis t networks . In particular , it concern s the\nlearnin g of sets and function s from example s and backgroun d information . Afte r a forma l statemen t\nof the problem , som e theorem s are provide d identifyin g the condition s necessar y and sufficien t for\nefficien t learning , with respec t to measure s of informatio n complexit y and computationa l complexity .\nIntuitiv e interpretation s of the definition s and theorem s are provided .\n1. Introductio n\nThis pape r concern s algorithm s that lear n sets and function s from examples . The\nresult s presente d in these paper s appeare d in preliminar y form in Nataraja n (1986 ,\n1987, 1988b ) and in Nataraja n and Tadepall i (1988) . Amon g others , the followin g\nauthor s have reporte d relate d investigations : Anglui n (1986) , Rives t and Schapir e\n(1987) , Berma n and Roo s (1987) , Lair d (1987) , Blumer , Ehrenfeucht , Haussler ,\nand Warmut h (1986) , and Kearns , Li, Pitt, and Valian t (1987a) . Althoug h there\nis som e overla p of result s betwee n this pape r and som e of the aforementione d\npapers , the result s in this pape r represen t independen t development s that ofte n\nfavor simple r proo f techniques .\nOver the years , man y paper s in the literatur e have addresse d the topic of concep t\nlearning . Thes e paper s can be broadl y classifie d into two categories : (1) the forma l\nwork on inductiv e inferenc e and, (2) the mor e empirica l wor",
    "title": "On Learning Sets and Functions",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022605311895",
    "published": "2003-04-04T16:55:36Z"
  },
  "62": {
    "pdf_path": "data/pdfs/machine learning_paper_574.pdf",
    "text_excerpt": " \n \n \n*Corresponding author. Email:  Hadeel.mohammed@uoanbar.edu.iq  \n                      \n \n \n \nResea rch Article  \nDiagnosis of HIV ( AIDS) by  Using deep learning and machine learning  \nHadeel M Saleh 1,*,, Abdulrahman Kareem Oleiwi1, , Ahmed Abed Hwaidi Abed1,  \n \n1 Center For  Continuing Education, Anbar University, Iraq . \n \n \nA R T I C L E  I N F O  \n \nArticle  Histo ry \nReceived 23  Aug  2023  \nAccepted 22  Oct  2023  \nPublished 15 Nov 2023  \n \nKeywords  \nHIV  \nAIDS  \nMachine learning  \ndeep learning  A B S T R A C T   \n \nHuman Immunodeficiency  Virus (HIV) is a global health issue that can progress to Acquired \nImmunodeficiency Syndrome (HIV) if not diagnosed and treated early. The advent of Artificial \nIntelligence (AI), particularly in machine learning and deep learning, presents new opportunities for \nimproving the accuracy and efficiency of HIV diagnosis. This research explores the application of AI \ntechniques in diagnos ing HIV by reviewing previous studies and proposing a novel AI -based approach. \nThe proposed methodology leverages deep learning algorithms, such as convolutional neural networks \n(CNNs), along with advanced data preprocessing techniques to enhance diagnosti c accuracy, sensitivity, \nand specificity. The results of the proposed CNN -based model show an accuracy of 96.2%, sensitivity \nof 95.8%, specificity of 96.8%, and an AUC -ROC score of 0.965. Compared to Random Forest \n(accuracy: 92.1%), SVM (accuracy: 91.5%), and traditional methods (accuracy: 89.0%), the CNN model \noutperforms existing techniques significantly in terms of accuracy, sensitivity, and specificity. This \ndemonstrates the effectiveness of the proposed AI approach for enhancing early and accurate HIV \ndetection.\n1. INTRODUCTION  \nHuman Immunodeficiency Virus (HIV) is a global public health crisis that severely impacts the immune system by targeting \nCD4 cells, which are essential for immune defense. If untreated, the progression of HIV can lead to Acquired \nImmunode",
    "title": "Diagnosis of HIV (AIDS)  by Using deep learning and machine learning",
    "abstract": "<jats:p>Human Immunodeficiency Virus (HIV) is a global health issue that can progress to Acquired Immunodeficiency Syndrome (HIV) if not diagnosed and treated early. The advent of Artificial Intelligence (AI), particularly in machine learning and deep learning, presents new opportunities for improving the accuracy and efficiency of HIV diagnosis. This research explores the application of AI techniques in diagnosing HIV by reviewing previous studies and proposing a novel AI-based approach. The proposed methodology leverages deep learning algorithms, such as convolutional neural networks (CNNs), along with advanced data preprocessing techniques to enhance diagnostic accuracy, sensitivity, and specificity. The results of the proposed CNN-based model show an accuracy of 96.2%, sensitivity of 95.8%, specificity of 96.8%, and an AUC-ROC score of 0.965. Compared to Random Forest (accuracy: 92.1%), SVM (accuracy: 91.5%), and traditional methods (accuracy: 89.0%), the CNN model outperforms existing techniques significantly in terms of accuracy, sensitivity, and specificity. This demonstrates the effectiveness of the proposed AI approach for enhancing early and accurate HIV detection.\r\n </jats:p>",
    "link": "https://doi.org/10.58496/bjml/2023/012",
    "published": "2024-12-18T17:52:27Z"
  },
  "63": {
    "pdf_path": "data/pdfs/machine learning_paper_67.pdf",
    "text_excerpt": "Risk Assessment for Machine Learning Models\nPaul Schwerdtner1,*, Florens Greßner1, Nikhil Kapoor2,\nFelix Assion1, René Sass2, Wiebke Günther1, Fabian Hüger2, and Peter Schlicht2\n1Neurocat GmbH\n2Volkswagen AG\n*Corresponding author: ps@neurocat.ai\nNovember 10, 2020Accecpted for presentation at the NeurIPS 2020 Virtual Workshop: Machine Learning for Autonomous Driving\nAbstract\nIn this paper we propose a framework for assessing the risk\nassociated with deploying a machine learning model in a\nspeciﬁed environment. For that we carry over the risk deﬁni-\ntion from decision theory to machine learning. We develop\nand implement a method that allows to deﬁne deployment\nscenarios, test the machine learning model under the con-\nditions speciﬁed in each scenario, and estimate the damage\nassociated with the output of the machine learning model\nunder test. Using the likelihood of each scenario together\nwith the estimated damage we deﬁne key risk indicators of a\nmachine learning model.\nThe deﬁnition of scenarios and weighting by their likeli-\nhood allows for standardized risk assessment in machine\nlearning throughout multiple domains of application. In\nparticular, in our framework, the robustness of a machine\nlearning model to random input corruptions, distributional\nshifts caused by a changing environment, and adversarial\nperturbations can be assessed.\n1 Introduction\nWith the deployment of machine learning (ML) models in\nsafety and security critical environments, risk assessment\nbecomes a pressing issue. Failure modes of a given ML\nmodel must be identiﬁed and the likelihood of occurrence\nand severity of the damage caused by failure must be as-\nsessed. In this work, we focus on failures that result from\ninput perturbations and provide a framework that allows to\nintegrate different sources of input perturbations and to com-\npute general risk scores for a given network and operational\nenvironment. These key risk indicators (KRIs) can guide the\ndecision on whether it is safe and secure",
    "title": "Risk Assessment for Machine Learning Models",
    "abstract": "In this paper we propose a framework for assessing the risk associated with\ndeploying a machine learning model in a specified environment. For that we\ncarry over the risk definition from decision theory to machine learning. We\ndevelop and implement a method that allows to define deployment scenarios, test\nthe machine learning model under the conditions specified in each scenario, and\nestimate the damage associated with the output of the machine learning model\nunder test. Using the likelihood of each scenario together with the estimated\ndamage we define \\emph{key risk indicators} of a machine learning model.\n  The definition of scenarios and weighting by their likelihood allows for\nstandardized risk assessment in machine learning throughout multiple domains of\napplication. In particular, in our framework, the robustness of a machine\nlearning model to random input corruptions, distributional shifts caused by a\nchanging environment, and adversarial perturbations can be assessed.",
    "link": "http://arxiv.org/abs/2011.04328v1",
    "published": "2020-11-09T10:50:50Z"
  },
  "64": {
    "pdf_path": "data/pdfs/machine learning_paper_65.pdf",
    "text_excerpt": "1 \n \n \n \n1 Faculty of Computer Science, Nahda University, Egypt  \n                                    2 Faculty of Computers and Information, Mansoura University, Egypt  \n                                                      3 Faculty of Computers and Information, Minia University, Egypt  \n \n \nAbstract  \nDue to the superiority and noteworthy progress of Quantum Comput ing (QC) in a lot of applications such as cryptography, \nchemistry, Big data, machine learning, optimization, Internet of Things (IoT), Blockchain, communication, and many more. \nFully towards to combine classical machine learning (ML) with Quantum Informati on Processing (QIP) to build a new field in \nthe quantum world is called Quantum Machine Learning (QML) to solve and improve problems that displayed in classical \nmachine learning (e.g. time and energy consumption, kernel estimation). The aim of this paper p resents and summarizes a \ncomprehensive survey of the state -of-the-art advances in Quantum Machine Learning (QML). Especially, recent QML \nclassification works. Also, we cover about 30 publications that are published lately in Quantum Machine Learning (QML).  we \npropose a classification scheme in the quantum world and discuss encoding methods for mapping classical data to quantum \ndata. Then, we provide quantum subroutines and some methods of Quantum Computing (QC) in improving performance and \nspeed up of class ical Machine Learning (ML). And also some of QML applications in various fields, challenges, and future vision \nwill be presented.  \n \nKEYWORDS  \nQuantum Machine Learning, Quantum Computing, Quantum Bit (Qubit), Quantum Inspired, Hybrid Quantum -Classical, \nVaria tional Quantum Classifier, Quantum Cl assification, Machine Learning.  \n \n1 | INTRODUCTION  \n \nAs is well known t he role of machine learning (1-4)  in data   analysis , feature selection, making decision, pattern \nclassificati on and future predictions of various applications with achieving better accuracy and performance w",
    "title": "Classification with Quantum Machine Learning: A Survey",
    "abstract": "Due to the superiority and noteworthy progress of Quantum Computing (QC) in a\nlot of applications such as cryptography, chemistry, Big data, machine\nlearning, optimization, Internet of Things (IoT), Blockchain, communication,\nand many more. Fully towards to combine classical machine learning (ML) with\nQuantum Information Processing (QIP) to build a new field in the quantum world\nis called Quantum Machine Learning (QML) to solve and improve problems that\ndisplayed in classical machine learning (e.g. time and energy consumption,\nkernel estimation). The aim of this paper presents and summarizes a\ncomprehensive survey of the state-of-the-art advances in Quantum Machine\nLearning (QML). Especially, recent QML classification works. Also, we cover\nabout 30 publications that are published lately in Quantum Machine Learning\n(QML). we propose a classification scheme in the quantum world and discuss\nencoding methods for mapping classical data to quantum data. Then, we provide\nquantum subroutines and some methods of Quantum Computing (QC) in improving\nperformance and speed up of classical Machine Learning (ML). And also some of\nQML applications in various fields, challenges, and future vision will be\npresented.",
    "link": "http://arxiv.org/abs/2006.12270v1",
    "published": "2020-06-22T14:05:31Z"
  },
  "65": {
    "pdf_path": "data/pdfs/machine learning_paper_250.pdf",
    "text_excerpt": "1  Opportunities and Challenges for Machine Learning in Materials Science Annual Review of Materials Research, Vol. 50  Dane Morgan and Ryan Jacobs  Department of Materials Science and Engineering University of Wisconsin-Madison, Madison, WI, 53706 ddmorgan@wisc.edu, rjacobs3@wisc.edu  Keywords: machine learning, materials informatics, materials science, model assessment, applicability domain, model errors, materials discovery, materials design, artificial intelligence  Abstract: Advances in machine learning have impacted myriad areas of materials science, ranging from the discovery of novel materials to the improvement of molecular simulations, with likely many more important developments to come. Given the rapid changes in this field, it is challenging to understand both the breadth of opportunities as well as best practices for their use. In this review, we address aspects of both problems by providing an overview of the areas where machine learning has recently had significant impact in materials science, and then provide a more detailed discussion on determining the accuracy and domain of applicability of some common types of machine learning models. Finally, we discuss some opportunities and challenges for the materials community to fully utilize the capabilities of machine learning. 1 Introduction\t Machine learning (ML) is playing an increasing role in our society, and more specifically in materials science and engineering (MS&E). This review seeks to provide a brief introduction to ML and its growing roles in an array of aspects of MS&E, as well as a more detailed discussion of some of the challenges and opportunities associated with using ML for predicting materials properties and accelerating the design of new materials. We hope it will therefore be of value for both the novice and experienced user. ML can be defined as the use of computer systems that do not require explicit programming to learn about the task they are completing. ML falls into two major ",
    "title": "Opportunities and Challenges for Machine Learning in Materials Science",
    "abstract": "Advances in machine learning have impacted myriad areas of materials science, such as the discovery of novel materials and the improvement of molecular simulations, with likely many more important developments to come. Given the rapid changes in this field, it is challenging to understand both the breadth of opportunities and the best practices for their use. In this review, we address aspects of both problems by providing an overview of the areas in which machine learning has recently had significant impact in materials science, and then we provide a more detailed discussion on determining the accuracy and domain of applicability of some common types of machine learning models. Finally, we discuss some opportunities and challenges for the materials community to fully utilize the capabilities of machine learning.",
    "link": "https://www.semanticscholar.org/paper/ede72940ae0246a292d644bd3c7e0ebf1e12a01a",
    "published": "2020-06-25"
  },
  "66": {
    "pdf_path": "data/pdfs/machine learning_paper_450.pdf",
    "text_excerpt": "Machine Learning, 28, 41–75 (1997)\nc°1997 Kluwer Academic Publishers. Manufactured in The Netherlands.\nMultitask Learning*\nRICH CARUANA caruana@cs.cmu.edu\nSchool of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213\nEditors: Lorien Pratt and Sebastian Thrun\nAbstract. Multitask Learning is an approach to inductive transfer that improves generalization by using the\ndomaininformationcontainedinthetrainingsignalsof relatedtasksasaninductivebias. Itdoesthisbylearning\ntasksinparallelwhileusingasharedrepresentation;whatislearnedforeachtaskcanhelpothertasksbelearnedbetter. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers taskrelatednesswithouttheneedofsupervisorysignals,andpresentsnewresultsforMTLwithk-nearestneighborandkernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitasklearningworks,andshowthattherearemanyopportunitiesformultitasklearninginrealdomains. Wepresentanalgorithmandresultsformultitasklearningwithcase-basedmethodslikek-nearestneighborandkernelregression,andsketchanalgorithmformultitasklearningindecisiontrees. Becausemultitasklearningworks,canbeappliedto many different kinds of domains, and can be used with different learning algorithms, we conjecture there willbe many opportunities for its use on real-world problems.\nKeywords: inductive transfer, parallel transfer, multitask learning, backpropagation, k-nearest neighbor, kernel\nregression, supervised learning, generalization\n1. Introduction\n1.1. OverviewMultitask Learning (MTL) is an inductive transfer mechanism whose principle goal is\nto improve generalization performance. MTL improves generalization by leveraging thedomain-speciﬁcinformationcontainedinthetrainingsignalsof relatedtasks. Itdoesthisby\ntrainingtasksinparallelwhileusingasharedrepresentation. Ineffect,thetrainingsignalsfor the extra tasks serve as an inductive bias. Section 1.2 argues that inductive transfer isimportantifwewishtosca",
    "title": "Multitask Learning",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1007379606734",
    "published": "2002-12-22T04:48:21Z"
  },
  "67": {
    "pdf_path": "data/pdfs/machine learning_paper_332.pdf",
    "text_excerpt": "Implementing Machine Learning in Health Care — Addressing \nEthical Challenges\nDanton S. Char, M.D. , Nigam H. Shah, M.B., B.S., Ph.D. , and David Magnus, Ph.D.\nDepartment of Anesthesiology, Division of Pediatric Cardiac Anesthesia (D.S.C.), the Center for \nBiomedical Ethics (D.S.C., D.M.), and the Center for Biomedical Informatics Research (N.S.), \nStanford University School of Medicine, Stanford, CA\nThe incorporation of machine learning into clinical medicine holds promise for substantially \nimproving health care delivery. Private companies are rushing to build machine learning into \nmedical decision making, pursuing both tools that support physicians and algorithms \ndesigned to function independently of them. Physician-researchers are predicting that \nfamiliarity with machine-learning tools for analyzing big data will be a fundamental \nrequirement for the next generation of physicians and that algorithms might soon rival or \nreplace physicians in fields that involve close scrutiny of images, such as radiology and \nanatomical pathology.1\nHowever, consideration of the ethical challenges inherent in implementing machine learning \nin health care is warranted if the benefits are to be realized. Some ethical challenges are \nstraightforward and need to be guarded against, such as concerns that algorithms may mirror \nhuman biases in decision making. Others, such as the possibility for algorithms to become \nthe repository of the collective medical mind, have less obvious risks but raise broader \nethical concerns.\nAlgorithms introduced in non-medical fields have already been shown to make problematic \ndecisions that reflect biases inherent in the data used to train them. For example, programs \ndesigned to aid judges in sentencing by predicting an offender’s risk of recidivism have \nshown an unnerving propensity for racial discrimination.2\nIt’s possible that similar racial biases could inadvertently be built into health care \nalgorithms. Health care delivery already varies b",
    "title": "Implementing Machine Learning in Health Care - Addressing Ethical Challenges.",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/adfc508b9b3d4fc3903aa383a290dc68fb8bbe5a",
    "published": "2018-03-14"
  },
  "68": {
    "pdf_path": "data/pdfs/machine learning_paper_12.pdf",
    "text_excerpt": "MLBench: How Good Are Machine Learning Clouds for\nBinary Classification Tasks on Structured Data?\n[Experiments and Analysis]\nYu Liu\nDepartment of Computer Science\nETH Zurich\nyu.liu@inf.ethz.chHantian Zhang\nDepartment of Computer Science\nETH Zurich\nhantian.zhang@inf.ethz.chLuyuan Zeng\nDepartment of Computer Science\nETH Zurich\nzengl@student.ethz.ch\nWentao Wu\nMicrosoft Research, Redmond\nwentao.wu@microsoft.comCe Zhang\nDepartment of Computer Science\nETH Zurich\nce.zhang@inf.ethz.ch\nABSTRACT\nWe conduct an empirical study of machine learning functionali-\nties provided by major cloud service providers, which we call ma-\nchine learning clouds . Machine learning clouds hold the promise\nof hiding all the sophistication of running large-scale machine\nlearning: Instead of specifying how to run a machine learning\ntask, users only specify what machine learning task to run and\nthe cloud figures out the rest. Raising the level of abstraction,\nhowever, rarely comes free — a performance penalty is possible.\nHow good, then, are current machine learning clouds on real-world\nmachine learning workloads?\nWe study this question with a focus on binary classification\nproblems. We present mlbench , a novel benchmark constructed\nby harvesting datasets from Kaggle competitions. We then com-\npare the performance of the top winning code available from\nKaggle with that of running machine learning clouds from both\nAzure and Amazon on mlbench . Our comparative study reveals\nthe strength and weakness of existing machine learning clouds\nand points out potential future directions for improvement.\n1 INTRODUCTION\nIn spite of the recent advancement of machine learning research,\nmodern machine learning systems are still far from easy to use,\nat least from the perspective of business users or even scientists\nwithout a computer science background [ 30]. Recently, there\nis a trend toward pushing machine learning onto the cloud as\na “service,” a.k.a. machine learning clouds . By putting a set of\nmachine learnin",
    "title": "MLBench: How Good Are Machine Learning Clouds for Binary Classification\n  Tasks on Structured Data?",
    "abstract": "We conduct an empirical study of machine learning functionalities provided by\nmajor cloud service providers, which we call machine learning clouds. Machine\nlearning clouds hold the promise of hiding all the sophistication of running\nlarge-scale machine learning: Instead of specifying how to run a machine\nlearning task, users only specify what machine learning task to run and the\ncloud figures out the rest. Raising the level of abstraction, however, rarely\ncomes free - a performance penalty is possible. How good, then, are current\nmachine learning clouds on real-world machine learning workloads?\n  We study this question with a focus on binary classication problems. We\npresent mlbench, a novel benchmark constructed by harvesting datasets from\nKaggle competitions. We then compare the performance of the top winning code\navailable from Kaggle with that of running machine learning clouds from both\nAzure and Amazon on mlbench. Our comparative study reveals the strength and\nweakness of existing machine learning clouds and points out potential future\ndirections for improvement.",
    "link": "http://arxiv.org/abs/1707.09562v3",
    "published": "2017-07-29T21:59:18Z"
  },
  "69": {
    "pdf_path": "data/pdfs/machine learning_paper_592.pdf",
    "text_excerpt": "Machine Learning, 37, 241–275 (1999)\nc°1999 Kluwer Academic Publishers. Manufactured in The Netherlands.\nLearning Function-Free Horn Expressions\nRONI KHARDON roni@dcs.ed.ac.uk\nDivision of Informatics,University of Edinburgh, The King’s Buildings, Edinburgh EH9 3JZ, Scotland\nEditors: Jonathan Baxter and Nicol` o Cesa-Bianchi\nAbstract. The problem of learning universally quantiﬁed function free ﬁrst order Horn expressions is studied.\nSeveralmodelsoflearningfromequivalenceandmembershipqueriesareconsidered,includingthemodelwhereinterpretations are examples (Learning from Interpretations), the model where clauses are examples (LearningfromEntailment),modelswhereextensionalorintentionalbackgroundknowledgeisgiventothelearner(asdonein Inductive Logic Programming), and the model where the reasoning performance of the learner rather thanidentiﬁcationisofinterest(LearningtoReason).Wepresentlearningalgorithmsforallthesetasksfortheclassofuniversally quantiﬁed function free Horn expressions. The algorithms are polynomial in the number of predicatesymbols in the language and the number of clauses in the target Horn expression but exponential in the arity ofpredicates and the number of universally quantiﬁed variables. We also provide lower bounds for these tasks bywayofcharacterisingtheVC-dimensionofthisclassofexpressions.Theexponentialdependenceonthenumberof variables is the main gap between the lower and upper bounds.\nKeywords: computational learning theory, inductive logic programming, Horn expressions, direct products\n1. Introduction\nWe study the problem of exactly identifying ﬁrst-order Horn expressions using Angluin’s\n(1988)modelofexactlearning.Muchoftheworkinlearningtheoryhasdealtwithlearningof Boolean expressions in propositional logic. Early treatments of relational expressionswere given by Valiant (1985) and Haussler (1989), but only recently more attention wasgiventothesubjectintheframeworkofInductiveLogicProgramming(seee.g.Muggleton& De Raedt, 1994; Cohen, 1995a; Nienhu",
    "title": "Learning Function-Free Horn Expressions",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1007610422992",
    "published": "2002-12-22T05:54:50Z"
  },
  "70": {
    "pdf_path": "data/pdfs/machine learning_paper_1.pdf",
    "text_excerpt": "lecture notes:\nOptimization for Machine Learning\nversion 0.57\nAll rights reserved.\nElad Hazan1\n1www.cs.princeton.edu/ ~ehazanarXiv:1909.03550v1  [cs.LG]  8 Sep 2019iiPreface\nThis text was written to accompany a series of lectures given at the Machine\nLearning Summer School Buenos Aires, following a lecture series at the\nSimons Center for Theoretical Computer Science, Berkeley. It was extended\nfor the course COS 598D - Optimization for Machine Learning, Princeton\nUniversity, Spring 2019.\nI am grateful to Paula Gradu for proofreading parts of this manuscript.\nI'm also thankful for the help of the following students and colleagues for\ncorrections and suggestions to this text: Udaya Ghai, John Hallman, No\u0013 e\nPion, Xinyi Chen.\niiiiv Preface\nFigure 1: Professor Arkadi Nemirovski, Pioneer of mathematical optimiza-\ntionContents\nPreface iii\n1 Introduction 3\n1.1 Examples of optimization problems in machine learning . . . 4\n1.1.1 Empirical Risk Minimization . . . . . . . . . . . . . . 4\n1.1.2 Matrix completion and recommender systems . . . . . 6\n1.1.3 Learning in Linear Dynamical Systems . . . . . . . . 7\n1.2 Why is mathematical programming hard? . . . . . . . . . . . 8\n1.2.1 The computational model . . . . . . . . . . . . . . . . 8\n1.2.2 Hardness of constrained mathematical programming . 9\n2 Basic concepts in optimization and analysis 11\n2.1 Basic de\fnitions and the notion of convexity . . . . . . . . . . 11\n2.1.1 Projections onto convex sets . . . . . . . . . . . . . . . 13\n2.1.2 Introduction to optimality conditions . . . . . . . . . . 14\n2.1.3 Solution concepts for non-convex optimization . . . . 15\n2.2 Potentials for distance to optimality . . . . . . . . . . . . . . 16\n2.3 Gradient descent and the Polyak stepsize . . . . . . . . . . . 18\n2.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.5 Bibliographic remarks . . . . . . . . . . . . . . . . . . . . . . 23\n3 Stochastic Gradient Descent 25\n3.1 Training feedforward neural networks . . . . . . . .",
    "title": "Lecture Notes: Optimization for Machine Learning",
    "abstract": "Lecture notes on optimization for machine learning, derived from a course at\nPrinceton University and tutorials given in MLSS, Buenos Aires, as well as\nSimons Foundation, Berkeley.",
    "link": "http://arxiv.org/abs/1909.03550v1",
    "published": "2019-09-08T21:49:42Z"
  },
  "71": {
    "pdf_path": "data/pdfs/Machine Learning for Clinical Predictive Analytics_paper_7.pdf",
    "text_excerpt": "Zurich, June 2020  – V2.1 \n \n \nMachine learning -based clinical prediction modeling  \nA practical guide for clinicians  \n \n \n \n \n \nJulius M. Kernbach, MD  \nNeurosurgical Artificial Intelligence Lab Aachen (NAILA), RWTH University Aachen, Germany  \n& \nVictor E. Staartjes, BMed  \nMachine Intelligence in Clinical Neuroscience (MICN) Laboratory, University Hospital Zurich, Switzerland  \n \n \n \n \nContent s: \nPart I: Introduction and general principles  \nPart II: Generalization and overfitting  \nPart III: Evaluation and points of significance  \nPart IV: A practical approach to binary classification problems  \nPart V: A practical approach to regression  problems  \n Machine learning -based clinical prediction modeling  \nPart I: Introduction and  general principles  \n \n*Julius M. Kernbach1, MD; Victor E. Staartjes2, BMed  \n \n1 Neurosurgical Artificial Intelligence Lab oratory  Aachen (NAILA), Department of Neurosurgery,  \nRWTH Aachen University Hospital, Aachen, Germany  \n2 Machine Intelligence in Clinical Neuroscience (MICN) Laboratory, Department of Neurosurgery,  \nUniversity Hospital Zurich, Clinical Neuroscience Center, University of Zurich, Zurich, Switzerland  \n \n \n \nCorresponding Author  \nVictor E. Staartjes, BMed  \nMachine Intelligence in Clinical Neuroscience (MICN) Laboratory  \nDepartment of Neurosurgery, University Hospital Zurich  \nClinical Neuroscience Center, University of Zurich  \nFrauenklinikstrasse 10  \n8091 Zurich, Switzerland  \nTel +41 44 2 55 2660  \nFax +41 44 255 4505  \nWeb www.micnlab.com  \nE-Mail victoregon.staartjes@usz.ch  \n \n* J.M. Kernbach and V.E. Staartjes have contributed equally to this series , and share first authorship.  \n \n \n \n \n \n \n \n \n \n \nAbstract \nAs analytical machine learning tools become readily available for clinicians to use, the understanding of key concepts \nand the awareness of analytical pitfalls are increasingly required for clinicians, investigators, reviewers and editors, \nwho eve n as experts in the ir clinical field, sometim",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "72": {
    "pdf_path": "data/pdfs/machine learning_paper_663.pdf",
    "text_excerpt": "Machine Learning, 53, 199–233, 2003\nc/circlecopyrt2003 Kluwer Academic Publishers. Manufactured in The Netherlands.\nLearning from Cluster Examples\nTOSHIHIRO KAMISHIMA mail@kamishima.net (http://www.kamishima.net/)\nFUMIO MOTOYOSHI f.motoyoshi@aist.go.jpNational Institue of Advanced Industrial Science and Technology (AIST), AIST Tsukuba Central 2,1-1-1 Umezono, Tsukuba, Ibaraki, 305-8568 Japan\nEditor: Douglas Fisher\nAbstract. Learning from cluster examples (LCE) is a hybrid task combining features of two common grouping\ntasks: learning from examples and clustering. In LCE, each training example is a partition of objects. The taskis then to learn from a training set, a rule for partitioning unseen object sets. A general method for learning suchpartitioning rules is useful in any situation where explicit algorithms for deriving partitions are hard to formalize,while individual examples of correct partitions are easy to specify. In the past, clustering techniques have beenapplied to such problems, despite being essentially unsuited to the task. We present a technique that has qualitativeadvantages over standard clustering approaches. We demonstrate these advantages by applying our method toproblems in two domains; one with dot patterns and one with more realistic vector-data images.\nKeywords: learning from examples, clustering, dot pattern, image segmentation\n1. Introduction\nClustering is a typical grouping task that involves partitioning a given object set into subsets,\nsuch that objects in the same subset are “similar.” (We use the term objects to refer to entities\nthat will be grouped.) Clustering is carried out based on prespeciﬁed knowledge in the form,for example, of preference potential functions or dissimilarity measures.\nIn this paper, we advocate the use not of prespeciﬁed knowledge, but of examples for\npartitioning. That is, we try to acquire a partitioning rule from an example set consistingof pairs of an object set and a true partition for the object set. Th",
    "title": "Learning from Cluster Examples",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1026351106797",
    "published": "2003-11-03T19:30:08Z"
  },
  "73": {
    "pdf_path": "data/pdfs/machine learning_paper_538.pdf",
    "text_excerpt": " \n \n \n*Corresponding author. Email:  alokchauhan.1983@gmail.com  \n                      \n \n \n \nReview  Article  \nMachine Learning Basics: A Comprehensive Guide.  A Review  \nAlok Singh Chauhan1,*,, H Mary Henrietta2 ,  \n \n1 School of Computer Applications and Technology, Galgotias University, Greater Noida, India  \n2 Saveetha Engineering College, Chennai, Tamil Nadu 602105. India.  \n \nA R T I C L E  I N F O  \n \nArticle  Histo ry \nReceived   07  Mar 2023  \nAccepted   03  May 2023  \nPublished  25 Jun 2023  \n \nKeywords  \nMachine learning  \ncomprehensive guide  \npractical applications  \nethical considerations  \n \n A B S T R A C T   \n \nThe domain of machine learning has experienced an unparalleled increase in attention and \nimplementation, becoming an essential component of diverse businesses. This review paper provides a \nthorough analysis of the comprehensive handbook named \"Machine Learning Basics: A Compreh ensive \nGuide.\" Written by [Dr. Jane Doe], this guide has become a vital reference for those at all levels of \nexpertise seeking to comprehend and traverse the intricate realm of machine learning.  \n \n \n \n \n \n  \n  \n1. INTRODUCTION  \nThe introduction acts as an engaging entrance into the complex realm of machine learning, expertly constructed by the \nauthor, Dr. Jane Doe. Dr. Doe skillfully establishes the foundation for a comprehensive examination of the fundamental \nprinciples of machine learning, adeptly leading readers through the main goals of the guide [1]. This introductory section not \nonly engages the reader's interest but also provides a clear description of the learning path that lies ahead.  Dr. Doe carefully \nfocuses on clearly stating the goals of the guide, ensuring that it is specifically tailored to meet the needs of a wide rang e of \npeople. Regardless of the reader's level of expertise, whether they are a beginner exploring the field of ma chine learning or \nan experienced professional looking to enhance their knowledge, the guide guarantees to pr",
    "title": "Machine Learning Basics: A Comprehensive Guide.  A Review",
    "abstract": "<jats:p>The domain of machine learning has experienced an unparalleled increase in attention and implementation, becoming an essential component of diverse businesses. This review paper provides a thorough analysis of the comprehensive handbook named \"Machine Learning Basics: A Comprehensive Guide.\" Written by [Dr. Jane Doe], this guide has become a vital reference for those at all levels of expertise seeking to comprehend and traverse the intricate realm of machine learning.</jats:p>",
    "link": "https://doi.org/10.58496/bjml/2023/006",
    "published": "2024-04-27T16:51:51Z"
  },
  "74": {
    "pdf_path": "data/pdfs/machine learning_paper_314.pdf",
    "text_excerpt": "A Performance and Cost Assessment of Machine Learning\nInteratomic Potentials\nYunxing Zuo, Chi Chen, Xiangguo Li, Zhi Deng, Yiming Chen, and Shyue Ping Ong∗\nDepartment of NanoEngineering, University of California San Diego,\n9500 Gilman Dr, Mail Code 0448, La Jolla, CA 92093-0448, United States\nJ¨ org Behler\nUniversit¨ at G¨ ottingen, Institut f¨ ur Physikalische Chemie,\nTheoretische Chemie, Tammannstraße 6, 37077 G¨ ottingen, Germany\nG´ abor Cs´ anyi\nDepartment of Engineering, University of Cambridge,\nTrumpington Street, Cambridge, CB2 1PZ, United Kingdom\nAlexander V. Shapeev\nSkolkovo Institute of Science and Technology,\nSkolkovo Innovation Center, Building 3, Moscow, 143026, Russia\nAidan P. Thompson and Mitchell A. Wood\nCenter for Computing Research, Sandia National Laboratories,\nAlbuquerque, New Mexico 87185, United States\n1arXiv:1906.08888v4  [physics.comp-ph]  24 Jul 2019Abstract\nMachine learning of the quantitative relationship between local environment descriptors and the\npotential energy surface of a system of atoms has emerged as a new frontier in the development\nof interatomic potentials (IAPs). Here, we present a comprehensive evaluation of ML-IAPs based\non four local environment descriptors — Behler-Parrinello symmetry functions, smooth overlap of\natomic positions (SOAP), the Spectral Neighbor Analysis Potential (SNAP) bispectrum compo-\nnents, and moment tensors — using a diverse data set generated using high-throughput density\nfunctional theory (DFT) calculations. The data set comprising bcc (Li, Mo) and fcc (Cu, Ni) met-\nals and diamond group IV semiconductors (Si, Ge) is chosen to span a range of crystal structures\nand bonding. All descriptors studied show excellent performance in predicting energies and forces\nfar surpassing that of classical IAPs, as well as predicting properties such as elastic constants and\nphonon dispersion curves. We observe a general trade-oﬀ between accuracy and the degrees of\nfreedom of each model, and consequently computationa",
    "title": "A Performance and Cost Assessment of Machine Learning Interatomic Potentials.",
    "abstract": "Machine learning of the quantitative relationship between local environment descriptors and the potential energy surface of a system of atoms has emerged as a new frontier in the development of interatomic potentials (IAPs). Here, we present a comprehensive evaluation of ML-IAPs based on four local environment descriptors --- atom-centered symmetry functions (ACSF), smooth overlap of atomic positions (SOAP), the Spectral Neighbor Analysis Potential (SNAP) bispectrum components, and moment tensors --- using a diverse data set generated using high-throughput density functional theory (DFT) calculations. The data set comprising bcc (Li, Mo) and fcc (Cu, Ni) metals and diamond group IV semiconductors (Si, Ge) is chosen to span a range of crystal structures and bonding. All descriptors studied show excellent performance in predicting energies and forces far surpassing that of classical IAPs, as well as predicting properties such as elastic constants and phonon dispersion curves. We observe a general trade-off between accuracy and the degrees of freedom of each model, and consequently computational cost. We will discuss these trade-offs in the context of model selection for molecular dynamics and other applications.",
    "link": "https://www.semanticscholar.org/paper/60446c372811c25acc1e47b044e8e7458f0a4986",
    "published": "2019-06-20"
  },
  "75": {
    "pdf_path": "data/pdfs/Machine Learning for Clinical Predictive Analytics_paper_3.pdf",
    "text_excerpt": "Explainable Prediction of Adverse Outcomes Using\nClinical Notes\nJustin R. Lovelace\nTexas A&M UniversityNathan C. Hurley\nTexas A&M University\nAdrian D. Haimovich\nYale School of MedicineBobak J. Mortazavi\nTexas A&M University\nAbstract\nClinical notes contain a large amount of clinically valuable information that is\nignored in many clinical decision support systems due to the difﬁculty that comes\nwith mining that information. Recent work has found success leveraging deep learn-\ning models for the prediction of clinical outcomes using clinical notes. However,\nthese models fail to provide clinically relevant and interpretable information that\nclinicians can utilize for informed clinical care. In this work, we augment a popular\nconvolutional model with an attention mechanism and apply it to unstructured\nclinical notes for the prediction of ICU readmission and mortality. We ﬁnd that\nthe addition of the attention mechanism leads to competitive performance while\nallowing for the straightforward interpretation of predictions. We develop clear\nvisualizations to present important spans of text for both individual predictions and\nhigh-risk cohorts. We then conduct a qualitative analysis and demonstrate that our\nmodel is consistently attending to clinically meaningful portions of the narrative\nfor all of the outcomes that we explore.\n1 Introduction\nThe intensive care unit (ICU) provides care for hospital patients with serious illnesses or injuries.\nIntensive care is extremely resource intensive so decisions to discharge patients from the ICU must\nbe made in order to allocate limited resources efﬁciently. An accurate predictor for the likelihood of\nadverse outcomes would be useful for planning the ICU discharge process and determining the level\nof monitoring that the patient would receive after discharge from the ICU. In this work, we focus on\npredicting the likelihood of ICU readmission and mortality using information available at the time of\nICU discharge.\nPrevious work has found",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "76": {
    "pdf_path": "data/pdfs/machine learning_paper_49.pdf",
    "text_excerpt": "© 2018 IEEE               Accepted for publication in IEEE Security and Privacy Magazine \n \n1 \n Privacy Preserving Machine Learning: Threats and Solutions \n \nMohammad Al-Rubaie | Iowa State University \nJ. Morris Chang | University of South Florida \n \nAbstract:  For privacy concerns to be addressed adequately in today’s machine learning systems, \nthe knowledge gap between the machine learning and privacy communities must be bridged. \nThis article aims to provide an introduction to the intersection of both fields with special \nemphasis on the techniques used to protect the data. \nKeywords: privacy, machine learning, differential privacy, cryptography, dimensionality \nreduction. \n \nIntroduction \nOur search queries, browsing history, purchase transactions, the videos we watch, and our \nmovies' preferences are but few types of information that are being collected and stored on daily \nbasis. This data collection happens within our mobile devices and computers, on the streets, and \neven in our own offices and homes. Such private data is being used for a variety of machine \nlearning applications. \nMachine learning (ML) is being increasingly utilized for a variety of applications from intrusion \ndetection to recommending new movies. Some ML applications require private individuals’ data. \nSuch private data is uploaded to centralized locations in clear text for ML algorithms to extract \npatterns, and build models from them. The problem is not limited to the threats associated with \nhaving all this private data exposed to insider threat at these companies, or outsider threat if the \ncompanies holding these data sets were hacked. In addition, it is possible to glean extra \ninformation about the private data sets even if the data was anonymized1, or the data itself and \nthe ML models were inaccessible and only the testing results were revealed2. \nIn this article, we will describe machine learning tasks and applications, and the potential threats \nassociated with current methods ",
    "title": "Privacy Preserving Machine Learning: Threats and Solutions",
    "abstract": "For privacy concerns to be addressed adequately in current machine learning\nsystems, the knowledge gap between the machine learning and privacy communities\nmust be bridged. This article aims to provide an introduction to the\nintersection of both fields with special emphasis on the techniques used to\nprotect the data.",
    "link": "http://arxiv.org/abs/1804.11238v1",
    "published": "2018-03-27T15:10:31Z"
  },
  "77": {
    "pdf_path": "data/pdfs/machine learning_paper_2.pdf",
    "text_excerpt": "arXiv:1811.04422v1  [cs.LG]  11 Nov 2018An Optimal Control View of Adversarial Machine Learning\nXiaojin Zhu\nDepartment of Computer Sciences, University of Wisconsin-Madiso n\nAbstract\nI describe an optimal control view of adversarial machine le arning, where the dynamical system is the\nmachine learner, the input are adversarial actions, and the control costs are deﬁned by the adversary’s\ngoals to do harm and be hard to detect. This view encompasses m any types of adversarial machine\nlearning, including test-item attacks, training-data poi soning, and adversarial reward shaping. The view\nencourages adversarial machinelearningresearcher touti lize advancesincontroltheoryandreinforcement\nlearning.\n1 Adversarial Machine Learning is not Machine Learning\nMachine learning has its mathematical foundation in concentration in equalities. This is a consequence of\nthe independent and identically-distributed (i.i.d.) data assumption. I n contrast, I suggest that adversarial\nmachine learning may adopt optimal control as its mathematical fou ndation [3,25]. There are telltale signs:\nadversarial attacks tend to be subtle and have peculiar non-i.i.d. st ructures – as control input might be.\n2 Optimal Control\nI will focus on deterministic discrete-time optimal control because it matches many existing adversarial\nattacks. Extensions to stochastic and continuous control are r elevant to adversarial machine learning, too.\nThe system to be controlled is called the plant, which is deﬁned by the s ystem dynamics:\nxt+1=f(xt,ut) (1)\nwherext∈Xtis the state of the system, ut∈Utis the control input, and Utis the control constraint\nset. The function fdeﬁnes the evolution of state under external control. The time ind extranges from 0\ntoT−1, and the time horizon Tcan be ﬁnite or inﬁnite. The quality of control is speciﬁed by the runn ing\ncost:\ngt(xt,ut) (2)\nwhich deﬁnes the step-by-step control cost, and the terminal c ost for ﬁnite horizon:\ngT(xT) (3)\nwhich deﬁnes the quality of the ﬁnal state. Th",
    "title": "An Optimal Control View of Adversarial Machine Learning",
    "abstract": "I describe an optimal control view of adversarial machine learning, where the\ndynamical system is the machine learner, the input are adversarial actions, and\nthe control costs are defined by the adversary's goals to do harm and be hard\nto detect. This view encompasses many types of adversarial machine learning,\nincluding test-item attacks, training-data poisoning, and adversarial reward\nshaping. The view encourages adversarial machine learning researcher to utilize\nadvances in control theory and reinforcement learning.",
    "link": "http://arxiv.org/abs/1811.04422v1",
    "published": "2018-11-11T14:28:34Z"
  },
  "78": {
    "pdf_path": "data/pdfs/machine learning_paper_226.pdf",
    "text_excerpt": "Principles and Practice of Explainable\nMachine Learning\nVaishak Belle1,2and Ioannis Papantonis1*\n1School of Informatics, University of Edinburgh, Edinburgh, United Kingdom,2Alan Turing Institute, London, United Kingdom\nArtiﬁcial intelligence (AI) provides many opportunities to improve private and public life.\nDiscovering patterns and structures in large troves of data in an automated manner is acore component of data science, and currently drives applications in diverse areas such ascomputational biology, law and ﬁnance. However, such a highly positive impact is coupled\nwith a signi ﬁcant challenge: how do we understand the decisions suggested by these\nsystems in order that we can trust them? In this report, we focus speci ﬁcally on data-driven\nmethods —machine learning (ML) and pattern recognition models in particular —so as to\nsurvey and distill the results and observations from the literature. The purpose of this reportcan be especially appreciated by noting that ML models are increasingly deployed in awide range of businesses. However, with the increasing prevalence and complexity ofmethods, business stakeholders in the very least have a growing number of concernsabout the drawbacks of models, data-speci ﬁc biases, and so on. Analogously, data\nscience practitioners are often not aware about approaches emerging from the academicliterature or may struggle to appreciate the differences between different methods, so endup using industry standards such as SHAP. Here, we have undertaken a survey to helpindustry practitioners (but also data scientists more broadly) understand the ﬁeld of\nexplainable machine learning better and apply the right tools. Our latter sections builda narrative around a putative data scientist, and discuss how she might go aboutexplaining her models by asking the right questions. From an organization viewpoint,after motivating the area broadly, we discuss the main developments, including theprinciples that allow us to study transparent models v",
    "title": "Principles and Practice of Explainable Machine Learning",
    "abstract": "Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods—machine learning (ML) and pattern recognition models in particular—so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions.",
    "link": "https://www.semanticscholar.org/paper/b5b98051b65da6b1b3b579862b0407d48c5bef48",
    "published": "2020-09-18"
  },
  "79": {
    "pdf_path": "data/pdfs/machine learning_paper_217.pdf",
    "text_excerpt": " \n \n Vol. 01, No. 02, pp. 140 –147 (2020 ) \nISSN: 2708 -0757 \n \nJOURNAL OF APPLIED SCIENCE AND TECHNOLOGY TRENDS  \n \nwww.jastt.org   \n \ndoi:10.38094/ jastt1457    \n140 \nA Review on Linear Regression Comprehensive in Machine \nLearning  \n \nDastan Hussen Maulud1, *, Adnan Mohsin Abdulazeez2  \n1 Duhok Polytechnic University, Duhok, Kurdistan Region, Iraq, mailto:dastan.mawlud@mhe -krg.org  \n2 Duhok Polytechnic University, Duhok, Kurdistan Region , Iraq, Adnan.mohsin@dpu.edu.krd  \n*Correspondence : dastan.mawlud@mhe -krg.org  \n \nAbstract  \nPerhaps one of the most common and comprehensive statistical and machine learning algorithms are linear regression. Linear \nregression is used to find a linear relationship between one or more predictors. The linear regression has two types: simple regression and \nmultiple regression (MLR). This paper discusses various works by different researchers on linear regression and polynomial regression \nand compares their performance using the best approach to optimize prediction and precision. Almost all of the articles analy zed in this \nreview is focused on datasets; in order to determine a model 's efficiency, it must be correlated with the actual values obtained for the \nexplanatory variables.  \n \nKeywords : Regression, Simple Linear Regression, Multiple Linear Regression, polynomial Regression, least square method.  \nReceived: September  15th, 2020  / Acc epted: December 29th, 2020  / Online: December 31th, 2020  \n \nI. INTRODUCTION  \nMachine learning  [1-5] is commonly used in diverse fields to \nsolve difficult problems that cannot be readily solved in based \non computer approaches. The linear regression is one of the \nsimplest  and most common machine learning algorithms. It is a \nmathematical approach used to perform predictive analysis. \nLinear regression allows continuous/real or mathematical \nvariables projections. Sir Francis Galton first suggested the \nconcept of linear regres sion in 1894.Linear regression [6-8] is \na mathematic",
    "title": "A Review on Linear Regression Comprehensive in Machine Learning",
    "abstract": "Perhaps one of the most common and comprehensive statistical and machine learning algorithms are linear regression. Linear regression is used to find a linear relationship between one or more predictors. The linear regression has two types: simple regression and multiple regression (MLR). This paper discusses various works by different researchers on linear regression and polynomial regression and compares their performance using the best approach to optimize prediction and precision. Almost all of the articles analyzed in this review is focused on datasets; in order to determine a model's efficiency, it must be correlated with the actual values obtained for the explanatory variables.",
    "link": "https://www.semanticscholar.org/paper/99afa67e28780754907b19b688bf2b35eb22e578",
    "published": null
  },
  "80": {
    "pdf_path": "data/pdfs/machine learning_paper_9.pdf",
    "text_excerpt": "Understanding Bias in Machine Learning\nJindong Gu1;2, Daniela Oelke2\n1The University of Munich\n2Siemens AG, Corporate Technology\nAbstract. Bias is known to be an impediment to fair decisions in many\ndomains such as human resources, the public sector, health care etc. Re-\ncently, hope has been expressed that the use of machine learning methods\nfor taking such decisions would diminish or even resolve the problem. At\nthe same time, machine learning experts warn that machine learning\nmodels can be biased as well.\nIn this article, our goal is to explain the issue of bias in machine learning\nfrom a technical perspective and to illustrate the impact that biased data\ncan have on a machine learning model. To reach such a goal, we develop\ninteractive plots to visualizing the bias learned from synthetic data. The\ninteractive plots are available1.\nKeywords: Bias in Machine Learning \u0001Visualization in Explainable AI.\n1 How does bias get into a machine learning model?\nTo be able to let the machine take a decision automatically, we have to teach it\nhow to do it right. One way to do so is to formulate explicitly as a rule when\nto take which decision. However, many situations are too complex for this. So\nwhat can we do? A central idea of machine learning is that the machine learns\nthe rules and patterns by itself from examples. The examples are decisions that\nhumans have taken in the past together with the information about the subject\n(the data) that they based their decision upon. We call this kind of data training\ndata, because the machine uses it to learn to take a decision as a human would\nhave done it.\nNow it should be clear, why a machine learning model can be biased as well:\nIf the data or the decisions taken on it are biased and the machine uses them as\nan example, then the machine is going to incorporate this bias into the model.\nIt learns the bias from the examples given to it.\nSo the \frst thing that you should keep in mind is this: Bias gets into the\nmodel through the dat",
    "title": "Understanding Bias in Machine Learning",
    "abstract": "Bias is known to be an impediment to fair decisions in many domains such as\nhuman resources, the public sector, health care etc. Recently, hope has been\nexpressed that the use of machine learning methods for taking such decisions\nwould diminish or even resolve the problem. At the same time, machine learning\nexperts warn that machine learning models can be biased as well. In this\narticle, our goal is to explain the issue of bias in machine learning from a\ntechnical perspective and to illustrate the impact that biased data can have on\na machine learning model. To reach such a goal, we develop interactive plots to\nvisualizing the bias learned from synthetic data.",
    "link": "http://arxiv.org/abs/1909.01866v1",
    "published": "2019-09-02T20:36:19Z"
  },
  "81": {
    "pdf_path": "data/pdfs/machine learning_paper_73.pdf",
    "text_excerpt": "P\nS\nI\n \nD\nr\naft \nS\np\neci\nfi\ncat\ni\no\nn\nM\na\nr\nk\n \nR\ne\ni\nd\n,\n \nJ\na\nm\ne\ns\n \nM\no\nn\nt\ng\no\nm\ne\nr\ny\n,\n \nB\na\nr\nr\ny\n \nD\nr\na\nk\ne\n,\n \nA\nv\nr\na\nh\na\nm\n \nR\nu\nd\ne\nr\nm\na\nn\nV\nersion 2. 12 September 2013\n1\n \nI\nn\nt\nr\no\nd\nu\nc\nt\nio\nn\n1\n.\n1\n \nM\no\nd\ne\nllin\ng\n \nM\na\nc\nh\nin\ne\n \nL\ne\na\nr\nn\nin\ng\n \nP\nr\no\nb\nle\nm\ns\n1\n.\n2\n \nD\ne\ns\nig\nn\n \nD\ne\nc\nis\nio\nn\ns\n1\n.\n2\n.\n1\n \nC\nh\no\nic\ne\n \no\nf\n \nA\nr\nc\nh\nit\ne\nc\nt\nu\nr\na\nl \nS\nt\ny\nle\n1\n.\n2\n.\n2\n \nC\nh\no\nic\ne\n \no\nf\n \nR\ne\ns\no\nu\nr\nc\ne\ns\n1\n.\n2\n.\n3\n \nC\nh\no\nic\ne\n \no\nf\n \nI\nn\ns\nt\na\nn\nc\ne\n \nR\ne\np\nr\ne\ns\ne\nn\nt\na\nt\nio\nn\n1\n.\n2\n.\n4\n \nU\ns\ne\n \no\nf\n \nA\nt\nt\nr\nib\nu\nt\ne\ns\n \na\nn\nd\n \nS\nc\nh\ne\nm\na\n1\n.\n2\n.\n5\n \nC\no\nm\np\no\ns\nit\nio\nn\n \nv\nia\n \nR\ne\nf\ne\nr\ne\nn\nc\ne\ns\n1\n.\n2\n.\n6\n \nR\nE\nS\nT\nf\nu\nln\ne\ns\ns\n \no\nf\n \nt\nh\ne\n \nA\nr\nc\nh\nit\ne\nc\nt\nu\nr\ne\n1\n.\n3\n \nV\na\nlu\ne\ns\n \na\nn\nd\n \nS\nc\nh\ne\nm\na\n1\n.\n3\n.\n1\n \nR\nic\nh\n \nV\na\nlu\ne\ns\n2\n \nT\nh\ne\n \nP\nS\nI\n \nS\ne\nr\nv\nic\ne\n \nA\nr\nc\nh\nit\ne\nc\nt\nu\nr\ne\n2\n.\n1\n \nC\no\nn\nv\ne\nn\nt\nio\nn\ns\n2\n.\n2\n \nD\nis\nc\no\nv\ne\nr\nin\ng\n \na\n \nP\nS\nI\n \nS\ne\nr\nv\nic\ne\n2\n.\n2\n.\n1\n \nD\nis\nc\no\nv\ne\nr\nin\ng\n \nR\ne\nla\nt\ne\nd\n \nS\ne\nr\nv\nic\ne\ns\n \na\nn\nd\n \nA\nc\nt\nio\nn\ns\n2\n.\n3\n \nS\nc\nh\ne\nm\na\n2\n.\n4\n \nT\nr\na\nn\ns\nf\no\nr\nm\ne\nr\ns\n2\n.\n4\n.\n1\n \nT\nr\na\nn\ns\nf\no\nr\nm\ne\nr\ns\n \na\ns\n \nR\ne\ns\no\nu\nr\nc\ne\ns\n2\n.\n4\n.\n1\n.\n1\n \nT\nr\na\nn\ns\nf\no\nr\nm\ne\nr\n \nR\ne\np\nr\ne\ns\ne\nn\nt\na\nt\nio\nn\n2\n.\n4\n.\n1\n.\n2\n \nA\np\np\nly\n2\n.\n4\n.\n1\n.\n3\n \nJ\no\nin\n2\n.\n5\n \nR\ne\nla\nt\nio\nn\ns\n \na\nn\nd\n \nI\nn\ns\nt\na\nn\nc\ne\ns\n2\n.\n5\n.\n1\n \nR\ne\nla\nt\nio\nn\ns\n \na\ns\n \nR\ne\ns\no\nu\nr\nc\ne\ns\n2\n.\n5\n.\n2\n \nS\ne\nle\nc\nt\nin\ng\n \nS\nu\nb\ns\ne\nt\ns\n \no\nf\n \nI\nn\ns\nt\na\nn\nc\ne\ns\n2\n.\n6\n \nA\nt\nt\nr\nib\nu\nt\ne\ns\n2\n.\n6\n.\n1\n \nA\nt\nt\nr\nib\nu\nt\ne\n \nC\no\nm\np\no\ns\nit\nio\nn\n2\n.\n6\n.\n2\n \nA\nt\nt\nr\nib\nu\nt\ne\ns\n \na\ns\n \nR\ne\ns\no\nu\nr\nc\ne\ns\n2\n.\n6\n.\n2\n.\n1\n \nC\nr\ne\na\nt\ne\n2\n.\n6\n.\n2\n.\n2\n \nA\nt\nt\nr\nib\nu\nt\ne\n \nR\ne\np\nr\ne\ns\ne\nn\nt\na\nt\nio\nn\n2\n.\n6\n.\n2\n.\n3\n \nA\np\np\nly\n2\n.\n6\n.\n2\n.\n4\n \nJ\no\nin\n \nw\nit\nh\n \na\n \nT\nr\na\nn\ns\nf\no\nr\nm\ne\nr\n2\n.\n6\n.\n2\n.\n5\n \nD\ne\nle\nt\ne\n2\n.\n7\n \nL\ne\na\nr\nn\ne\nr\ns\n2\n.\n7\n.\n1\n \nT\na\ns\nk\ns\n \na\nn\nd\n \nT\na\ns\nk\n \nS\nc\nh\ne\nm\na\n2\n.\n7\n.\n2\n \nL\ne\na\nr\nn\ne\nr\ns\n \na\ns\n \nR\ne\ns\no\nu\nr\nc\ne\ns\n2\n.\n7\n.\n2\n.\n1\n \nL\ne\na\nr\nn\ne\nr\n \nR\ne\np\nr\ne\ns\ne\nn\nt\na\nt\nio\nn\n2\n.\n7\n.\n2\n.\n2\n \nP\nr\no\nc\ne\ns\ns\n2\n.\n8\n \nP\nr\ne\nd\nic\nt\no\nr",
    "title": "PSI Draft Specification",
    "abstract": "This document presents the draft specification for delivering machine\nlearning services over HTTP, developed as part of the Protocols and Structures\nfor Inference project, which concluded in 2013. It presents the motivation for\nproviding machine learning as a service, followed by a description of the\nessential and optional components of such a service.",
    "link": "http://arxiv.org/abs/2205.09488v1",
    "published": "2022-05-02T02:42:16Z"
  },
  "82": {
    "pdf_path": "data/pdfs/machine learning_paper_330.pdf",
    "text_excerpt": "TÉCNICAS  Y TECNOLOGÍAS  PARA LA COMPRA  INTELIGENTE  EN \nUNA EMPRESA  DEL SECTOR  TELECOMUNICACIONES  (IFX \nNETWORKS)  \n \n \n \n \n \n \n \n \nAUTOR  \nFERNANDO ADOLFO RINCON VALBUENA \nAdministrador de Empresas \nest.fernandoa.rincon@unimilitar.edu.co  \n \n \n \n \n \n \n \n \n \nArtículo Trabajo Final del programa de Especialización en Gerencia Logística Integral  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nESPECIALIZACIÓN EN GERENCIA LOGISTICA INTEGRAL  \nUNIVERSIDAD MILITAR NUEVA GRANADA  \nFACULTAD DE INGENIERÍA  DICIEMBRE, 2022 \nTÉCNICAS Y TECNOLOGÍAS PARA LA \nCOMPRA INTELIGENTE EN UNA EMPRESA \nDEL SECTOR TELECOMUNICACIONES (IFX \nNETWORKS) \n \nINTELLIGENT PURCHASING TECHNIQUES AND \nTECHNOLOGIES IN A TELECOMMUNICATIONS SECTOR \nCOMPANY \n \nEstudiante: Fernando Adolfo Rincon Valbuena \nCarrera: Administrador de Empresas \nest.fernandoa.rincon  @unimilitar.edu.co \n \nRESUMEN \n \nEl presente artículo presenta los resultados de un proceso de selección tecnológica. \nEsta se produce con el fin de identificar, evaluar y seleccionar los sistemas más \nadecuados para establecer una solución de compra inteligente para la empresa IFX \nNetworks, firma comercializadora de servicios de conectividad de redes para el \nsegmento empresarial con presencia en varios países de América Latina. En esta \nrevisión se analizan 7 artículos publicados entre 2017 y 2022 en revistas científicas de \nlas base de datos IEEE y ScienceDirect, que presentan resultados del desarrollo y \npuesta a prueba de software  y metodologías de inteligencia de negocios dirigidas a \nmejorar los procesos de compra y stock en empresas distribuidoras y \ncomercializadoras. Las tendencias de los resultados indican que la compra inteligente \ndepende de dos grandes aspectos: por un lado, investigaciones que apuestan por la \naplicación de algoritmos avanzados de inteligencia artificial y aprendizaje automático \nque permiten predecir la idoneidad de una opción de compra de acuerdo con las \nvariables definidas por la organización; por otro la",
    "title": "Machine Learning Made Easy: A Review of Scikit-learn Package in Python Programming Language",
    "abstract": "Machine learning is a popular topic in data analysis and modeling. Many different machine learning algorithms have been developed and implemented in a variety of programming languages over the past 20 years. In this article, we first provide an overview of machine learning and clarify its difference from statistical inference. Then, we review Scikit-learn, a machine learning package in the Python programming language that is widely used in data science. The Scikit-learn package includes implementations of a comprehensive list of machine learning methods under unified data and modeling procedure conventions, making it a convenient toolkit for educational and behavior statisticians.",
    "link": "https://www.semanticscholar.org/paper/a8fadb33a38f1096f84f64bd66345717a5bc3241",
    "published": "2019-02-20"
  },
  "83": {
    "pdf_path": "data/pdfs/machine learning_paper_17.pdf",
    "text_excerpt": "ICML 2015 AutoML Workshop\nAutoCompete: A Framework for Machine Learning\nCompetitions\nAbhishek Thakur thakur@aisbi.de andArtus Krohn-Grimberghe artus@aisbi.de\nAISBI, University of Paderborn, Germany\nAbstract\nIn this paper, we propose AutoCompete, a highly automated machine learning framework\nfor tackling machine learning competitions. This framework has been learned by us, vali-\ndated and improved over a period of more than two years by participating in online machine\nlearning competitions. It aims at minimizing human interference required to build a \frst\nuseful predictive model and to assess the practical di\u000eculty of a given machine learning\nchallenge. The proposed system helps in identifying data types, choosing a machine learn-\ning model, tuning hyper-parameters, avoiding over-\ftting and optimization for a provided\nevaluation metric. We also observe that the proposed system produces better (or compa-\nrable) results with less runtime as compared to other approaches.\nKeywords: auto-machine learning, predictive modelling\n1. Introduction\nIn the industry, business analysts are usually not concerned with the algorithms, feature\nselection, feature engineering or selection of appropriate hyperparameters. All they want\nis a fast track to a highly accurate predictive model which they can apply with minimum\nknowledge and e\u000bort on their problems and datasets. To satisfy this need, many \"one-click\"\nmachine learning platforms have emerged that speci\fcally target those users. Platforms such\nas Google Predict and BigML take the dataset as input from the end user and provide them\nwith a predictive model for the dataset and a web service to consume it but that is beyond\nthe scope of this paper.\nIn machine learning research, this topic has arrived under the umbrella term AutoML\nthat subsumes and integrates disjunct areas of research such as identi\fcation of the problem\n(classi\fcation/regression, identifying the type of data, types of features and selection of\nfeatures). Besides conn",
    "title": "AutoCompete: A Framework for Machine Learning Competition",
    "abstract": "In this paper, we propose AutoCompete, a highly automated machine learning\nframework for tackling machine learning competitions. This framework has been\nlearned by us, validated and improved over a period of more than two years by\nparticipating in online machine learning competitions. It aims at minimizing\nhuman interference required to build a first useful predictive model and to\nassess the practical difficulty of a given machine learning challenge. The\nproposed system helps in identifying data types, choosing a machine learn- ing\nmodel, tuning hyper-parameters, avoiding over-fitting and optimization for a\nprovided evaluation metric. We also observe that the proposed system produces\nbetter (or comparable) results with less runtime as compared to other\napproaches.",
    "link": "http://arxiv.org/abs/1507.02188v1",
    "published": "2015-07-08T15:07:39Z"
  },
  "84": {
    "pdf_path": "data/pdfs/machine learning_paper_75.pdf",
    "text_excerpt": "Fairness and Randomness in Machine Learning:\nStatistical Independence and Relativization*\nRabanus Derr\nUniversity of Tübingen\nrabanus.derr@uni-tuebingen.deRobert C. Williamson\nUniversity of Tübingen\nand Tübingen AI Centre\nbob.williamson@uni-tuebingen.de\nNovember 17, 2022\nAbstract\nFair Machine Learning endeavors to prevent unfairness arising in the context of machine\nlearning applications embedded in society. Despite the variety of deﬁnitions of fairness and\nproposed “fair algorithms,” there remain unresolved conceptual problems regarding fairness. In\nthis paper, we dissect the role of statistical independence in fairness and randomness notions\nregularly used in machine learning. Thereby, we are led to a suprising hypothesis: randomness\nand fairness can be considered equivalent concepts in machine learning.\nIn particular, we obtain a relativized notion of randomness expressed as statistical indepen-\ndence by appealing to Von Mises’ century-old foundations for probability. This notion turns\nout to be “orthogonal” in an abstract sense to the commonly used i.i.d.-randomness. Using\nstandard fairness notions in machine learning, which are deﬁned via statistical independence,\nwe then link the ex ante randomness assumptions about the data to the ex post requirements\nfor fair predictions. This connection proves fruitful: we use it to argue that randomness and\nfairness are essentially relative and that both concepts should reﬂect their nature as modeling\nassumptions in machine learning.\n1 Introduction\nUnder the name “Fair Machine Learning” researchers have attempted to tackle problems of injustice,\nfairness, discrimination arising in the context of machine learning applications embedded in society\n[Barocas et al., 2017]. Despite the variety of deﬁnitions of fairness and proposed “fair algorithms,”\nwe still lack a conceptual understanding of fairness in machine learning [Passi and Barocas, 2019,\nScantamburlo, 2021]. What does it mean for predictions to be fair? How does the st",
    "title": "Fairness and Randomness in Machine Learning: Statistical Independence\n  and Relativization",
    "abstract": "Fair Machine Learning endeavors to prevent unfairness arising in the context\nof machine learning applications embedded in society. Despite the variety of\ndefinitions of fairness and proposed \"fair algorithms\", there remain unresolved\nconceptual problems regarding fairness. In this paper, we dissect the role of\nstatistical independence in fairness and randomness notions regularly used in\nmachine learning. Thereby, we are led to a suprising hypothesis: randomness and\nfairness can be considered equivalent concepts in machine learning.\n  In particular, we obtain a relativized notion of randomness expressed as\nstatistical independence by appealing to Von Mises' century-old foundations for\nprobability. This notion turns out to be \"orthogonal\" in an abstract sense to\nthe commonly used i.i.d.-randomness. Using standard fairness notions in machine\nlearning, which are defined via statistical independence, we then link the ex\nante randomness assumptions about the data to the ex post requirements for fair\npredictions. This connection proves fruitful: we use it to argue that\nrandomness and fairness are essentially relative and that both concepts should\nreflect their nature as modeling assumptions in machine learning.",
    "link": "http://arxiv.org/abs/2207.13596v2",
    "published": "2022-07-27T15:55:05Z"
  },
  "85": {
    "pdf_path": "data/pdfs/machine learning_paper_287.pdf",
    "text_excerpt": "Choosing prediction over explanation in psychology: Lessons \nfrom machine learning\nTal Yarkoni  and Jacob Westfall\nUniversity of Texas at Austin\nAbstract\nPsychology has historically been concerned, first and foremost, with explaining the causal \nmechanisms that give rise to behavior. Randomized, tightly controlled experiments are enshrined \nas the gold standard of psychological research, and there are endless investigations of the various \nmediating and moderating variables that govern various behaviors. We argue that psychology’s \nnear-total focus on explaining the causes of behavior has led much of the field to be populated by \nresearch programs that provide intricate theories of psychological mechanism, but that have little \n(or unknown) ability to predict future behaviors with any appreciable accuracy. We propose that \nprinciples and techniques from the field of machine learning can help psychology become a more \npredictive science. We review some of the fundamental concepts and tools of machine learning \nand point out examples where these concepts have been used to conduct interesting and important \npsychological research that focuses on predictive research questions. We suggest that an increased \nfocus on prediction, rather than explanation, can ultimately lead us to greater understanding of \nbehavior.\nKeywords\nprediction; explanation; machine learning\nThe goal of scientific psychology is to understand human behavior. Historically this has \nmeant being able both to explain  behavior—that is, to accurately describe its causal \nunderpinnings—and to predict  behavior—that is, to accurately forecast behaviors that have \nnot yet been observed. In practice, however, these two goals are rarely distinguished. The \nunderstanding seems to be that the two are so deeply intertwined that there would be little \npoint in distinguishing them, except perhaps as a philosophical exercise. According to this \nunderstanding, explanation necessarily facilitates prediction; the model",
    "title": "Choosing Prediction Over Explanation in Psychology: Lessons From Machine Learning",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/7c63a6e6d3b31b14ae4236bfbd574ea37cab18a7",
    "published": "2017-08-25"
  },
  "86": {
    "pdf_path": "data/pdfs/machine learning_paper_499.pdf",
    "text_excerpt": "Machin e Learning , 18, 23-5 0 (1995 )\n© 1995 Kluwe r Academi c Publishers , Boston . Manufacture d in The Netherlands .\nProbabl y Almos t Discriminativ e Learnin g\nKENJ I YAMANISH I yamanisi@research.nj.nec.co m\nNEC  Research  Institute,  Inc.,  4 Independence  Way,  Princeton  NJ 08540\nEditor : Davi d Haussle r\nAbstract . This pape r develop s a new computationa l mode l for learnin g stochasti c rules , calle d PAD (Probabl y\nAlmos t Discriminative)-learnin g model , base d on statistica l hypothesi s testin g theory . The mode l deal s with the\nproble m of designin g a discriminatio n algorith m to test whethe r or not any give n test sequenc e of example s of\npairs of (instance , label ) has com e from a give n stochasti c rule P*. Her e a composit e hypothesi s P is unknow n\nother than it belong s to a give n class C.\nIn this model , we propos e a new discriminatio n algorith m on the basi s of the MD L (Minimu m Descriptio n\nLength ) principle , and then deriv e uppe r bound s on the least test sampl e size require d by the algorith m to guarante e\nthat two type s of error probabilitie s are respectivel y less than S1 and 62 provide d that the distanc e betwee n the\ntwo rules to be discriminate d is not less than e.\nFor the parametri c case wher e C is a parametri c class , this pape r show s that an uppe r boun d on test sampl e\nsize is give n by O( j In j- + 4j In -^ + 7 In 7 H—^~^) - Her e k is the numbe r of real-value d parameter s\nfor the composit e hypothesi s P, and i(M)  is the descriptio n lengt h for the countabl e mode l for P. Furthe r\nthis pape r show s that the MDL-base d discriminatio n algorith m perform s well in the sens e of sampl e complexit y\nefficiency , comparin g it with othe r kind s of information-criteria-base d discriminatio n algorithms . Thi s pape r\nalso show s how to transfor m any stochasti c PAC (Probabl y Approximatel y Correct)-learnin g algorith m into a\nPAD-learnin g algorithm .\nFor the non-parametri c case wher e C is a",
    "title": "Probably Almost Discriminative Learning",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022870506888",
    "published": "2003-04-04T16:57:10Z"
  },
  "87": {
    "pdf_path": "data/pdfs/machine learning_paper_175.pdf",
    "text_excerpt": "REVIEW ARTICLE OPEN\nRecent advances and applications of machine learning in solid-\nstate materials science\nJonathan Schmidt1, Mário R. G. Marques1, Silvana Botti2and Miguel A. L. Marques1\nOne of the most exciting tools that have entered the material science toolbox in recent years is machine learning. This collection of\nstatistical methods has already proved to be capable of considerably speeding up both fundamental and applied research. At\npresent, we are witnessing an explosion of works that develop and apply machine learning to solid-state systems. We provide acomprehensive overview and analysis of the most recent research in this topic. As a starting point, we introduce machine learning\nprinciples, algorithms, descriptors, and databases in materials science. We continue with the description of different machine\nlearning approaches for the discovery of stable materials and the prediction of their crystal structure. Then we discuss research innumerous quantitative structure –property relationships and various approaches for the replacement of ﬁrst-principle methods by\nmachine learning. We review how active learning and surrogate-based optimization can be applied to improve the rational design\nprocess and related examples of applications. Two major questions are always the interpretability of and the physicalunderstanding gained from machine learning models. We consider therefore the different facets of interpretability and theirimportance in materials science. Finally, we propose solutions and future research paths for various challenges in computational\nmaterials science.\nnpj Computational Materials            (2019) 5:83 ; https://doi.org/10.1038/s41524-019-0221-0\nINTRODUCTION\nIn recent years, the availability of large datasets combined with\nthe improvement in algorithms and the exponential growth incomputing power led to an unparalleled surge of interest in the\ntopic of machine learning. Nowadays, machine learning algo-\nrithms are successfully employed for clas",
    "title": "Recent advances and applications of machine learning in solid-state materials science",
    "abstract": "One of the most exciting tools that have entered the material science toolbox in recent years is machine learning. This collection of statistical methods has already proved to be capable of considerably speeding up both fundamental and applied research. At present, we are witnessing an explosion of works that develop and apply machine learning to solid-state systems. We provide a comprehensive overview and analysis of the most recent research in this topic. As a starting point, we introduce machine learning principles, algorithms, descriptors, and databases in materials science. We continue with the description of different machine learning approaches for the discovery of stable materials and the prediction of their crystal structure. Then we discuss research in numerous quantitative structure–property relationships and various approaches for the replacement of first-principle methods by machine learning. We review how active learning and surrogate-based optimization can be applied to improve the rational design process and related examples of applications. Two major questions are always the interpretability of and the physical understanding gained from machine learning models. We consider therefore the different facets of interpretability and their importance in materials science. Finally, we propose solutions and future research paths for various challenges in computational materials science.",
    "link": "https://www.semanticscholar.org/paper/0273507eb05f1135f3a05f9c7adc9a56f12c7c5c",
    "published": "2019-08-08"
  },
  "88": {
    "pdf_path": "data/pdfs/machine learning_paper_230.pdf",
    "text_excerpt": "manuscript submitted to Water Resources Research\nWhat Role Does Hydrological Science Play in the Age 1\nof Machine Learning? 2\nGrey S. Nearing1, Frederik Kratzert2, Alden Keefe Sampson3, Craig S. 3\nPelissier4, Daniel Klotz2, Jonathan M. Frame1, Cristina Prieto5, Hoshin V. 4\nGupta65\n1Department of Geological Sciences, University of Alabama; Tuscaloosa, AL USA 6\n2LIT AI Lab & Institute for Machine Learning, Johannes Kepler University; Linz, Austria 7\n3Upstream Tech, Natel Energy Inc.; Alameda, CA USA 8\n4NASA Center for Climate Simulation, NASA Goddard Space Flight Center; Greenbelt, MD USA 9\n5IHCantabria { Instituto de Hidr\u0013 aulica Ambiental de la Universidad de Cantabria, Santander, Spain 10\n6Department of Hydrology and Atmospheric Sciences, University of Arizona; Tucson, AZ USA 11\nKey Points: 12\n\u000fHydrology lacks scale-relevant theories but deep learning experiments suggests that 13\nthese theories should exist 14\n\u000fIt is up to hydrologists to clearly show where and when hydrological theory adds 15\nvalue to simulation and forecasting 16\nCorresponding author: Grey Nearing, gsnearing@ua.edu\n{1{manuscript submitted to Water Resources Research\nAbstract 17\nThis paper is derived from a keynote talk given at the Google's 2020 Flood Forecasting 18\nMeets Machine Learning Workshop. Recent experiments applying deep learning to rainfall- 19\nruno\u000b simulation indicate that there is signi\fcantly more information in large-scale hy- 20\ndrological data sets than hydrologists have been able to translate into theory or mod- 21\nels. While there is growing interest in machine learning in the hydrological sciences com- 22\nmunity, in many ways our community still holds deeply subjective and non-evidence-based 23\npreferences for models based on a certain type of `process understanding' that has his- 24\ntorically not translated into accurate theory, models, or predictions. This commentary 25\nis a call to action for the hydrology community to focus on developing a quantitative un- 26\nderstanding ",
    "title": "What Role Does Hydrological Science Play in the Age of Machine Learning?",
    "abstract": "This paper is derived from a keynote talk given at the Google's 2020 Flood Forecasting Meets Machine Learning Workshop. Recent experiments applying deep learning to rainfall‐runoff simulation indicate that there is significantly more information in large‐scale hydrological data sets than hydrologists have been able to translate into theory or models. While there is a growing interest in machine learning in the hydrological sciences community, in many ways, our community still holds deeply subjective and nonevidence‐based preferences for models based on a certain type of “process understanding” that has historically not translated into accurate theory, models, or predictions. This commentary is a call to action for the hydrology community to focus on developing a quantitative understanding of where and when hydrological process understanding is valuable in a modeling discipline increasingly dominated by machine learning. We offer some potential perspectives and preliminary examples about how this might be accomplished.",
    "link": "https://www.semanticscholar.org/paper/62d4aaaf562df94c4bdb116ee1e5cc2843c88bec",
    "published": "2020-02-11"
  },
  "89": {
    "pdf_path": "data/pdfs/machine learning_paper_79.pdf",
    "text_excerpt": "A Survey on Bias and Fairness in Machine Learning\nNINAREH MEHRABI, FRED MORSTATTER, NRIPSUTA SAXENA,\nKRISTINA LERMAN, and ARAM GALSTYAN, USC-ISI\nWith the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting\nfor fairness has gained significant importance in designing and engineering of such systems. AI systems can be\nused in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure\nthat these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently\nsome work has been developed in traditional machine learning and deep learning that address such challenges in\ndifferent subdomains. With the commercialization of these systems, researchers are becoming more aware of\nthe biases that these applications can contain and are attempting to address them. In this survey we investigated\ndifferent real-world applications that have shown biases in various ways, and we listed different sources of\nbiases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning\nresearchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined\ndifferent domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes\nin the state-of-the-art methods and ways they have tried to address them. There are still many future directions\nand solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will\nmotivate researchers to tackle these issues in the near future by observing existing work in their respective fields.\nCCS Concepts: •Computing methodologies →Artificial intelligence ;Philosophical/theoretical founda-\ntions of artificial intelligence ;\nAdditional Key Words and Phrases: Fairness and Bias in Artificial Intelligence, Machine Learning, Deep\nLearning, Natural Language Processing, Representa",
    "title": "A Survey on Bias and Fairness in Machine Learning",
    "abstract": "With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.",
    "link": "https://www.semanticscholar.org/paper/0090023afc66cd2741568599057f4e82b566137c",
    "published": "2019-08-23"
  },
  "90": {
    "pdf_path": "data/pdfs/machine learning_paper_256.pdf",
    "text_excerpt": "Machine Learning in Medicine\nRahul C. Deo, MD, PhD\nCardiovascular Research Institute, Department of Medicine and Institute for Human Genetics, \nUniversity of California, San Francisco, and California Institute for Quantitative Biosciences, San \nFrancisco, CA\nAbstract\nSpurred by advances in processing power, memory, storage, and an unprecedented wealth of data, \ncomputers are being asked to tackle increasingly complex learning tasks, often with astonishing \nsuccess. Computers have now mastered a popular variant of poker, learned the laws of physics \nfrom experimental data, and become experts in video games – tasks which would have been \ndeemed impossible not too long ago. In parallel, the number of companies centered on applying \ncomplex data analysis to varying industries has exploded, and it is thus unsurprising that some \nanalytic companies are turning attention to problems in healthcare. The purpose of this review is to \nexplore what problems in medicine might benefit from such learning approaches and use examples \nfrom the literature to introduce basic concepts in machine learning. It is important to note that \nseemingly large enough medical data sets and adequate learning algorithms have been available \nfor many decades – and yet, although there are thousands of papers applying machine learning \nalgorithms to medical data, very few have contributed meaningfully to clinical care. This lack of \nimpact stands in stark contrast to the enormous relevance of machine learning to many other \nindustries. Thus part of my effort will be to identify what obstacles there may be to changing the \npractice of medicine through statistical learning approaches, and discuss how these might be \novercome.\nKeywords\ncomputers; statistics; risk factor; prognosis; machine learning\nMachine learning is the scientific discipline that focuses on how computers learn from \ndata4,5. It arises at the intersection of statistics, which seeks to learn relationships from data, \nand computer science",
    "title": "Machine Learning in Medicine",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/2346d121f38fc19c77e0b062415519843f478163",
    "published": "2015-11-17"
  },
  "91": {
    "pdf_path": "data/pdfs/machine learning_paper_577.pdf",
    "text_excerpt": "Machin e Learnin g 3: 79-92 , 1988\n© 1988 Kluwe r Academi c Publishers , Bosto n - Manufacture d in The Netherland s\nA Revie w of Machin e Learnin g at AAAI-8 7\nRUSSEL L GREINE R (GREINER@AI.TORONTO.EDU )\nDepartment  of Computer  Science,  University  of Toronto,\nToronto,  Ontario  M5S  1A4,  CANADA\nBERNAR D SILVE R (SILVER@AI.AI.MIT.EDU )\nComputer  and Intelligent  Systems  Laboratory,  GTE  Laboratories  Incorporated,\n40 Sylvan  Road,  Waltham  MA 02254,  V.S.A\nSUE BECKE R (BECKER@AI.TORONTO.EDU )\nMICHAE L GRUNINGE R (GRUNINGER@AI.TORONTO.EDU )\nDepartment  of Computer  Science,  University  of Toronto,\nToronto,  Ontario  M5S  1A4,  CANADA\n1. Introductio n\nSome of us can remembe r the first AAA I conferenc e in 1980 - a cozy gath -\nering of 400 AI researcher s tucke d awa y in one corne r of a universit y campus .\nTher e wer e only two paralle l session s of papers , and they fille d one relativel y\nthin proceeding s volume . Ther e wer e no tutoria l session s and no exhibitio n\nhall. For bette r or worse , the field of artificia l intelligenc e has grow n consider -\nably in the subsequen t seve n years . The recen t Sixt h Nationa l Conferenc e on\nArtificia l Intelligence , AAAI-87 , involve d over 6,70 0 peopl e and require d the\nfull accommodation s of the Seattl e Cente r in Seattle , Washingto n (site of a\nforme r world' s fair) for its four paralle l technica l session s and exhibitio n show ;\nand it still neede d the Universit y of Washingto n campu s for its four paralle l\ntutoria l sessions .\nMachin e learnin g (ML ) has also develope d over thes e years . It has recentl y\nemerge d as the subfiel d of AI that deal s with technique s for improvin g the\nperformanc e of a computationa l system . It is now distinguishe d from studie s\nof huma n learnin g and from specifi c knowledg e acquisitio n (KA ) tools . In\nadditio n to severa l ML pape r sessions , ther e are now tutorial s on the topi c\nand even ML program s and book s on displa y in the trad e show .",
    "title": "A Review of Machine Learning at AAAI-87",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022637632387",
    "published": "2003-04-04T16:55:36Z"
  },
  "92": {
    "pdf_path": "data/pdfs/machine learning_paper_4.pdf",
    "text_excerpt": "Machine Learning for Clinical Predictive Analytics\nWei-Hung Weng1\nLearning Objectives\n\u000fUnderstand the basics of machine learning\ntechniques and the reasons behind why they\nare useful for solving clinical prediction\nproblems.\n\u000fUnderstand the intuition behind some ma-\nchine learning models, including regression,\ndecision trees, and support vector machines.\n\u000fUnderstand how to apply these models to\nclinical prediction problems using publicly\navailable datasets via case studies.\n1. Machine Learning for Healthcare\n1.1. Introduction\nIn this chapter, we provide a brief overview of applying ma-\nchine learning techniques for clinical prediction tasks. We\nbegin with a quick introduction to the concepts of machine\nlearning, and outline some of the most common machine\nlearning algorithms. Next, we demonstrate how to apply\nthe algorithms with appropriate toolkits to conduct machine\nlearning experiments for clinical prediction tasks.\nThis chapter is composed of ﬁve sections. First, we will\nexplain why machine learning techniques are helpful for\nresearchers in solving clinical prediction problems (section\n1). Understanding the motivations behind machine learn-\ning approaches in healthcare are essential, since precision\nand accuracy are often critical in healthcare problems, and\neverything from diagnostic decisions to predictive clinical\nanalytics could dramatically beneﬁt from data-based pro-\ncesses with improved efﬁciency and reliability. In the sec-\nond section, we will introduce several important concepts in\nmachine learning in a colloquial manner, such as learning\nscenarios, objective/target function, error and loss function\nand metrics, optimization and model validation, and ﬁnally\na summary of model selection methods (section 2). These\ntopics will help us utilize machine learning algorithms in\nan appropriate way. Following that, we will introduce some\n1MIT CSAIL, Cambridge, MA, USA. Correspondence to: Wei-\nHung Weng <ckbjimmy@mit.edu >.popular machine learning algorithms for ",
    "title": "Machine Learning for Clinical Predictive Analytics",
    "abstract": "In this chapter, we provide a brief overview of applying machine learning\ntechniques for clinical prediction tasks. We begin with a quick introduction to\nthe concepts of machine learning and outline some of the most common machine\nlearning algorithms. Next, we demonstrate how to apply the algorithms with\nappropriate toolkits to conduct machine learning experiments for clinical\nprediction tasks. The objectives of this chapter are to (1) understand the\nbasics of machine learning techniques and the reasons behind why they are\nuseful for solving clinical prediction problems, (2) understand the intuition\nbehind some machine learning models, including regression, decision trees, and\nsupport vector machines, and (3) understand how to apply these models to\nclinical prediction problems using publicly available datasets via case\nstudies.",
    "link": "http://arxiv.org/abs/1909.09246v1",
    "published": "2019-09-19T22:02:00Z"
  },
  "93": {
    "pdf_path": "data/pdfs/machine learning_paper_33.pdf",
    "text_excerpt": "UNIVERSIDAD AUT \u0013ONOMA DE CHIHUAHUA August - December 2021 Semester\nDetection of brain tumors\nusing machine learning algorithms\nPattern Recognition\nM. I. Joseph Isaac Ram\u0013 \u0010rez Hern\u0013 andez\nH. Corral 320695, J. Melchor 320722, B. Sotelo 320711, J. Vera 324651\nCircuito Universitario S/N, Nuevo Campus Universitario C.P. 31125, Chihuahua Chih. M\u0013 exico.\nFacultad de Ingenier\u0013 \u0010a\nAbstract\nAn algorithm capable of processing NMR images was developed for analysis using machine learning tech-\nniques to detect the presence of brain tumors.\nKey words| PCA, Machine learning, Nuclear Magnetic Resonance, Image recognition, Tumors, Support\nVector Machine, Adaboost, Decision Tree, Random Forest.\n1 Introduction\nBrain tumors rank 19th among all neoplasms, and 10th\namong the most lethal. In Mexico it is estimated that\nan incidence of 3.5 per 100,000 inhabitants and repre-\nsents the second and \ffth causes of cancer mortality in\nthe age groups from 0 to 18 and 18 to 29 years. [1].\nAn early diagnosis is a determining factor in the\nsurvival chances of the patient. A severity 4 tumor, for\nexample, is capable of doubling in size in 25 days, re-\nducing the patient's life expectancy to less than 1 year\n[2]. Among the techniques used for detection is Nu-\nclear Magnetic Resonance (NMR), a non-radioactive\nand non-intrusive process that allows obtaining high-\nresolution images of the interior of the human body.\nAlthough it has limitations due to the fact that not\nall tissues are visible to the technique, MRI does allow\nthe area of a brain a\u000bected by cancer to be clearly visu-\nalized, which gives to the technique reliability against\nthis condition.\nIn this way, the images can be used for the train-\ning of machine learning algorithms in order to create a\nuseful tool for the doctors in charge of the diagnosis.\nSpeci\fcally, an algorithm capable of detecting whether\nor not a patient has a brain tumor can speed up and\n/ or facilitate the diagnostic process by giving to the\nmedical specialist an insight ",
    "title": "Detection of brain tumors using machine learning algorithms",
    "abstract": "An algorithm capable of processing NMR images was developed for analysis\nusing machine learning techniques to detect the presence of brain tumors.",
    "link": "http://arxiv.org/abs/2201.04703v1",
    "published": "2022-01-12T21:08:38Z"
  },
  "94": {
    "pdf_path": "data/pdfs/machine learning_paper_105.pdf",
    "text_excerpt": "RESEA RCH ARTICL E\nSoilGrids250m: Global gridded soil\ninformation based onmachine learning\nTomislav Hengl1*,Jorge Mendes deJesus1,Gerard B.M.Heuvelink1,Maria Ruiperez\nGonzalez1,Milan Kilibarda2,Aleksandar Blagotić3,Wei Shangguan4,Marvin N.Wright5,\nXiaoyuan Geng6,Bernhard Bauer-Marschallinger7,Mario Antonio Guevara8,\nRodrigo Vargas8,Robert A.MacMillan9,Niels H.Batjes1,Johan G.B.Leenaars1,\nEloi Ribeiro1,Ichsani Wheeler10,Stephan Mantel1,Bas Kempen1\n1ISRIC ÐWorld SoilInformation, Wageningen, theNetherlands ,2Faculty ofCivil Engineeri ng,University of\nBelgrade, Belgrade, Serbia, 3GILab Ltd,Belgrade, Serbia, 4School ofAtmospher icSciences, Sun Yat-sen\nUniversity ,Guangzhou, China, 5Institut fuÈrMedizinisc heBiometrie undStatistik, LuÈbeck, Germa ny,\n6Agriculture andAgri-Food Canada, Ottawa (Ontario), Canada, 7Departm entofGeodesy and\nGeoinform ation, Vienna University ofTechnology ,Vienna, Austria, 8University ofDelawa re,Newar k(DE),\nUnited States ofAmerica, 9LandMapper Environm ental Solutions Inc., Edmonto n(Alberta) ,Canada,\n10Envirome trixInc., Wagenin gen, theNetherlan ds\n*tom.hengl @isric.org\nAbstract\nThis paper describes thetechnical development and accuracy assessment ofthemost\nrecent and improved version oftheSoilGrids system at250m resolution (June 2016\nupdate). SoilGrids provides global predictions forstandard numeric soilproperties\n(organic carbon, bulk density, Cation Exchange Capacity (CEC), pH,soiltexture fractions\nand coarse fragments) atseven standard depths (0,5,15,30,60,100 and 200 cm), in\naddition topredictions ofdepth tobedrock and distribution ofsoilclasses based onthe\nWorld Reference Base (WRB) and USDA classification systems (ca. 280 raster layers in\ntotal). Predictions were based onca.150,000 soilprofiles used fortraining and astack of\n158 remote sensing-ba sedsoilcovariates (primarily derived from MODIS land products,\nSRTM DEM derivatives, climatic images and global landform and lithology maps), which\nwere used tofitanensemble ofmachine learnin",
    "title": "SoilGrids250m: Global gridded soil information based on machine learning",
    "abstract": "This paper describes the technical development and accuracy assessment of the most recent and improved version of the SoilGrids system at 250m resolution (June 2016 update). SoilGrids provides global predictions for standard numeric soil properties (organic carbon, bulk density, Cation Exchange Capacity (CEC), pH, soil texture fractions and coarse fragments) at seven standard depths (0, 5, 15, 30, 60, 100 and 200 cm), in addition to predictions of depth to bedrock and distribution of soil classes based on the World Reference Base (WRB) and USDA classification systems (ca. 280 raster layers in total). Predictions were based on ca. 150,000 soil profiles used for training and a stack of 158 remote sensing-based soil covariates (primarily derived from MODIS land products, SRTM DEM derivatives, climatic images and global landform and lithology maps), which were used to fit an ensemble of machine learning methods—random forest and gradient boosting and/or multinomial logistic regression—as implemented in the R packages ranger, xgboost, nnet and caret. The results of 10–fold cross-validation show that the ensemble models explain between 56% (coarse fragments) and 83% (pH) of variation with an overall average of 61%. Improvements in the relative accuracy considering the amount of variation explained, in comparison to the previous version of SoilGrids at 1 km spatial resolution, range from 60 to 230%. Improvements can be attributed to: (1) the use of machine learning instead of linear regression, (2) to considerable investments in preparing finer resolution covariate layers and (3) to insertion of additional soil profiles. Further development of SoilGrids could include refinement of methods to incorporate input uncertainties and derivation of posterior probability distributions (per pixel), and further automation of spatial modeling so that soil maps can be generated for potentially hundreds of soil variables. Another area of future research is the development of methods for multiscale merging of SoilGrids predictions with local and/or national gridded soil products (e.g. up to 50 m spatial resolution) so that increasingly more accurate, complete and consistent global soil information can be produced. SoilGrids are available under the Open Data Base License.",
    "link": "https://www.semanticscholar.org/paper/9e27190f2d9b2167d4a66b88696def4585072fd5",
    "published": "2017-02-16"
  },
  "95": {
    "pdf_path": "data/pdfs/machine learning_paper_52.pdf",
    "text_excerpt": "Preprint under review.\nTHERML:\nTHETHERMODYNAMICS OF MACHINE LEARNING\nAlexander A. Alemi\nGoogle Research\n1600 Amphitheatre Parkway\nMountain View, CA 94043\nalemi@google.comIan Fischer\nGoogle Research\n1600 Amphitheatre Parkway\nMountain View, CA 94043\niansf@google.com\nABSTRACT\nIn this work we offer an information-theoretic framework for representation learn-\ning that connects with a wide class of existing objectives in machine learning. We\ndevelop a formal correspondence between this work and thermodynamics and dis-\ncuss its implications.\n1 I NTRODUCTION\nLetX;Y be some paired data, for example: a set of images Xand their labels Y. We imagine\nthe data comes from some true, unknown data generating process \b1, from which we have drawn a\ntraining set ofNpairs:\nTN\u0011(xN;yN)\u0011fx1;y1;x2;y2;:::;xN;yNg\u0018\u001e(xN;yN): (1)\nWe further imagine the process is exchangeable2and the data is conditionally independent given the\ngoverning process \b:\np(xN;yNj\u001e) =Y\nip(xij\u001e)p(yijxi;\u001e): (2)\nAs machine learners, we believe that by studying the training set, we should be able to infer or\npredict new draws from the same data generating process. Call a set of Mfuture draws from the\ndata generating process T0\nM\u0011fXM;YMgthetest set .\nThepredictive information (Bialek et al., 2001) is the mutual information between the training set\nand a inﬁnite test set, equivalently the amount of information the training set provides about the\ngenerative process itself:\nIpred(TN)\u0011lim\nM!1I(TN;T0\nM) =I(TN; \b) =I(XN;YN; \b): (3)\nThe predictive information measures the underlying complexity of the data generating process (Still,\n2014), and is fundamentally limited and must grow sublinearly in the dataset size (Bialek et al.,\n2001). Hence, the predictive information is a vanishing fraction of the total information in the\ntraining set3:\nlim\nN!1Ipred(TN)\nH(TN)= 0 (4)\nA vanishing fraction of the information present in our training data is in any way useful for future\ntasks. A vanishing fraction of the information contained in the tr",
    "title": "TherML: Thermodynamics of Machine Learning",
    "abstract": "In this work we offer a framework for reasoning about a wide class of\nexisting objectives in machine learning. We develop a formal correspondence\nbetween this work and thermodynamics and discuss its implications.",
    "link": "http://arxiv.org/abs/1807.04162v3",
    "published": "2018-07-11T14:39:17Z"
  },
  "96": {
    "pdf_path": "data/pdfs/machine learning_paper_155.pdf",
    "text_excerpt": "NBER WORKING PAPER SERIES\nEMPIRICAL ASSET PRICING VIA MACHINE LEARNING\nShihao Gu\nBryan Kelly\nDacheng Xiu\nWorking Paper 25398\nhttp://www.nber.org/papers/w25398\nNATIONAL BUREAU OF ECONOMIC RESEARCH\n1050 Massachusetts Avenue\nCambridge, MA 02134\nDecember 2018, Revised September 2019\nWe benefitted from discussions with Joseph Babcock, Si Chen (Discussant), Rob Engle, Andrea \nFrazzini, Amit Goyal (Discussant), Lasse Pedersen, Lin Peng (Discussant), Alberto Rossi \n(Discussant), Guofu Zhou (Discussant), and seminar and conference participants at Erasmus \nSchool of Economics, NYU, Northwestern, Imperial College, National University of Singapore, \nUIBE, Nanjing University, Tsinghua PBC School of Finance, Fannie Mae, U.S. Securities and \nExchange Commission, City University of Hong Kong, Shenzhen Finance Institute at CUHK, \nNBER Summer Institute, New Methods for the Cross Section of Returns Conference, Chicago \nQuantitative Alliance Conference, Norwegian Financial  Research Conference, EFA, China \nInternational Conference in Finance, 10th World Congress of the Bachelier Finance Society, \nFinancial Engineering and Risk Management International Symposium, Toulouse Financial \nEconometrics Conference, Chicago Conference on New Aspects of Statistics, Financial  \nEconometrics, and Data Science, Tsinghua Workshop on Big Data and Internet Economics, Q \ngroup, IQ-KAP Research Prize Symposium, Wolfe Re- search, INQUIRE UK, Australasian \nFinance and Banking Conference, Goldman Sachs Global Alternative Risk Premia Conference, \nAFA, and Swiss Finance Institute. We gratefully acknowledge the computing support from the \nResearch Computing  Center at the University of Chicago. The views expressed herein are those \nof the authors and do not necessarily reflect the views of the National Bureau of Economic \nResearch.\nAt least one co-author has disclosed a financial relationship of potential relevance for this \nresearch. Further information is available online at http://www.nber.org/papers/w25398",
    "title": "Empirical Asset Pricing Via Machine Learning",
    "abstract": "\n We perform a comparative analysis of machine learning methods for the canonical problem of empirical asset pricing: measuring asset risk premiums. We demonstrate large economic gains to investors using machine learning forecasts, in some cases doubling the performance of leading regression-based strategies from the literature. We identify the best-performing methods (trees and neural networks) and trace their predictive gains to allowing nonlinear predictor interactions missed by other methods. All methods agree on the same set of dominant predictive signals, a set that includes variations on momentum, liquidity, and volatility.\n Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.",
    "link": "https://www.semanticscholar.org/paper/caf9e0fa2c340fb07cef8d547ea8849508e5c358",
    "published": "2018-12-01"
  },
  "97": {
    "pdf_path": "data/pdfs/Machine Learning for Clinical Predictive Analytics_paper_9.pdf",
    "text_excerpt": "arXiv:2506.23315v1  [cs.CL]  29 Jun 2025i\nEnsemble BERT for Medication Event\nClassification on Electronic Health Records (EHRs)\nShouvon Sarker, Xishuang Dong, and Lijun Qian, Senior Member, IEEE\nAbstract —Identification of key variables such as medications,\ndiseases, relations from health records and clinical notes has\na wide range of applications in the clinical domain. n2c2\n2022 provided shared tasks on challenges in natural language\nprocessing for clinical data analytics on electronic health records\n(EHR), where it built a comprehensive annotated clinical data\nContextualized Medication Event Dataset (CMED) . This study\nfocuses on subtask 2 in Track 1 of this challenge that is to\ndetect and classify medication events from clinical notes through\nbuilding a novel BERT-based ensemble model. It started with\npretraining BERT models on different types of big data such\nas Wikipedia and MIMIC. Afterwards, these pretrained BERT\nmodels were fine-tuned on CMED training data. These fine-tuned\nBERT models were employed to accomplish medication event\nclassification on CMED testing data with multiple predictions.\nThese multiple predictions generated by these fine-tuned BERT\nmodels were integrated to build final prediction with voting\nstrategies. Experimental results demonstrated that BERT-based\nensemble models can effectively improve strict Micro-F score by\nabout 5% and strict Macro-F score by about 6%, respectively.\nIndex Terms —Electronic Health Records, Medication Events,\nBidirectional Encoder Representations from Transformers\n(BERT), Ensemble Model\nI. I NTRODUCTION\nElectronic Health Records (EHRs) mostly contain unstruc-\ntured texts regarding patient’s history and health conditions,\ndischarge paper and clinical notes. The unstructured texts\nmake it very difficult to process for most state-of-the-art lan-\nguage model representations[1]. Also, the scarcity of biomed-\nical dataset related to the privacy and the security of the\npatients make it very difficult to train a deep lea",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "98": {
    "pdf_path": "data/pdfs/machine learning_paper_337.pdf",
    "text_excerpt": "Machin e Learnin g 3: 95-99 , 1988\n© 1988 Kluwe r Academi c Publisher s - Manufacture d in The Netherland s\nGUES T EDITORIA L\nGeneti c Algorithm s and Machin e Learnin g\nMetaphor s for learnin g\nTher e is no a priori  reaso n why machin e learnin g mus t borro w from nature .\nA field coul d exist , complet e wit h well-define d algorithms , dat a structures ,\nand theorie s of learning , withou t once referrin g to organisms , cognitiv e or\ngeneti c structures , and psychologica l or evolutionar y theories . Yet at the end\nof the day, with the positio n paper s written , the computer s plugge d in, and\nthe program s debugged , a learnin g edific e devoi d of natura l metapho r woul d\nlack something . It woul d ignor e the fact that all thes e creation s hav e becom e\npossibl e only afte r thre e billio n year s of evolutio n on this planet . It woul d\nmiss the poin t tha t the very idea s of adaptatio n and learnin g are concept s\ninvente d by the mos t recen t representative s of the specie s Homo  sapiens  from\nthe carefu l observatio n of themselve s and life aroun d them . It woul d miss the\npoint that natura l example s of learnin g and adaptatio n are treasur e trove s of\nrobus t procedure s and structures .\nFortunately , the field of machin e learnin g does rely upo n nature' s bount y\nfor bot h inspiratio n and mechanism . Man y machin e learnin g system s now\nborro w heavil y from curren t thinkin g in cognitiv e science , and rekindle d in-\nteres t in neura l network s and connectionis m is evidenc e of seriou s mechanisti c\nand philosophica l current s runnin g throug h the field . Anothe r area wher e nat-\nural exampl e has bee n tappe d is in wor k on genetic  algorithms  (GAs ) and\ngenetics-base d machin e learning . Roote d in the earl y cybernetic s movemen t\n(Holland , 1962) , progres s has been mad e in both theor y (Holland , 1975 ; Hol-\nland, Holyoak , Nisbett , & Thagard , 1986 ) and applicatio n (Goldberg , 1989 ;\nGrefenstette , 1985 , 1987 ) to",
    "title": "Genetic algorithms and Machine Learning",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/c61134ada9f0e3f3373d635c31a8b3caa37f9977",
    "published": "1988-10-01"
  },
  "99": {
    "pdf_path": "data/pdfs/machine learning_paper_253.pdf",
    "text_excerpt": "1122\ncomment  | FOCUS\nDOME: recommendations for supervised \nmachine learning validation in biology\nDOME is a set of community-wide recommendations for reporting supervised machine learning–based analyses \napplied to biological studies. Broad adoption of these recommendations will help improve machine learning assessment and reproducibility.\nIan Walsh, Dmytro Fishman, Dario Garcia-Gasulla, Tiina Titma, Gianluca Pollastri, ELIXIR Machine \nLearning Focus Group, Jennifer Harrow, Fotis E. Psomopoulos and Silvio C. E. T osatto\nWith the steep decline in the \ncost of many high-throughput technologies, large amounts \nof biological data are being generated and made accessible to researchers. Machine learning (ML) has come into the spotlight as a very useful approach for understanding cellular\n1, genomic2, \nproteomic3, post-translational4, metabolic5 \nand drug discovery data6, with the potential \nto result in ground-breaking medical applications\n7,8. This is clearly reflected in the \ncorresponding growth of ML publications (Fig. 1), reporting a wide range of modeling techniques in biology. While ideally ML methods should be validated experimentally, this happens only in a fraction of the publications\n9. We believe that the time is \nright for the ML community to develop standards for reporting ML-based analyses to enable critical assessment\n10 and improve \nreproducibility11,12.\nGuidelines or recommendations  \non how to appropriately construct ML a\nlgorithms can help to ensure correct  \nresults and predictions13,14. In biomedical \nresearch, communities have defined standard guidelines and best practices for scientific data management\n15 and \nreproducibility of computational tools16,17. \nOn the ML community side, there is demand for a cohesive and combined  \nset of recommendations with respect  \nto data, the optimization techniques,  the final model, and evaluation protocols  as a whole.\nA r\necent comment highlighted the  \nneed for standards in ML18, arguing for  \nthe adoption of",
    "title": "DOME: recommendations for supervised machine learning validation in biology",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/c370197b15fcd382094132bde4daa2c248b7cedf",
    "published": "2021-07-27"
  },
  "100": {
    "pdf_path": "data/pdfs/Machine Learning for Clinical Predictive Analytics_paper_2.pdf",
    "text_excerpt": "                                                                       IEEE TRANSACTIONS AND JOURNALS TEMPLATE     1 \n \n  \n \nPredicting Mortality and Functional Status Scores of Traumatic Brain Injury \nPatients using Supervised Machine Learning  \n \n  \n \nAbstract — \nTraumatic brain injury (TBI) is a major public health concern \nthat often leads to mortality or long-term disability. Accurate \nprediction of outcomes, such as mortality and Functional Status \nScale (FSS) scores, improves treatment strategies and guides \nclinical decisions. This study applies supervised machine \nlearning (ML) techniques to predict mortality and FSS score in \na real-world dataset of 300 pediatric TBI patients compiled by \nthe University of Colorado School of Medicine. The dataset \nincludes detailed clinical information, such as demographics, \ninjury mechanisms, and hospitalization outcomes. Eighteen ML \nmodels predict mortality, while thirteen models forecast FSS \nscores. Performance metrics, including accuracy, ROC AUC, \nF1-score, and mean squared error, evaluate the models. Logistic \nregression and Extra Trees show high precision in mortality \nprediction, while linear regression achieves the best FSS score \nprediction with an R² score of 0.62. Feature selection reduces \n103 clinical variables to the most relevant ones, improving \nmodel efficiency and interpretability. The study highlights how \nML models help clinicians identify high-risk cases and support \npersonalized interventions. This research demonstrates the \npotential of ML-driven predictive analytics to integrate into \nclinical workflows, providing healthcare professionals with \ndata-driven tools to improve TBI care. \nKeywords —  \nMachine learning, Traumatic Brain Injury (TBI), Mortality \nprediction, Functional Status Scale (FSS), Predictive analytics, \nClinical decision support, Pediatric patients, Healthcare data, \nData preprocessing, Model evaluation \nI. INTRODUCTION  \nTraumatic Brain Injury (TBI) is a common diagnosis \nwith o",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "101": {
    "pdf_path": "data/pdfs/Machine Learning for Clinical Predictive Analytics_paper_14.pdf",
    "text_excerpt": "   Aiming for Relevance Bar Eini-Porat1, Danny Eytan, MD, PhD1,2, Uri Shalit, PhD1 1 Technion – Israel Institute of Technology, Haifa, Israel;  2 Rambam Medical Center, Haifa, Israel   Abstract Vital signs are crucial in intensive care units (ICUs). They are used to track the patient’s state and to identify clinically significant changes. Predicting vital sign trajectories is valuable for early detection of adverse events. However, conventional machine learning metrics like RMSE often fail to capture the true clinical relevance of such predictions. We introduce novel vital sign prediction performance metrics that align with clinical contexts, focusing on deviations from clinical norms, overall trends, and trend deviations. These metrics are derived from empirical utility curves obtained in a previous study through interviews with ICU clinicians. We validate the metrics’ usefulness using simulated and real clinical datasets (MIMIC and eICU). Furthermore, we employ these metrics as loss functions for neural networks, resulting in models that excel in predicting clinically significant events. This research paves the way for clinically relevant machine learning model evaluation and optimization, promising to improve ICU patient care.  Introduction Intensive care medicine is dedicated to treating critically-ill individuals who frequently contend with multiple physiological systems disturbances. Specialized healthcare professionals, including physicians and nurses, continuously track and estimate the patients’ physiological state. These estimates serve as a foundation for making informed decisions regarding treatments and interventions. Moreover, given the patients' often unstable condition, their clinical state can rapidly change over minutes. Vital signs, namely basic physiological variables such as heart rate, blood pressure, respiratory rate and arterial oxygen saturation are used both to track the patient’s state and to identify clinically significant changes. Predic",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "102": {
    "pdf_path": "data/pdfs/machine learning_paper_142.pdf",
    "text_excerpt": "A Review of Feature Selection\nMethods for Machine Learning-Based\nDisease Risk Prediction\nNicholas Pudjihartono1, Tayaza Fadason1,2, Andreas W. Kempa-Liehr3* and\nJustin M. O ’Sullivan1,2,4,5,6*\n1Liggins Institute, University of Auckland, Auckland, New Zealand,2Maurice Wilkins Centre for Molecular Biodiscovery, Auckland,\nNew Zealand,3Department of Engineering Science, The University of Auckland, Auckland, New Zealand,4MRC Lifecourse\nEpidemiology Unit, University of Southampton, Southampton, United Kingdom,5Singapore Institute for Clinical Sciences, Agency\nfor Science, Technology and Research (A*STAR), Singapore, Singapore,6Australian Parkinson ’s Mission, Garvan Institute of\nMedical Research, Sydney, NSW, Australia\nMachine learning has shown utility in detecting patterns within large, unstructured, and\ncomplex datasets. One of the promising applications of machine learning is in precisionmedicine, where disease risk is predicted using patient genetic data. However, creating anaccurate prediction model based on genotype data remains challenging due to the so-called “curse of dimensionality ”(i.e., extensively larger number of features compared to the\nnumber of samples). Therefore, the generalizability of machine learning models bene ﬁts\nfrom feature selection, which aims to extract only the most “informative ”features and\nremove noisy “non-informative, ”irrelevant and redundant features. In this article, we\nprovide a general overview of the different feature selection methods, their advantages,disadvantages, and use cases, focusing on the detection of relevant features (i.e., SNPs)for disease risk prediction.\nKeywords: machine learing, feature selection (FS), risk prediction, disease risk prediction, statistical approaches\n1 INTRODUCTION\n1.1 Precision Medicine and Complex Disease Risk Prediction\nThe advancement of genetic sequencing technology over the last decade has re-ignited interest in\nprecision medicine and the goal of providing healthcare based on a patient ’s i",
    "title": "A Review of Feature Selection Methods for Machine Learning-Based Disease Risk Prediction",
    "abstract": "Machine learning has shown utility in detecting patterns within large, unstructured, and complex datasets. One of the promising applications of machine learning is in precision medicine, where disease risk is predicted using patient genetic data. However, creating an accurate prediction model based on genotype data remains challenging due to the so-called “curse of dimensionality” (i.e., extensively larger number of features compared to the number of samples). Therefore, the generalizability of machine learning models benefits from feature selection, which aims to extract only the most “informative” features and remove noisy “non-informative,” irrelevant and redundant features. In this article, we provide a general overview of the different feature selection methods, their advantages, disadvantages, and use cases, focusing on the detection of relevant features (i.e., SNPs) for disease risk prediction.",
    "link": "https://www.semanticscholar.org/paper/911fbaec109f72130815e05e2633ec879590382c",
    "published": "2022-06-27"
  },
  "103": {
    "pdf_path": "data/pdfs/machine learning_paper_126.pdf",
    "text_excerpt": "Machine Learning for Fluid\nMechanics\nSteven L. Brunton,1Bernd R. Noack2 3and\nPetros Koumoutsakos4\n1Mechanical Engineering, University of Washington, Seattle, WA, USA, 98195\n2LIMSI, CNRS, Universit\u0013 e Paris-Saclay, F-91403 Orsay, France\n3Institut f ur Str omungsmechanik und Technische Akustik, TU Berlin, D-10634,\nGermany\n4Professorship for Computational Science, ETH Zurich, CH-8092, Switzerland;\nemail: petros@ethz.ch\nXxxx. Xxx. Xxx. Xxx. YYYY. AA:1{32\nhttps://doi.org/10.1146/((please add\narticle doi))\nCopyright c\rYYYY by Annual Reviews.\nAll rights reservedKeywords\nmachine learning, data-driven modeling, optimization, control\nAbstract\nThe \feld of \ruid mechanics is rapidly advancing, driven by unprece-\ndented volumes of data from \feld measurements, experiments and large-\nscale simulations at multiple spatiotemporal scales. Machine learning\no\u000bers a wealth of techniques to extract information from data that\ncould be translated into knowledge about the underlying \ruid me-\nchanics. Moreover, machine learning algorithms can augment domain\nknowledge and automate tasks related to \row control and optimiza-\ntion. This article presents an overview of past history, current devel-\nopments, and emerging opportunities of machine learning for \ruid me-\nchanics. It outlines fundamental machine learning methodologies and\ndiscusses their uses for understanding, modeling, optimizing, and con-\ntrolling \ruid \rows. The strengths and limitations of these methods are\naddressed from the perspective of scienti\fc inquiry that considers data\nas an inherent part of modeling, experimentation, and simulation. Ma-\nchine learning provides a powerful information processing framework\nthat can enrich, and possibly even transform, current lines of \ruid me-\nchanics research and industrial applications.\n1arXiv:1905.11075v3  [physics.flu-dyn]  4 Jan 20201. INTRODUCTION\nFluid mechanics has traditionally dealt with massive amounts of data from experiments,\n\feld measurements, and large-scale numerical simulati",
    "title": "Machine Learning for Fluid Mechanics",
    "abstract": "The field of fluid mechanics is rapidly advancing, driven by unprecedented volumes of data from experiments, field measurements, and large-scale simulations at multiple spatiotemporal scales. Machine learning (ML) offers a wealth of techniques to extract information from data that can be translated into knowledge about the underlying fluid mechanics. Moreover, ML algorithms can augment domain knowledge and automate tasks related to flow control and optimization. This article presents an overview of past history, current developments, and emerging opportunities of ML for fluid mechanics. We outline fundamental ML methodologies and discuss their uses for understanding, modeling, optimizing, and controlling fluid flows. The strengths and limitations of these methods are addressed from the perspective of scientific inquiry that considers data as an inherent part of modeling, experiments, and simulations. ML provides a powerful information-processing framework that can augment, and possibly even transform, current lines of fluid mechanics research and industrial applications.",
    "link": "https://www.semanticscholar.org/paper/4087e84fc695bb6433d0104ee94f9d7e9f4b7da5",
    "published": "2019-05-27"
  },
  "104": {
    "pdf_path": "data/pdfs/machine learning_paper_387.pdf",
    "text_excerpt": "Machine Learning, 9, 5-7 (1992)\n© 1992 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\nMachine Learning: A Maturing Field\nWith this volume I complete my four-year term as executive editor of Machine Learning,\nand Tom Dietterich, who has been co-executive editor with me recently, takes over the\nhelm—or starts serving his sentence, depending upon one's point of view. Let me take this\nopportunity to make a few reflections about the state of the field; past, present and future,\nbased on personal observations.\nA decade ago machine learning was regrouping from the rather uneventful 1970s. The\nfirst machine learning workshop was held in 1980 at Carnegie Mellon University with some\ntwo dozen participants and photocopied preprints. Shortly thereafter we started preparing\nthe first machine learning book, and I was in charge of finding a publication venue. However\nthe title \"Machine Learning\" raised skeptical eyebrows in publishers. By \"machine learn-\ning\" did we not really mean learning about machines rather than learning by machines?\nCouldn't we think of something more scientific-sounding to call the book? And anyway\nhadn't Minsky and Papert debunked this learning nonsense? Since it proved difficult to\nexplain the difference between linear perceptrons and symbolic learning to those publishers\n(who shall go unnamed, to protect the guilty), we approached Nils Nilsson and his Tioga\nPress. Nils embraced the project with foresight and enthusiasm, and the rest, as they say\nin the tired cliche, is history.\nThe uneventful '70s, and the struggling turn of the decade, were followed by the boom-\ning '80s. Machine learning became respectable: some would argue (this writer included)\nit even became scientifically sound. In the process, it also became much more popular.\nMachine learning conferences in the late '80s drew ten times as many participants as diat\nfirst CMU workshop: UCAI and AAAI have very well represented machine learning tracks;\nthis journal was establish",
    "title": "Machine Learning: A Maturing Field",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022665512030",
    "published": "2003-04-04T16:55:36Z"
  },
  "105": {
    "pdf_path": "data/pdfs/machine learning_paper_27.pdf",
    "text_excerpt": "Inverse Problems and Data Assimilation:\nA Machine Learning Approach\nEviatar Bach\\^, Ricardo Baptista\\, Daniel Sanz-AlonsoZ,Andrew Stuart\\\n\\Caltech^University of ReadingZUniversity of ChicagoarXiv:2410.10523v1  [stat.ML]  14 Oct 20242\nIntroduction\nAim and Overview\nThe aim of the notes is to demonstrate the potential for ideas in machine learning to\nimpact on the fields of inverse problems and data assimilation. The perspective is one\nthat is primarily aimed at researchers from inverse problems and/or data assimilation\nwho wish to see a mathematical presentation of machine learning as it pertains to\ntheir fields. As a by-product of the presentation we present a succinct mathematical\ntreatment of various topics in machine learning. The material on machine learning,\nalong with some other related topics, is summarized in Part III, Appendix. Part I of the\nnotes is concerned with inverse problems, employing material from Part III; Part II of\nthe notes is concerned with data assimilation, employing material from Parts I and III.\nExisting Review Articles\nThis is a rapidly evolving research area and there are already several articles that review\naspects of the material we will cover. See, for example, [ 20] for uses of ML in inverse\nproblems, and [44, 81] and Chapter 10 of [72] for uses of ML in data assimilation.\nPrerequisites\nEven though the subjects of inverse problems and data assimilation are succinctly\nreviewed in these notes, there remains an assumption of previous knowledge of these\ntopics. The book [ 325] (also available in closely related form on arXiv) provides a good\nbackground on these topics, and we have tried to maintain similar notation. These\nnotes also assume familiarity with linear algebra, probability, multivariable calculus,\nand matrix calculus.\nNotation\nThroughout the notes we adopt the following notational conventions:\n•Sets. Ndenotesthepositiveintegers {1,2,3, . . .},andZ+:=N∪{0}={0,1,2,3,···}\ndenotes the non-negative integers. R𝑑denotes the set of 𝑑-d",
    "title": "Inverse Problems and Data Assimilation: A Machine Learning Approach",
    "abstract": "The aim of these notes is to demonstrate the potential for ideas in machine\nlearning to impact on the fields of inverse problems and data assimilation. The\nperspective is one that is primarily aimed at researchers from inverse problems\nand/or data assimilation who wish to see a mathematical presentation of machine\nlearning as it pertains to their fields. As a by-product, we include a succinct\nmathematical treatment of various topics in machine learning.",
    "link": "http://arxiv.org/abs/2410.10523v1",
    "published": "2024-10-14T14:01:35Z"
  },
  "106": {
    "pdf_path": "data/pdfs/machine learning_paper_95.pdf",
    "text_excerpt": " \n POST NOTE  \nThe Parliamentary Office of Science and Technology, Westminster, London SW1A 0AA  \n02072192840 post@parliament.uk  post.parliament.uk  @POST_UK  Number 633 October 2020  \nInterpretable machine learning  \n \nThis POSTnote gives a n overview of  machine \nlearning  (ML) and its role  in decision -making . It \nexamines the challenges  of understanding how \na complex ML system has reached its output , \nand some of the  technical approaches to  \nmaking ML easier to interpret . It gives a  brief \noverview of  some of the proposed tools for \nmaking  ML systems more accountable , such as \nalgorithm audit and impact assessments.   Overview   \n◼ Machine learning (ML) is being used to \nsupport decision -making in applications such \nas recruitment and medical dia gnoses.  \n◼ Concerns have been raised about some \ncomplex types of ML , where  it is difficult  to \nunderstand how a decision has been made . \n◼ A further risk is the potential for ML system s \nto introduce or perpetuate biases.   \n◼ Approaches to improving the interpretabi lity \nof ML include designing systems using \nsimpler methods and using tools to gain an \ninsight  into how complex systems function.  \n◼ Interpretable ML can improve user trust and  \nML performance, however there are \nchallenges such as  commercial sensitivity . \n◼ Proposed  ways to improve ML accountability \ninclude auditing and impact assessments.  \n \nBackground  \nMachine learning  (ML), a type of artificial intelligence (AI, Box \n1), is increasingly being used for a variety of applications from \nverifying a person’s identity based on their voice to diagnosing \ndisease. ML has the potential to bring many social and \neconomic benefits , including increased labour produc tivity and \nimprove d services across a wide range of sectors.1–3  \nHowever, there are concerns that decisions that are made or \ninformed by the outputs of ML can lack transparency  and \naccountability. This can be a particular  issue for certain  types of \nML (such as ",
    "title": "Interpretable Machine Learning",
    "abstract": "Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.",
    "link": "https://www.semanticscholar.org/paper/b0c34618ffd1154f35863e2ce7250ac6b6f2c424",
    "published": "2019-11-07"
  },
  "107": {
    "pdf_path": "data/pdfs/machine learning_paper_385.pdf",
    "text_excerpt": "Machin e Learnin g 2: 99-102 , 1987\n© 1987 Kluwe r Academi c Publishers , Bosto n - Manufacture d in The Netherland s\nEDITORIA L\nMachin e Learnin g and Concep t Formatio n\nThe task of concep t formatio n\nThe vast majorit y of researc h in machin e learnin g has focuse d on the\nacquisitio n of concepts.  Ye t despit e this emphasis , littl e of the wor k has\nany direc t relevanc e to modelin g huma n concep t learning . For a variet y\nof reasons , machin e learnin g researcher s have tende d to mak e assumption s\nthat violat e our knowledg e abou t the representation , use, and acquisitio n\nof huma n concepts .\nFor instance , mos t AI researcher s have attempte d to describ e concept s\nin logica l terms , as the conjunctio n or disjunctio n of features . However ,\nwe know that man y huma n concept s canno t be describe d in such a logica l\nformalism . Som e instance s of a concep t are mor e typica l than others ; e.g.,\nrobin s are mor e typica l bird s than penguins . This woul d seem to requir e a\nmore flexibl e representation , combine d with some form of partia l matching .\nAnothe r commo n assumptio n is that concep t learnin g is supervised;  i.e.,\na benevolen t teache r is presen t to direc t the learnin g process . However , we\nknow that muc h huma n concep t learnin g is unsupervised.  Childre n form\nusefu l concept s long befor e they acquir e enoug h languag e to benefi t from a\ntutor' s advice , and they seem to accomplis h this throug h direc t interactio n\nwith thei r environment .\nResearcher s have also tende d to focu s on learnin g one or a few concept s\nat a singl e leve l of abstraction . However , we know that human s organize\ntheir concept s into larg e hierarchie s that describ e categorie s at varyin g\nlevels of specificity . Thus , model s of concep t learnin g shoul d addres s the\nformatio n of conceptua l hierarchie s as well as the formatio n of individua l\ncategories .\nFinally , muc h of the work on concep t acquisitio n has used nonincrem",
    "title": "Machine Learning and Concept Formation",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022896407371",
    "published": "2003-04-04T16:57:10Z"
  },
  "108": {
    "pdf_path": "data/pdfs/machine learning_paper_23.pdf",
    "text_excerpt": "Ten-year Survival Prediction\nfor Breast Cancer Patients\nChangmao Li, Han He, Yunze Hao, Caleb Ziems\nEmory University\nfchangmao.li, han.he, yunze.hao, cziems g@emory.edu\n1 Introduction\nDifferent stages of breast cancer require different treat-\nments. Understanding the current stage of a patient’s breast\ncancer is crucial then for applying the best treatment. For\nthis purpose, machine learning models may be used to\nlearn and predict patient survival or other clinical out-\ncomes from professionally-labeled features or large-scale\ngenomic proﬁles [2, 7].\nIn the present study, we train and tune models to predict\nthe 10-year survival of breast cancer patients using the\nMETABRIC (Molecular Taxonomy of Breast Cancer Inter-\nnational Consortium) dataset. This dataset includes both\nhand-labeled clinical data and high-dimensional genomic\ndata from over 2,000 patients. We trained our ﬁrst set of\nmodels on the clinical data and our second set of models\non genomic data. Our goal was to construct a model from\nthe latter set which could outperform our best model from\nthe former.\nWe deﬁned our learning objective as a classiﬁcation\nproblem with binary class labels. Class 1 was assigned to\npatients who had died of the disease within 10 years (120\nmonths) of prognosis. Class 2 was assigned to patients\nwho survived longer than 10 years.\nThe nature of the METABRIC data makes our learning\nproblem a challenge. We recognize that the clinical fea-\ntures were hand-selected by experts and reﬁned through\ndecades of medical research. Thus, we might reasonably\nexpect many of these features to be strong predictors of\npatient outcomes. Genomic data, on the other hand, is\nquite noisy, and may lead our models to overﬁt because the\nnumber of genomic features far outweighs the number of\nsamples. For this reason, we implemented four advancedlearning algorithms which we selected to overcome this\nchallenge. These include semi-supervised learning, L1-\nregularized logistic regression, a multi-layer perceptro",
    "title": "Ten-year Survival Prediction for Breast Cancer Patients",
    "abstract": "This report assesses different machine learning approaches to 10-year\nsurvival prediction of breast cancer patients.",
    "link": "http://arxiv.org/abs/1911.00776v1",
    "published": "2019-11-02T19:53:32Z"
  },
  "109": {
    "pdf_path": "data/pdfs/machine learning_paper_3.pdf",
    "text_excerpt": "arXiv:1707.04849v1  [cs.LG]  16 Jul 2017Minimax deviation strategies for machine\nlearning and recognition with short learning\nsamples\nSchlesinger M.I. and Vodolazskiy E.V.\nAugust 31, 2018\nAbstract\nThe article is devoted to the problem of small learning sampl es in\nmachine learning. Theﬂaws of maximum likelihood learning a nd min-\nimax learning are looked into and the concept of minimax devi ation\nlearning is introduced that is free of those ﬂaws.\n1 Introduction\nThe small learning sample problem has been around in machine learning\nunder diﬀerent names during its whole life. The learning sample is used t o\ncompensate for the lack of knowledge about the recognized objec t when its\nstatistical model is not completely known. Naturally, the longer the learning\nsample is, the better is the subsequent recognition. However, whe n the learn-\ning sample becomes too small (2, 3, 5 elements) an eﬀect of small sam ples\nbecomes evident. In spite of the fact that any learning sample (eve n a very\nsmall one) provides some additional information about the object, it may be\nbetter to ignore the learning sample than to utilize it with the commonly\nused methods.\nExample 1. Let us consider an object that can be in one of two random\nstatesy= 1 and y= 2 with equal probabilities. In each state the object\ngenerates two independent Gaussian randomsignals x1andx2with variances\nequal 1. Mean values of signals depend on the state as it is shown on F ig.\n11. In the ﬁrst state the mean value is (2 ,0). In the second state the mean\nvalue depends on an unknown parameter θand is (0,θ). Even if no learning\nsample is given a minimax strategy can be used to make a decision about the\nstatey. The minimax strategy ignores the second signal and makes decision\ny∗= 1 when x1>1 and decision y∗= 2 when x1≤1.\nx2x1\n/Bullet /Bullet\nθ/Bullet\n2\n0y∗= 1\ny∗= 2p(x1,x2|y= 1)\np(x1,x2|y= 2)\nFigure 1: Example 1. ( x1,x2)∈R2– signal, y∈ {1,2}– state.\nNow let us assume that there is a sample of signals generated by an\nobjecti",
    "title": "Minimax deviation strategies for machine learning and recognition with\n  short learning samples",
    "abstract": "The article is devoted to the problem of small learning samples in machine\nlearning. The flaws of maximum likelihood learning and minimax learning are\nlooked into and the concept of minimax deviation learning is introduced that is\nfree of those flaws.",
    "link": "http://arxiv.org/abs/1707.04849v1",
    "published": "2017-07-16T09:15:08Z"
  },
  "110": {
    "pdf_path": "data/pdfs/machine learning_paper_466.pdf",
    "text_excerpt": "Machine Learning, 39, 59–91, 2000.\nc°2000 Kluwer Academic Publishers. Printed in The Netherlands.\nA Machine Learning Approach to POS Tagging\nLLU´IS M`ARQUEZ lluism@lsi.upc.es\nLLU´IS PADR ´O padro@lsi.upc.es\nHORACIO RODR ´IGUEZ horacio@lsi.upc.es\nDepartament de Llenguatges i Sistemes Inform `atics, Universitat Polit `ecnica de Catalunya, c/ Jordi Girona 1–3.\nBarcelona 08034, Catalonia\nEditor:Raymond Mooney\nAbstract. WehaveappliedtheinductivelearningofstatisticaldecisiontreesandrelaxationlabelingtotheNatural\nLanguage Processing ( NLP) task of morphosyntactic disambiguation (Part Of Speech Tagging). The learning\nprocess is supervised and obtains a language model oriented to resolve POSambiguities, consisting of a set of\nstatisticaldecisiontreesexpressingdistributionoftagsandwordsinsomerelevantcontexts.Theacquireddecisiontrees have been directly used in a tagger that is both relatively simple and fast, and which has been tested andevaluated on the Wall Street Journal (\nWSJ) corpus with competitive accuracy. However, better results can be\nobtained by translating the trees into rules to feed a ﬂexible relaxation labeling based tagger. In this directionwe describe a tagger which is able to use information of any kind ( n-grams, automatically acquired constraints,\nlinguistically motivated manually written constraints, etc.), and in particular to incorporate the machine-learneddecisiontrees.Simultaneously,weaddresstheproblemoftaggingwhenonlylimitedtrainingmaterialisavailable,which is crucial in any process of constructing, from scratch, an annotated corpus. We show that high levels ofaccuracy can be achieved with our system in this situation, and report some results obtained when using it todevelop a 5.5 million words Spanish corpus from scratch.\nKeywords: part of speech tagging, corpus-based and statistical language modeling, decision trees induction,\nconstraint satisfaction, relaxation labeling\n1. Introduction\nPartofSpeech( POS)TaggingisaverybasicandwellknownNaturalLanguag",
    "title": "A Machine Learning Approach to POS Tagging",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1007673816718",
    "published": "2002-12-22T05:54:50Z"
  },
  "111": {
    "pdf_path": "data/pdfs/machine learning_paper_103.pdf",
    "text_excerpt": "ilastik: interactive machine learning for\n(bio)image analysis\nStuart Berg1, Dominik Kutra2,3, Thorben Kroeger2, Christoph N. Straehle2, Bernhard X. Kausler2, Carsten Haubold2, Martin\nSchiegg2, Janez Ales2, Thorsten Beier2, Markus Rudy2, Kemal Eren2, Jaime I Cervantes2, Buote Xu2, Fynn Beuttenmueller2,3,\nAdrian Wolny2, Chong Zhang2, Ullrich Koethe2, Fred A. Hamprecht2, \u0000, and Anna Kreshuk2,3, \u0000\n1HHMI Janelia Research Campus, Ashburn, Virginia, USA\n2HCI/IWR, Heidelberg University, Heidelberg, Germany\n3European Molecular Biology Laboratory, Heidelberg, Germany\nWe present ilastik, an easy-to-use interactive tool that brings\nmachine-learning-based (bio)image analysis to end users with-\nout substantial computational expertise. It contains pre-deﬁned\nworkﬂows for image segmentation, object classiﬁcation, count-\ning and tracking. Users adapt the workﬂows to the problem at\nhand by interactively providing sparse training annotations for\na nonlinear classiﬁer. ilastik can process data in up to ﬁve di-\nmensions (3D, time and number of channels). Its computational\nback end runs operations on-demand wherever possible, allow-\ning for interactive prediction on data larger than RAM. Once\nthe classiﬁers are trained, ilastik workﬂows can be applied to\nnew data from the command line without further user interac-\ntion. We describe all ilastik workﬂows in detail, including three\ncase studies and a discussion on the expected performance.\nmachine learning | image analysis | software\nCorrespondence:\nfred.hamprecht@iwr.uni-heidelberg.de\nanna.kreshuk@embl.de\nMain\nRapid development of imaging technology is bringing more\nand more life scientists to experimental pipelines where the\nsuccess of the entire undertaking hinges on the analysis of\nimages. Image segmentation, object tracking and counting\nare time consuming, tedious and error-prone processes when\nperformed manually. Besides, manual annotation is hard to\nscale for biological images, since expert annotators are typ-\nically required for cor",
    "title": "ilastik: interactive machine learning for (bio)image analysis",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/5d433da6d0f143f20936379910104d2bb139d4ae",
    "published": "2019-09-30"
  },
  "112": {
    "pdf_path": "data/pdfs/machine learning_paper_671.pdf",
    "text_excerpt": "Machine Learning 3: 373-375, 1989\n© 1989 Kluwer Academic Publishers - Manufactured in The Netherlands\nNews and Notes\n1. Third International Conference on Genetic Algorithms\nGeorge Mason University; June 4-7, 1989\nThe Third International Conference on Genetic Algorithms (ICGA-89), will\nbe held on June 4-7, 1989, at George Mason University in Fairfax, Virginia\n(near Washington, DC). Authors are invited to submit papers on all aspects\nof genetic algorithms, including: foundations of genetic algorithms, search,\noptimization, machine learning using genetic algorithms, classifier systems,\nalgorithms for credit apportionment, and relationships to other search and\nlearning paradigms. Papers discussing specific applications (e.g., operations\nresearch, engineering, science) are encouraged.\nAuthors should send four copies (hard copy only) of a full paper to the\nprogram chair:\nJ. David Schaffer\nPhilips Laboratories\n345 Scarborough Road\nBriarcliff Manor, New York 10510, U.S.A.\nTelephone: (914) 945-6168\nEmail: DSl@PHILABS.PHILIPS.COM\nSubmissions must be received by February 10, 1989. Authors will be notified\nof acceptance or rejection by March 10, and revised, camera-ready versions\nwill be due by April 10.\nThe program committee for the conference includes: Kenneth A. De Jong,\nGeorge Mason University (Conference Chair); Lashon B. Booker, Naval Re-\nsearch Laboratory (Local Arrangements); J. David Schaffer, Philips Laborato-\nries (Program Chair); Lawrence Davis, Bolt, Beranek and Newman, Inc.; David\nE. Goldberg, University of Alabama; John J. Grefenstette, Naval Research Lab-\noratory; John H. Holland, University of Michigan; George G. Robertson, Xerox\nPARC; Stephen F. Smith, Carnegie Mellon University; and Stewart W. Wilson,\nRowland Institute for Science.\n2. Second Workshop on Computational Learning Theory\nUniversity of California, Santa Cruz; July 31-August 2, 1989\nThe Second Workshop on Computational Learning Theory (COLT '89) will\nbe held at the University of California, Santa Cr",
    "title": "News and Notes",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022649902345",
    "published": "2003-04-04T16:55:36Z"
  },
  "113": {
    "pdf_path": "data/pdfs/machine learning_paper_71.pdf",
    "text_excerpt": "Systematic Training and Testing for Machine\nLearning Using Combinatorial Interaction Testing\nTyler Codya\u0003, Erin Lanusb, Daniel D. Doylea, Laura Freemana\naNational Security Institute, Virginia Tech , Arlington Virginia, USA\nbThe MITRE Corporation , McLean Virginia, USA\n\u0003Corresponding Author: tcody@vt.edu\nAbstract —This paper demonstrates the systematic use of\ncombinatorial coverage for selecting and characterizing test\nand training sets for machine learning models. The presented\nwork adapts combinatorial interaction testing, which has been\nsuccessfully leveraged in identifying faults in software testing, to\ncharacterize data used in machine learning. The MNIST hand-\nwritten digits data is used to demonstrate that combinatorial\ncoverage can be used to select test sets that stress machine\nlearning model performance, to select training sets that lead to\nrobust model performance, and to select data for ﬁne-tuning\nmodels to new domains. Thus, the results posit combinatorial\ncoverage as a holistic approach to training and testing for\nmachine learning. In contrast to prior work which has focused\non the use of coverage in regard to the internal of neural\nnetworks, this paper considers coverage over simple features\nderived from inputs and outputs. Thus, this paper addresses\nthe case where the supplier of test and training sets for machine\nlearning models does not have intellectual property rights to the\nmodels themselves. Finally, the paper addresses prior criticism of\ncombinatorial coverage and provides a rebuttal which advocates\nthe use of coverage metrics in machine learning applications.\nIndex Terms —combinatorial interaction testing, machine\nlearning, black-box testing, test set construction, training set\nconstruction, data labeling\nI. I NTRODUCTION\nCombinatorial interaction testing has been widely used in\nsoftware testing to understand code coverage, fault location,\nand to develop pseudo-exhaustive test strategies. Testing ma-\nchine learning (ML), however, introduces ne",
    "title": "Systematic Training and Testing for Machine Learning Using Combinatorial\n  Interaction Testing",
    "abstract": "This paper demonstrates the systematic use of combinatorial coverage for\nselecting and characterizing test and training sets for machine learning\nmodels. The presented work adapts combinatorial interaction testing, which has\nbeen successfully leveraged in identifying faults in software testing, to\ncharacterize data used in machine learning. The MNIST hand-written digits data\nis used to demonstrate that combinatorial coverage can be used to select test\nsets that stress machine learning model performance, to select training sets\nthat lead to robust model performance, and to select data for fine-tuning\nmodels to new domains. Thus, the results posit combinatorial coverage as a\nholistic approach to training and testing for machine learning. In contrast to\nprior work which has focused on the use of coverage in regard to the internal\nof neural networks, this paper considers coverage over simple features derived\nfrom inputs and outputs. Thus, this paper addresses the case where the supplier\nof test and training sets for machine learning models does not have\nintellectual property rights to the models themselves. Finally, the paper\naddresses prior criticism of combinatorial coverage and provides a rebuttal\nwhich advocates the use of coverage metrics in machine learning applications.",
    "link": "http://arxiv.org/abs/2201.12428v1",
    "published": "2022-01-28T21:33:31Z"
  },
  "114": {
    "pdf_path": "data/pdfs/machine learning_paper_130.pdf",
    "text_excerpt": "FUNDAMENTALS\nMachine learning and deep learning\nChristian Janiesch1&Patrick Zschech2&Kai Heinrich3\nReceived: 7 October 2020 / Accepted: 19 March 2021\n#The Author(s) 2021, corrected publication 2021\nAbstract\nToday, intelligent systems that offer artificial intelligence capabilities often rely on machine learning. Machine learning describes\nthe capacity of systems to learn from problem-specific training data to automate the process of analytical model building and\nsolve associated tasks. Deep learning is a machine learning concept based on artificial neural networks. For many applications,\ndeep learning models outperform shallow machine learning models and traditional data analysis approaches. In this article, we\nsummarize the fundamentals of machine learning and deep learning to generate a broader understanding of the methodical\nunderpinning of current intelligent systems. In particular, we provide a conceptual distinction between relevant terms and\nconcepts, explain the process of automated analytical model building through machine learning and deep learning, and discuss\nthe challenges that arise when implementing such intelligent systems in the field of electronic markets and networked business.\nThese naturally go beyond technological aspects and highlight issues in human-machine interaction and artificial intelligence\nservitization.\nKeywords Machine learning .Deep learning .Artificial intelligence .Artificial neural networks .Analytical model building\nJEL classification C6.C8.M15 .O3\nIntroduction\nIt is considered easier to explain to a child the nature of what\nconstitutes a sports car as opposed to a normal car by showing\nhim or her examples, rather than trying to formulate explicit\nrules that define a sports car.\nSimilarly, instead of codifying knowledge into computers,\nmachine learning (ML) seeks to automatically learnmeaningful relationships and patterns from examples and ob-\nservations (Bishop 2006 ). Advances in ML have enabled the\nrecent rise of intelligent ",
    "title": "Machine learning and deep learning",
    "abstract": "Today, intelligent systems that offer artificial intelligence capabilities often rely on machine learning. Machine learning describes the capacity of systems to learn from problem-specific training data to automate the process of analytical model building and solve associated tasks. Deep learning is a machine learning concept based on artificial neural networks. For many applications, deep learning models outperform shallow machine learning models and traditional data analysis approaches. In this article, we summarize the fundamentals of machine learning and deep learning to generate a broader understanding of the methodical underpinning of current intelligent systems. In particular, we provide a conceptual distinction between relevant terms and concepts, explain the process of automated analytical model building through machine learning and deep learning, and discuss the challenges that arise when implementing such intelligent systems in the field of electronic markets and networked business. These naturally go beyond technological aspects and highlight issues in human-machine interaction and artificial intelligence servitization.",
    "link": "https://www.semanticscholar.org/paper/a0f303b6e22ef52943355993f57d65938997066a",
    "published": "2021-04-08"
  },
  "115": {
    "pdf_path": "data/pdfs/machine learning_paper_277.pdf",
    "text_excerpt": "1\nMachine Learning Testing:\nSurvey, Landscapes and Horizons\nJie M. Zhang*, Mark Harman, Lei Ma, Y ang Liu\nAbstract —This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing\n(ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components\n(e.g., the data, learning program, and framework), testing workﬂow (e.g., test generation and test evaluation), and application scenarios\n(e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research\nfocus, concluding with research challenges and promising research directions in ML testing.\nIndex Terms —machine learning, software testing, deep neural network,\nF\n1 I NTRODUCTION\nThe prevalent applications of machine learning arouse\nnatural concerns about trustworthiness. Safety-critical ap-\nplications such as self-driving systems [1], [2] and medical\ntreatments [3], increase the importance of behaviour relating\nto correctness, robustness, privacy, efﬁciency and fairness.\nSoftware testing refers to any activity that aims to detect\nthe differences between existing and required behaviour [4].\nWith the recent rapid rise in interest and activity, testing\nhas been demonstrated to be an effective way to expose\nproblems and potentially facilitate to improve the trustwor-\nthiness of machine learning systems.\nFor example, DeepXplore [1], a differential white-box\ntesting technique for deep learning, revealed thousands of\nincorrect corner case behaviours in autonomous driving\nlearning systems; Themis [5], a fairness testing technique\nfor detecting causal discrimination, detected signiﬁcant ML\nmodel discrimination towards gender, marital status, or race\nfor as many as 77.2% of the individuals in datasets to which\nit was applied.\nIn fact, some aspects of the testing problem for machine\nlearning systems are shared with well-known solutions\nalready widely",
    "title": "Machine Learning Testing: Survey, Landscapes and Horizons",
    "abstract": "This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.",
    "link": "https://www.semanticscholar.org/paper/218062f45c15f39bc8f4fb2c930ddf20b5809b11",
    "published": "2019-06-19"
  },
  "116": {
    "pdf_path": "data/pdfs/machine learning_paper_341.pdf",
    "text_excerpt": "Potential Biases in Machine Learning Algorithms Using \nElectronic Health Record Data\nMilena A. Gianfrancesco, PhD, MPH , Suzanne Tamang, PhD, MS , Jinoos Yazdany, MD, \nMPH, and Gabriela Schmajuk, MD, MS\nDivision of Rheumatology, Department of Medicine, University of California, San Francisco \n(Gianfrancesco, Yazdany, Schmajuk); Center for Population Health Sciences, Stanford University, \nPalo Alto, California (Tamang); Veterans Affairs Medical Center, San Francisco, California \n(Schmajuk).\nAbstract\nA promise of machine learning in health care is the avoidance of biases in diagnosis and treatment; \na computer algorithm could objectively synthesize and interpret the data in the medical record. \nIntegration of machine learning with clinical decision support tools, such as computerized alerts or \ndiagnostic support, may offer physicians and others who provide health care targeted and timely \ninformation that can improve clinical decisions. Machine learning algorithms, however, may also \nbe subject to biases. The biases include those related to missing data and patients not identified by \nalgorithms, sample size and underestimation, and misclassification and measurement error. There \nis concern that biases and deficiencies in the data used by machine learning algorithms may \ncontribute to socioeconomic disparities in health care. This Special Communication outlines the \npotential biases that may be introduced into machine learning–based clinical decision support \ntools that use electronic health record data and proposes potential solutions to the problems of \noverreliance on automation, algorithms based on biased data, and algorithms that do not provide \ninformation that is clinically meaningful. Existing health care disparities should not be amplified \nby thoughtless or excessive reliance on machines.\nCorresponding Author:  Milena A. Gianfrancesco, PhD, MPH, Division of Rheumatology, Department of Medicine, University of \nCalifornia, San Francisco, 513 Parnassus Ave, Sa",
    "title": "Potential Biases in Machine Learning Algorithms Using Electronic Health Record Data",
    "abstract": "A promise of machine learning in health care is the avoidance of biases in diagnosis and treatment; a computer algorithm could objectively synthesize and interpret the data in the medical record. Integration of machine learning with clinical decision support tools, such as computerized alerts or diagnostic support, may offer physicians and others who provide health care targeted and timely information that can improve clinical decisions. Machine learning algorithms, however, may also be subject to biases. The biases include those related to missing data and patients not identified by algorithms, sample size and underestimation, and misclassification and measurement error. There is concern that biases and deficiencies in the data used by machine learning algorithms may contribute to socioeconomic disparities in health care. This Special Communication outlines the potential biases that may be introduced into machine learning–based clinical decision support tools that use electronic health record data and proposes potential solutions to the problems of overreliance on automation, algorithms based on biased data, and algorithms that do not provide information that is clinically meaningful. Existing health care disparities should not be amplified by thoughtless or excessive reliance on machines.",
    "link": "https://www.semanticscholar.org/paper/161ce338538f94b0b9be51ae2336db0aa4b012e5",
    "published": "2018-11-01"
  },
  "117": {
    "pdf_path": "data/pdfs/machine learning_paper_128.pdf",
    "text_excerpt": " \n \n Vol. 02, No. 01, pp. 20 – 28 (2021 ) \nISSN: 2708 -0757 \n \nJOURNAL OF APPLIED SCIENCE AND TECHNOLOGY TRENDS  \n \nwww.jastt.org   \n \n20 \ndoi: 10.38094/jastt20 165       \n \nClassification Based on Decision Tree Algorithm for \nMachine Learning  \n \nBahzad Taha Jijo1*, Adnan Mohsin Abdulazeez2 \n1IT Department , Technical College of Informatics Akre, Duhok Polytechnic University, Duhok , Kurdistan Region, Iraq, \nbahzad .taha@dpu.edu.krd   \n2Presidency of Duhok Polytechnic University, Duhok, Kurdistan Region, Iraq, adnan.mohsin@dpu.edu.krd  \n*Correspondence : bahzad .taha@dpu.edu.krd   \n \n \nAbstract  \nDecision tree classifiers are regarded to be a standout of the most well -known methods to data classification representation of classifiers. \nDifferent researchers from various fields and backgrounds have considered the problem of extending a decision tree from avail able \ndata, such as m achine study, pattern recognition, and statistics. In various fields such as medical disease analysis, text classification, \nuser smartphone classification, images, and many more the employment of Decision tree classifiers has been proposed in many w ays. \nThis paper provides a detailed approach to the decision trees. Furthermore, paper specifics, such as algorithms/approaches used , \ndatasets, and outcomes achieved, are evaluated and outlined comprehensively. In addition, all of the approaches analyzed were  \ndiscussed to illustrate the themes of the authors and identify the most accurate classifiers. As a result, the uses of different  types of \ndatasets are discussed and their findings are analyzed.  \n \nKeywords:  Machine Learning, Supervised, Classification, Decision Tree.  \nReceived: January  11th, 2021  / Accepted: March 15th, 2021  / Online: March 24th, 2021  \n \nI. INTRODUCTION  \nNowadays, technology has developed a lot, especially in \nthe field of Machine Learning (ML), which is useful for \nreducing human work. In the field of artificial intelligence, ML \nintegrates statistics an",
    "title": "Classification Based on Decision Tree Algorithm for Machine Learning",
    "abstract": "Decision tree classifiers are regarded to be a standout of the most well-known methods to data classification representation of classifiers. Different researchers from various fields and backgrounds have considered the problem of extending a decision tree from available data, such as machine study, pattern recognition, and statistics. In various fields such as medical disease analysis, text classification, user smartphone classification, images, and many more the employment of Decision tree classifiers has been proposed in many ways. This paper provides a detailed approach to the decision trees. Furthermore, paper specifics, such as algorithms/approaches used, datasets, and outcomes achieved, are evaluated and outlined comprehensively. In addition, all of the approaches analyzed were discussed to illustrate the themes of the authors and identify the most accurate classifiers. As a result, the uses of different types of datasets are discussed and their findings are analyzed.",
    "link": "https://www.semanticscholar.org/paper/0d6ef817813d04a3b3ec6c3ce008e104fb3e587a",
    "published": "2021-03-24"
  },
  "118": {
    "pdf_path": "data/pdfs/machine learning_paper_261.pdf",
    "text_excerpt": "Noname manuscript No.\n(will be inserted by the editor)\nMachine Learning with a Reject Option: A survey\nHendrickx Kilian* ·Perini Lorenzo* ·Van\nder Plas Dries ·Meert Wannes ·Davis Jesse\nReceived: 23 July 2021 / Revised: 27 July 2023 & 15 February 2024\nAbstract Machine learning models always make a prediction, even when it is\nlikely to be inaccurate. This behavior should be avoided in many decision support\napplications, where mistakes can have severe consequences. Albeit already studied\nin 1970, machine learning with rejection recently gained interest. This machine\nlearning subfield enables machine learning models to abstain from making a pre-\ndiction when likely to make a mistake.\nThis survey aims to provide an overview on machine learning with rejection.\nWe introduce the conditions leading to two types of rejection, ambiguity and nov-\nelty rejection, which we carefully formalize. Moreover, we review and categorize\nstrategies to evaluate a model’s predictive and rejective quality. Additionally, we\ndefine the existing architectures for models with rejection and describe the stan-\ndard techniques for learning such models. Finally, we provide examples of relevant\n* These authors contributed equally to this work.\n·K. Hendrickx\nSiemens Digital Industries Software, Leuven, Belgium\nKU Leuven, Leuven, Belgium\nE-mail: kilian.hendrickx@cs.kuleuven.be\nL. Perini\nKU Leuven, Leuven, Belgium\nE-mail: lorenzo.perini@cs.kuleuven.be\nD. Van der Plas\nOSG bv, Natus Medical, Kontich, Belgium\nKU Leuven, Leuven, Belgium\nUniversity of Antwerp, Antwerp, Belgium\nE-mail: dries.vanderplas@cs.kuleuven.be\nW. Meert\nKU Leuven; Leuven.AI, Leuven, Belgium\nE-mail: wannes.meert@cs.kuleuven.be\nJ. Davis\nKU Leuven; Leuven.AI, Leuven, Belgium\nE-mail: jesse.davis@cs.kuleuven.be\nCorresponding author: Kilian HendrickxarXiv:2107.11277v3  [cs.LG]  21 Feb 20242 Hendrickx K., Perini L., et al\napplication domains and show how machine learning with rejection relates to other\nmachine learning research areas.\nKeywords ",
    "title": "Machine Learning with a Reject Option: A survey",
    "abstract": "Machine learning models always make a prediction, even when it is likely to be inaccurate. This behavior should be avoided in many decision support applications, where mistakes can have severe consequences. Albeit already studied in 1970, machine learning with rejection recently gained interest. This machine learning subfield enables machine learning models to abstain from making a prediction when likely to make a mistake. This survey aims to provide an overview on machine learning with rejection. We introduce the conditions leading to two types of rejection, ambiguity and novelty rejection, which we carefully formalize. Moreover, we review and categorize strategies to evaluate a model's predictive and rejective quality. Additionally, we define the existing architectures for models with rejection and describe the standard techniques for learning such models. Finally, we provide examples of relevant application domains and show how machine learning with rejection relates to other machine learning research areas.",
    "link": "https://www.semanticscholar.org/paper/24864a7f899718477c04ede9c0bea906c5dc2667",
    "published": "2021-07-23"
  },
  "119": {
    "pdf_path": "data/pdfs/machine learning_paper_37.pdf",
    "text_excerpt": "epl draft\nMachine learning in physics: a short guide\nFrancisco A. Rodrigues\n1Instituto de Ciˆ encias Matem´ aticas e de Computa¸ c˜ ao, Universidade de S˜ ao Paulo, S˜ ao, Carlos, S˜ ao Paulo, Brazil\nPACS 42.30.Sy – Pattern recognition\nPACS 07.05.Mh – Neural networks, fuzzy logic, artificial intelligence\nAbstract – Machine learning is a rapidly growing field with the potential to revolutionize many\nareas of science, including physics. This review provides a brief overview of machine learning in\nphysics, covering the main concepts of supervised, unsupervised, and reinforcement learning, as\nwell as more specialized topics such as causal inference, symbolic regression, and deep learning.\nWe present some of the principal applications of machine learning in physics and discuss the\nassociated challenges and perspectives.\nErnest Rutherford once declared: “if your experiment\nneeds statistics, you ought to have done a better experi-\nment” [1]. His remark reflects his belief in the significance\nof well-controlled experiments and the need for experi-\nmental designs that minimize uncertainties and sources of\nerrors. However, while Rutherford’s statement may have\nmerit in his time, it no longer applies in the modern scien-\ntific landscape. The growing complexity of experiments,\nthe necessity to quantify uncertainty, the pivotal role of\nhypothesis testing and inference, the incorporation of sta-\ntistical analysis in experimental design and power analysis,\nand the emergence of advanced data analysis techniques\nhave rendered statistics an indispensable tool in contem-\nporary scientific research [2, 3]. During the last decades,\nstatistics have empowered researchers to navigate complex\ndata, derive valid conclusions, and make evidence-based\ndecisions, playing an instrumental role in advancing scien-\ntific understanding [1,4,5].\nIn the last decade, machine learning (ML) methods have\ncomplemented the statistical analysis in Physics [2, 6–9].\nML has been used in processing satellite dat",
    "title": "Machine learning in physics: a short guide",
    "abstract": "Machine learning is a rapidly growing field with the potential to\nrevolutionize many areas of science, including physics. This review provides a\nbrief overview of machine learning in physics, covering the main concepts of\nsupervised, unsupervised, and reinforcement learning, as well as more\nspecialized topics such as causal inference, symbolic regression, and deep\nlearning. We present some of the principal applications of machine learning in\nphysics and discuss the associated challenges and perspectives.",
    "link": "http://arxiv.org/abs/2310.10368v1",
    "published": "2023-10-16T13:05:47Z"
  },
  "120": {
    "pdf_path": "data/pdfs/Machine Learning for Clinical Predictive Analytics_paper_13.pdf",
    "text_excerpt": "Clinical XLNet: Modeling Sequential Clinical Notes\nand Predicting Prolonged Mechanical Ventilation\nKexin Huang*1,Abhishek Singh*2,Sitong Chen*3,Edward T. Moseley4,\nChih-Ying Deng4,Naomi George5, and Charlotta Lindvall4,5\n1Health Data Science, Harvard T.H. Chan School of Public Health\n2Media Lab, Massachusetts Institute of Technology\n3Biomedical Informatics, Harvard Medical School\n4Dana-Farber Cancer Institute\n5Brigham and Women’s Hospital\n1kexinhuang@hsph.harvard.edu, abhi24@mit.edu, sitong_chen@hms.harvard.edu,\nnrgeorge@bwh.harvard.edu,\n{edward_moseley,chih-ying_deng,charlotta_lindvall}@dfci.harvard.edu\nAbstract\nClinical notes contain rich data, which is unexploited in predictive modeling compared\nto structured data. In this work, we developed a new text representation Clinical XLNet\nfor clinical notes which also leverages the temporal information of the sequence of the\nnotes. We evaluated our models on prolonged mechanical ventilation prediction problem\nand our experiments demonstrated that Clinical XLNet outperforms the best baselines\nconsistently.\n1 Introduction\nUnstructured clinical notes within Electronic Health Records (EHR) contain valuable information to\nsupport clinical decisions [ 1]. However, most prognostic models used in medical practice currently\nrely on scoring systems that only incorporates structured data [ 2;3;4;5]. Yet to make accurate clinical\ndecisions, clinicians have to go through numerous clinical notes to extract additional prognostic\ninformation from unstructured text. This adds to the workload of clinicians when they need to make\nquick decisions and thus may introduce human errors. Therefore, predictive models that utilize\nunstructured data could be very helpful in medical practice.\nA major challenge to unleash the clinical power of unstructured data is in representing notes in\nways that allow effective mining of clinically meaningful knowledge. Natural Language Processing\n(NLP) methods can be used to generate effective notes representati",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "121": {
    "pdf_path": "data/pdfs/machine learning_paper_653.pdf",
    "text_excerpt": "MachineLearning, 29, 45–63(1997)\nc°1997 Kluwer AcademicPublishers. ManufacturedinThe Netherlands.\nOnline LearningversusOfﬂineLearning *\nSHAI BEN-DAVID shai@cs.technion.ac.il\nComputerScience Dept.,Technion,Israel.\nEYAL KUS HILEVITZ ** eyalk@cs.technion.ac.il\nComputerScience Dept.,Technion,Israel.\nYISHAY MANSOUR y mansour@gemini.math.tau.ac.il\nComputerScience Dept.,Tel-AvivUniversity, Israel.\nEditor:Sally Goldman\nAbstract. Wepresentanoff-linevariantof themistake-boundmodelof learning. Thisisanintermediatemodel\nbetween the on-linelearning model (Littlestone, 1988, Littlestone, 1989) and the self-directed learning model\n(Goldman,Rivest&Schapire, 1993,Goldman &Sloan,1994). Just likeinthe othertwomodels,alearner inthe\noff-line model has to learn an unknown concept from a sequence of elements of the instance space on which it\nmakes “guess and test” trials. In all models, the aim of the learner is to make as fewmistakes as possible. The\ndifferencebetweenthemodelsisthat,whileintheon-linemodelonlythe setofpossibleelementsisknown,inthe\noff-linemodelthe sequence of elements(i.e.,the identityofthe elementsas wellastheorderinwhichtheyare to\nbe presented) is knownto the learner in advance. On the other hand, the learner is weaker than the self-directed\nlearner,which isallowedtochoose adaptivelythe sequenceof elementspresented tohim.\nWe study some of the fundamental properties of the off-linemodel. In particular, we compare the number of\nmistakes made by the off-line learner on certain concept classes to those made by the on-line and self-directedlearners. Wegiveboundsonthepossiblegapsbetweenthevariousmodelsandshowexamplesthatprovethatour\nboundsare tight.\nAnothercontributionofthispaperistheextensionofthecombinatorialtooloflabeledtreestoauniﬁedapproach\nthatcapturesthevariousmistakeboundmeasuresof allthe modelsdiscussed. Webelievethatthistool willprove\ntobe usefulforfurther studyof modelsof incrementallearning.\nKeywords: On-Line Learning. Mistake-Bound,Rankof Trees\n1. Introduction\nTh",
    "title": "Online Learning versus Offline Learning",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1007465907571",
    "published": "2002-12-22T04:48:21Z"
  },
  "122": {
    "pdf_path": "data/pdfs/machine learning_paper_42.pdf",
    "text_excerpt": "arXiv:1505.06614v1  [stat.ML]  25 May 2015Electre Tri-Machine Learning Approach to the\nRecord Linkage Problem\nRenato De Leone∗Valentina Minnetti†‡\nNovember 10, 2021\nAbstract\nIn this short paper, the Electre Tri-Machine Learning Metho d, gener-\nally used to solve ordinal classiﬁcation problems, is propo sed for solving\nthe Record Linkage problem. Preliminary experimental resu lts show that,\nusing the Electre Tri method, high accuracy can be achieved a nd more\nthan 99% of the matches and nonmatches were correctly identi ﬁed by the\nprocedure.\n1 Introduction\nMachine Learning is a scientiﬁc discipline that is concerned with the des ign\nand development of algorithms that allow computers to “learn data” . More\nprecisely, “learn” is here intended as the possibility to automatically r ecognize\ncomplex patterns and make “intelligent” decisions, based on informa tion data.\nHence, machine learning is closely related to ﬁelds such as statistics, probability\ntheory, data mining, pattern recognition, artiﬁcial intelligence, ad aptive control\nand theoretical computer science.\nMachine learning algorithms can be classiﬁed in the following types:\n•supervised learning algorithms: a function/classiﬁer is generated, that\nmaps outputs on the training inputs, based on labeled examples input -\noutput;\n•unsupervised learning algorithms: patterns in the input are recogn ized,\nthe examples have no labels;\n•semi-supervised learning algorithms: supervised and unsupervised learn-\ning information is combined;\n∗School ofScience and Technology, Universityof Camerino, I talyrenato.deleone@unicam.it\n†Faculty of Information Engineering, Informatics and Stati stics, Sapienza University of\nRome, Italy valentina.minnetti@uniroma1.it\n‡Italian National Institute of Statistics, Rome, Italy minnetti@istat.it\n1•reinforcement learning: actions from observation of the world are gener-\nated. Every action has some impact in the environment and the envir on-\nment provides feedbacks that are translated into a score ",
    "title": "Electre Tri-Machine Learning Approach to the Record Linkage Problem",
    "abstract": "In this short paper, the Electre Tri-Machine Learning Method, generally used\nto solve ordinal classification problems, is proposed for solving the Record\nLinkage problem. Preliminary experimental results show that, using the Electre\nTri method, high accuracy can be achieved and more than 99% of the matches and\nnonmatches were correctly identified by the procedure.",
    "link": "http://arxiv.org/abs/1505.06614v1",
    "published": "2015-05-25T13:02:32Z"
  },
  "123": {
    "pdf_path": "data/pdfs/machine learning_paper_112.pdf",
    "text_excerpt": "Large-Scale Machine Learning\nwith Stochastic Gradient Descent\nL\u0013 eon Bottou\nNEC Labs America, Princeton NJ 08542, USA\nleon@bottou.org\nAbstract. During the last decade, the data sizes have grown faster than the speed\nof processors. In this context, the capabilities of statistical machine learning meth-\nods is limited by the computing time rather than the sample size. A more pre-\ncise analysis uncovers qualitatively di\u000berent tradeo\u000bs for the case of small-scale\nand large-scale learning problems. The large-scale case involves the computational\ncomplexity of the underlying optimization algorithm in non-trivial ways. Unlikely\noptimization algorithms such as stochastic gradient descent show amazing perfor-\nmance for large-scale problems. In particular, second order stochastic gradient and\naveraged stochastic gradient are asymptotically e\u000ecient after a single pass on the\ntraining set.\nKeywords: Stochastic gradient descent, Online learning, E\u000eciency\n1 Introduction\nThe computational complexity of learning algorithm becomes the critical\nlimiting factor when one envisions very large datasets. This contribution ad-\nvocates stochastic gradient algorithms for large scale machine learning prob-\nlems. The \frst section describes the stochastic gradient algorithm. The sec-\nond section presents an analysis that explains why stochastic gradient algo-\nrithms are attractive when the data is abundant. The third section discusses\nthe asymptotical e\u000eciency of estimates obtained after a single pass over the\ntraining set. The last section presents empirical evidence.\n2 Learning with gradient descent\nLet us \frst consider a simple supervised learning setup. Each example z\nis a pair (x;y) composed of an arbitrary input xand a scalar output y. We\nconsider a loss function `(^y;y) that measures the cost of predicting ^ ywhen the\nactual answer is y, and we choose a family Fof functions fw(x) parametrized\nby a weight vector w. We seek the function f2F that minimizes the loss\nQ(z;w) =`(fw(x);y) averag",
    "title": "Large-Scale Machine Learning with Stochastic Gradient Descent",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/fbc6562814e08e416e28a268ce7beeaa3d0708c8",
    "published": null
  },
  "124": {
    "pdf_path": "data/pdfs/machine learning_paper_379.pdf",
    "text_excerpt": "Machine Learning 2: 5-8, 1987\n© 1987 Kluwer Academic Publishers, Boston - Manufactured in The Netherlands\nEDITORIAL\nMachine Learning and Grammar Induction\nLanguage and the Acquisition of Syntax\nLanguage is a major component of cognition, and as such, its acquisition\nhas been a central concern of machine learning researchers. Some of the\nearliest AI learning work focused on this topic, and interest has continued\nto the present. However, progress in this area has been much slower than\nin most other learning tasks, undoubtedly due to the inherent complexity\nof natural language.\nDespite its complexity, the task of natural language processing can be\ndivided into a number of well-defined subtasks, and one of these centers on\nsyntax. This aspect of language has been studied in detail by linguists, and\ndevelopmental studies have provided a variety of empirical generalizations\nabout the stages that children traverse in their acquisition of grammar.\nBecause our knowledge of syntax is more complete than that for other\ncomponents of language, the vast majority of language-related research in\nmachine learning has focused on the task of grammar induction.\nTwo Views of Grammar Induction\nWithin this effort, two different paradigms have emerged for describ-\ning the grammar induction task. The first approach was formulated by\nSolomonoff (1959) and others in the early days of AI. It assumed only a\nset of legal sentences as input, from which the learner induced a grammar\nthat would parse those sentences. This approach was quite popular during\nthe 1960's, during which Gold (1967) and others formulated a number of\nformal results about the task. This paradigm has sometimes been called\ngrammatical inference.\nThe second approach did not appear until the late 1960's, when some\nresearchers noted that in natural languages, grammars were used for more\nthan simply parsing sentences - they also mapped sentences onto meaning\nstructures. This led to an alternative view of the grammar induction task",
    "title": "Machine Learning and Grammar Induction",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022808826027",
    "published": "2003-04-04T16:57:10Z"
  },
  "125": {
    "pdf_path": "data/pdfs/machine learning_paper_369.pdf",
    "text_excerpt": "1\nA Survey on Data Collection for\nMachine Learning\nA Big Data - AI Integration Perspective\nYuji Roh, Geon Heo, Steven Euijong Whang, Senior Member, IEEE\nAbstract —Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are\nlargely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we\nare seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep\nlearning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts\nof labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and\ncomputer vision communities, but also from the data management community due to the importance of handling large amounts of data.\nIn this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely\nconsists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these\noperations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of\nmachine learning and data management for data collection is part of a larger trend of Big data and Artiﬁcial Intelligence (AI) integration\nand opens many opportunities for new research.\nIndex Terms —data collection, data acquisition, data labeling, machine learning\nF\n1 I NTRODUCTION\nWEare living in exciting times where machine learning\nis having a profound inﬂuence on a wide range of\napplications from text understanding, image and speech\nrecognition, to health care and genomics. As a striking\nexample, deep learning techniques are known to perform\non par with ophthalmologists on identifying diabetic eye\ndiseases in images [1]. Much of the recent success is due\nto ",
    "title": "A Survey on Data Collection for Machine Learning: A Big Data - AI Integration Perspective",
    "abstract": "Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.",
    "link": "https://www.semanticscholar.org/paper/3a83d8595e6727269c876fcebd23ee9ddd524b76",
    "published": "2018-11-08"
  },
  "126": {
    "pdf_path": "data/pdfs/machine learning_paper_124.pdf",
    "text_excerpt": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 1724–1734,\nOctober 25-29, 2014, Doha, Qatar. c\r2014 Association for Computational Linguistics\nLearning Phrase Representations using RNN Encoder–Decoder\nfor Statistical Machine Translation\nKyunghyun Cho\nBart van Merri ¨enboer Caglar Gulcehre\nUniversit ´e de Montr ´eal\nfirstname.lastname@umontreal.caDzmitry Bahdanau\nJacobs University, Germany\nd.bahdanau@jacobs-university.de\nFethi Bougares Holger Schwenk\nUniversit ´e du Maine, France\nfirstname.lastname@lium.univ-lemans.frYoshua Bengio\nUniversit ´e de Montr ´eal, CIFAR Senior Fellow\nfind.me@on.the.web\nAbstract\nIn this paper, we propose a novel neu-\nral network model called RNN Encoder–\nDecoder that consists of two recurrent\nneural networks (RNN). One RNN en-\ncodes a sequence of symbols into a ﬁxed-\nlength vector representation, and the other\ndecodes the representation into another se-\nquence of symbols. The encoder and de-\ncoder of the proposed model are jointly\ntrained to maximize the conditional prob-\nability of a target sequence given a source\nsequence. The performance of a statisti-\ncal machine translation system is empiri-\ncally found to improve by using the con-\nditional probabilities of phrase pairs com-\nputed by the RNN Encoder–Decoder as an\nadditional feature in the existing log-linear\nmodel. Qualitatively, we show that the\nproposed model learns a semantically and\nsyntactically meaningful representation of\nlinguistic phrases.\n1 Introduction\nDeep neural networks have shown great success in\nvarious applications such as objection recognition\n(see, e.g., (Krizhevsky et al., 2012)) and speech\nrecognition (see, e.g., (Dahl et al., 2012)). Fur-\nthermore, many recent works showed that neu-\nral networks can be successfully used in a num-\nber of tasks in natural language processing (NLP).\nThese include, but are not limited to, language\nmodeling (Bengio et al., 2003), paraphrase detec-\ntion (Socher et al., 2011) and word",
    "title": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation",
    "abstract": "In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
    "link": "https://www.semanticscholar.org/paper/0b544dfe355a5070b60986319a3f51fb45d1348e",
    "published": "2014-06-03"
  },
  "127": {
    "pdf_path": "data/pdfs/machine learning_paper_323.pdf",
    "text_excerpt": "Machine Learning at the Network Edge: A Survey\nM.G. SARWAR MURSHED, Clarkson University\nCHRISTOPHER MURPHY, SRC, Inc.\nDAQING HOU, Clarkson University\nNAZAR KHAN, Department of Computer Science, University of the Punjab, Pakistan\nGANESH ANANTHANARAYANAN, Microsoft Research\nFARAZ HUSSAIN, Clarkson University\nResource-constrained IoT devices, such as sensors and actuators, have become ubiquitous in recent years. This has led to the generation\nof large quantities of data in real-time, which is an appealing target for AI systems. However, deploying machine learning models\non such end-devices is nearly impossible. A typical solution involves offloading data to external computing systems (such as cloud\nservers) for further processing but this worsens latency, leads to increased communication costs, and adds to privacy concerns. To\naddress this issue, efforts have been made to place additional computing devices at the edge of the network, i.e close to the IoT devices\nwhere the data is generated. Deploying machine learning systems on such edge computing devices alleviates the above issues by\nallowing computations to be performed close to the data sources. This survey describes major research efforts where machine learning\nsystems have been deployed at the edge of computer networks, focusing on the operational aspects including compression techniques,\ntools, frameworks, and hardware used in successful applications of intelligent edge systems.\nAdditional Key Words and Phrases: edge intelligence, mobile edge computing, machine learning, resource-constrained, IoT, low-power,\ndeep learning, embedded, distributed computing.\n1 INTRODUCTION\nDue to the explosive growth of wireless communication technology, the number of Internet of Things (IoT) devices\nhas increased dramatically in recent years. It has been estimated that by 2020, more than 25 billion devices will have\nbeen connected to the Internet [ 94] and the potential economic impact of the IoT will be $3.9 trillion to $11.1 tri",
    "title": "Machine Learning at the Network Edge: A Survey",
    "abstract": "Resource-constrained IoT devices, such as sensors and actuators, have become ubiquitous in recent years. This has led to the generation of large quantities of data in real-time, which is an appealing target for AI systems. However, deploying machine learning models on such end-devices is nearly impossible. A typical solution involves offloading data to external computing systems (such as cloud servers) for further processing but this worsens latency, leads to increased communication costs, and adds to privacy concerns. To address this issue, efforts have been made to place additional computing devices at the edge of the network, i.e., close to the IoT devices where the data is generated. Deploying machine learning systems on such edge computing devices alleviates the above issues by allowing computations to be performed close to the data sources. This survey describes major research efforts where machine learning systems have been deployed at the edge of computer networks, focusing on the operational aspects including compression techniques, tools, frameworks, and hardware used in successful applications of intelligent edge systems.",
    "link": "https://www.semanticscholar.org/paper/3e9a40a567c4a95b591530ff5771296b478a0f0c",
    "published": "2019-07-31"
  },
  "128": {
    "pdf_path": "data/pdfs/machine learning_paper_493.pdf",
    "text_excerpt": "Machine Learning, 30, 127–132 (1998)\nc°1998 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\nGuest Editors’ Introduction:\nOn Applied Research in Machine Learning\nFOSTER PROVOST provost@acm.org\nBell Atlantic Science and Technology, 400 Westchester Avenue, White Plains, New York 10604\nRON KOHAVI ronnyk@sgi.com\nData Mining and Visualization, Silicon Graphics Inc., 2011 N. Shoreline Blvd, Mountain View, CA. 94043\nCommon arguments for including applications papers in the Machine Learning literature\nareoftenbasedonthepapers’valueforadvertisingsuccessstoriesandforboostingmorale.Forexample,high-proﬁleapplicationscanhelptosecurefundingforfutureresearchandcanhelptoattracthighcaliberstudents. However,thereisanotherreasonwhysuchpapersareofvaluetotheﬁeld,whichis,arguably,evenmorevital. Applicationpapersare essentialin\norder for Machine Learning to remain a viable science. They focus research on importantunsolved problems that currently restrict the practical applicability of machine learningmethods.\nMuchofthe“science”ofMachineLearningisascienceofengineering.\n1Bythiswemean\nthat it is dedicated to creating and compiling veriﬁable knowledge related to the designand construction of artifacts. The scientiﬁc knowledge comprises theoretical arguments,observationalcategorizations,empiricalstudies,andpracticaldemonstrations. Theartifactsare computer programs that use data to build models that are practically or theoreticallyuseful. Becausetheobjectsofstudyareintendedtohavepracticalutility,itisessentialforresearch activities to be focused (in part) on the elimination of obstacles that impede theirpractical application.\nMostoftentheseobstaclestaketheformofrestrictivesimplifyingassumptionscommonly\nmade in research. Consider as an example the assumption, common in classiﬁer learningresearch, that misclassiﬁcation errors have equal costs. The vast majority of classiﬁerlearningresearchinMachineLearninghasbeenconductedunderthisassumption,throughthe use of classiﬁcation accur",
    "title": "Guest Editors' Introduction: On Applied Research in Machine Learning",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1007442505281",
    "published": "2002-12-22T04:48:21Z"
  },
  "129": {
    "pdf_path": "data/pdfs/machine learning_paper_580.pdf",
    "text_excerpt": "Machin e Learning , 6, 251-27 6 (1991 )\n© 1991 Kluwe r Academi c Publishers , Boston . Manufacture d in The Netherlands .\nA Neares t Hyperrectangl e Learnin g Metho d\nSTEVE N SALZBER G (SALZBERG@CSJHU.EDU )\nDepartment  of Computer  Science,  Johns  Hopkins  University,  Baltimore,  MD 21218\nEditor : Tom Dietteric h\nAbstract . This pape r present s a theor y of learnin g calle d neste d generalize d exempla r (NGE ) theory , in whic h\nlearnin g is accomplishe d by storin g object s in Euclidea n n-space , E\", as hyperrectangles . The hyperrectangle s\nmay be neste d insid e one anothe r to arbitrar y depth . In contras t to generalizatio n processe s that replac e symboli c\nformula e by more genera l formulae , the NGE algorith m modifie s hyperrectangle s by growin g and reshapin g them\nin a well-define d fashion . The axes of these hyperrectangle s are define d by the variable s measure d for each exam -\nple. Each variabl e can have any rang e on the real line; thus the theor y is not restricte d to symboli c or binar y values .\nThis pape r describe s som e advantage s and disadvantage s of NGE theory , position s it as a form of exemplar -\nbased learning , and compare s it to othe r inductiv e learnin g theories . An implementatio n has been teste d in three\ndifferen t domains , for whic h result s are presente d below : predictio n of breas t cancer , classificatio n of iris flowers ,\nand predictio n of surviva l time s for hear t attac k patients . The result s in thes e domain s suppor t the claim that\nNGE theor y can be used to creat e compac t representation s with excellen t predictiv e accuracy .\nKeywords . Exemplar , induction , generalization , prediction , incrementa l learning , exception s\n1. Introductio n\nThis pape r present s a theor y of learnin g from example s calle d Neste d Generalize d Exempla r\n(NGE ) theory . NGE theor y is derive d from a learnin g mode l calle d exemplar-based  learn-\ning that was propose d originall y as a mode l of huma n ",
    "title": "A Nearest Hyperrectangle Learning Method",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022661727670",
    "published": "2003-04-04T16:55:36Z"
  },
  "130": {
    "pdf_path": "data/pdfs/machine learning_paper_29.pdf",
    "text_excerpt": "The 1th IEEE International Conference on Data Stream Mining  & Processing 23-27 August 2016, Lviv, Ukraine Linear, Machine Learning and Probabilistic Approaches for Time Series Analysis  B.M.Pavlyshenko  Ivan Franko National University of Lviv, e-mail: b.pavlyshenko@gmail.com   In this paper we study different approaches for time series modeling. The forecasting approaches using linear models, ARIMA alpgorithm, XGBoost machine learning algorithm are described. Results of different model combinations are shown. For probabilistic modeling the approaches using copulas and Bayesian inference are considered.   Keywords — time series; ARIMA; forecasting; predictive analytics. I. INTRODUCTION  Time series analysis, especially forecasting, is an important problem of modern predictive analytics. The goal of this study is to consider different aproaches for time series modeling.  For our analysis, we used stores sales historical data from Kaggle competition “Rossmann Store Sales” [1]. These data represent the sales time series of Rossmann stores. For time series forecasting such approaches as linear models and ARIMA algorithm are widely used. Machine lerning algorithm make it possible to find patterns in the time series. Sometimes we need to forecast not only more probable values of sales but also their distribution. Especially we need it in the risk analysis for assessing different risks related to sales dynamics. In this case, we need to take into account sales distributions and dependencies between sales time series features (e.g. day of week, month, average sales, etc.) and external factors such as promo, distance to competitors, etc. One can consider sales as a stochastic variable with some marginal distributions. If we have sales distribution, we can calculate value at risk (VaR) which is one of risk assessment features. In probabilistic analysis of sales, we can use copulas which allows us to  analyze  the dependence between sales and different factors.  To find distri",
    "title": "Linear, Machine Learning and Probabilistic Approaches for Time Series\n  Analysis",
    "abstract": "In this paper we study different approaches for time series modeling. The\nforecasting approaches using linear models, ARIMA alpgorithm, XGBoost machine\nlearning algorithm are described. Results of different model combinations are\nshown. For probabilistic modeling the approaches using copulas and Bayesian\ninference are considered.",
    "link": "http://arxiv.org/abs/1703.01977v1",
    "published": "2017-02-26T10:41:26Z"
  },
  "131": {
    "pdf_path": "data/pdfs/machine learning_paper_143.pdf",
    "text_excerpt": "Vol.:(0123456789)Artificial Intelligence Review (2023) 56:3005–3054\nhttps://doi.org/10.1007/s10462-022-10246-w\n1 3\nHuman‑in‑the‑loop machine learning: a state of the art\nEduardo Mosqueira‑Rey1  · Elena Hernández‑Pereira1 · David Alonso‑Ríos1 · \nJosé Bobes‑Bascarán1 · Ángel Fernández‑Leal1\nPublished online: 17 August 2022 \n© The Author(s) 2022\nAbstract\nResearchers are defining new types of interactions between humans and machine learn-\ning algorithms generically called human-in-the-loop machine learning. Depending on who \nis in control of the learning process, we can identify: active learning, in which the sys-\ntem remains in control; interactive machine learning, in which there is a closer interaction \nbetween users and learning systems; and machine teaching, where human domain experts \nhave control over the learning process. Aside from control, humans can also be involved \nin the learning process in other ways. In curriculum learning human domain experts try to \nimpose some structure on the examples presented to improve the learning; in explainable \nAI the focus is on the ability of the model to explain to humans why a given solution was \nchosen. This collaboration between AI models and humans should not be limited only to \nthe learning process; if we go further, we can see other terms that arise such as Usable and \nUseful AI. In this paper we review the state of the art of the techniques involved in the new \nforms of relationship between humans and ML algorithms. Our contribution is not merely \nlisting the different approaches, but to provide definitions clarifying confusing, varied and \nsometimes contradictory terms; to elucidate and determine the boundaries between the dif-\nferent methods; and to correlate all the techniques searching for the connections and influ-\nences between them.\nKeywords Human-in-the-loop machine learning · Active learning · Interactive machine \nlearning · Machine teaching · Curriculum learning · Explainable AI\n * Eduardo Mosqueira-Rey \n e",
    "title": "Human-in-the-loop machine learning: a state of the art",
    "abstract": "Researchers are defining new types of interactions between humans and machine learning algorithms generically called human-in-the-loop machine learning. Depending on who is in control of the learning process, we can identify: active learning, in which the system remains in control; interactive machine learning, in which there is a closer interaction between users and learning systems; and machine teaching, where human domain experts have control over the learning process. Aside from control, humans can also be involved in the learning process in other ways. In curriculum learning human domain experts try to impose some structure on the examples presented to improve the learning; in explainable AI the focus is on the ability of the model to explain to humans why a given solution was chosen. This collaboration between AI models and humans should not be limited only to the learning process; if we go further, we can see other terms that arise such as Usable and Useful AI. In this paper we review the state of the art of the techniques involved in the new forms of relationship between humans and ML algorithms. Our contribution is not merely listing the different approaches, but to provide definitions clarifying confusing, varied and sometimes contradictory terms; to elucidate and determine the boundaries between the different methods; and to correlate all the techniques searching for the connections and influences between them.",
    "link": "https://www.semanticscholar.org/paper/62cadbc4fcc73204a72847300cb2214f4401efad",
    "published": "2022-08-17"
  },
  "132": {
    "pdf_path": "data/pdfs/machine learning_paper_5.pdf",
    "text_excerpt": "TOWARDS MODULAR MACHINE LEARNING SOLUTION DEVELOPMENT :\nBENEFITS AND TRADE -OFFS\nSamiyuru Menik1Lakshmish Ramaswamy1\nABSTRACT\nMachine learning technologies have demonstrated immense capabilities in various domains. They play a key\nrole in the success of modern businesses. However, adoption of machine learning technologies has a lot of\nuntouched potential. Cost of developing custom machine learning solutions that solve unique business problems\nis a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic\nnature prevalent in today’s machine learning applications stands in the way of efﬁcient and cost effective\ncustomized machine learning solution development. In this work we explore the beneﬁts of modular machine\nlearning solutions and discuss how modular machine learning solutions can overcome some of the major solution\nengineering limitations of monolithic machine learning solutions. We analyze the trade-offs between modular and\nmonolithic machine learning solutions through three deep learning problems; one text based and the two image\nbased. Our experimental results show that modular machine learning solutions have a promising potential to reap\nthe solution engineering advantages of modularity while gaining performance and data advantages in a way the\nmonolithic machine learning solutions do not permit.\n1 I NTRODUCTION\nMachine learning (ML) has gained a lot of attention over the\npast years. Machine learning technologies have become a\npart of many organizational workﬂows and day to day tasks\nof individuals knowingly or unknowingly. Big tech compa-\nnies and academic entities are taking the lead in developing\ncutting edge ML technologies that push the boundaries of\nwhat ML can accomplish. Cutting edge computer vision and\nlanguage modeling technologies provide a good example\nfor this. Beyond the heightened attention, existing appli-\ncations and large scale organization and academic driven\ndevelopments, there is a lot o",
    "title": "Towards Modular Machine Learning Solution Development: Benefits and\n  Trade-offs",
    "abstract": "Machine learning technologies have demonstrated immense capabilities in\nvarious domains. They play a key role in the success of modern businesses.\nHowever, adoption of machine learning technologies has a lot of untouched\npotential. Cost of developing custom machine learning solutions that solve\nunique business problems is a major inhibitor to far-reaching adoption of\nmachine learning technologies. We recognize that the monolithic nature\nprevalent in today's machine learning applications stands in the way of\nefficient and cost effective customized machine learning solution development.\nIn this work we explore the benefits of modular machine learning solutions and\ndiscuss how modular machine learning solutions can overcome some of the major\nsolution engineering limitations of monolithic machine learning solutions. We\nanalyze the trade-offs between modular and monolithic machine learning\nsolutions through three deep learning problems; one text based and the two\nimage based. Our experimental results show that modular machine learning\nsolutions have a promising potential to reap the solution engineering\nadvantages of modularity while gaining performance and data advantages in a way\nthe monolithic machine learning solutions do not permit.",
    "link": "http://arxiv.org/abs/2301.09753v1",
    "published": "2023-01-23T22:54:34Z"
  },
  "133": {
    "pdf_path": "data/pdfs/machine learning_paper_312.pdf",
    "text_excerpt": "CAUSALITY FOR MACHINE LEARNING\nBernhard Schölkopf\nMax Planck Institute for Intelligent Systems, Max-Planck-Ring 4, 72076 Tübingen, Germany\nbs@tuebingen.mpg.de\nABSTRACT\nGraphical causal inference as pioneered by Judea Pearl arose from research on artiﬁcial intelligence\n(AI), and for a long time had little connection to the ﬁeld of machine learning. This article discusses\nwhere links have been and should be established, introducing key concepts along the way. It argues\nthat the hard open problems of machine learning and AI are intrinsically related to causality, and\nexplains how the ﬁeld is beginning to understand them.\n1 Introduction\nThe machine learning community’s interest in causality has signiﬁcantly increased in recent years. My understanding\nof causality has been shaped by Judea Pearl and a number of collaborators and colleagues, and much of it went into a\nbook written with Dominik Janzing and Jonas Peters (Peters et al., 2017). I have spoken about this topic on various\noccasions,1and some of it is in the process of entering the machine learning mainstream, in particular the view that\ncausal modeling can lead to more invariant or robust models. There is excitement about developments at the interface\nof causality and machine learning, and the present article tries to put my thoughts into writing and draw a bigger picture.\nI hope it may not only be useful by discussing the importance of causal thinking for AI, but it can also serve as an\nintroduction to some relevant concepts of graphical or structural causal models for a machine learning audience.\nIn spite of all recent successes, if we compare what machine learning can do to what animals accomplish, we observe\nthat the former is rather bad at some crucial feats where animals excel. This includes transfer to new problems, and any\nform of generalization that is not from one data point to the next one (sampled from the same distribution), but rather\nfrom one problem to the next one —– both have been termed general",
    "title": "Causality for Machine Learning",
    "abstract": "Graphical causal inference as pioneered by Judea Pearl arose from research on artificial intelligence (AI), and for a long time had little connection to the field of machine learning. \nThis article discusses where links have been and should be established, introducing key concepts along the way. It argues that the hard open problems of machine learning and AI are intrinsically related to causality, and explains how the field is beginning to understand them.",
    "link": "https://www.semanticscholar.org/paper/b5461f9c5d65e87561e00848921ee797902dae14",
    "published": "2019-11-24"
  },
  "134": {
    "pdf_path": "data/pdfs/machine learning_paper_171.pdf",
    "text_excerpt": "Machine learning and the physical sciences\nGiuseppe Carleo\nCenter for Computational Quantum Physics,\nFlatiron Institute,\n162 5th Avenue,\nNew York, NY 10010,\nUSA\u0003\nIgnacio Cirac\nMax-Planck-Institut fur Quantenoptik,\nHans-Kopfermann-Straße 1,\nD-85748 Garching,\nGermany\nKyle Cranmer\nCenter for Cosmology and Particle Physics,\nCenter of Data Science,\nNew York University, 726 Broadway,\nNew York, NY 10003,\nUSA\nLaurent Daudet\nLightOn, 2 rue de la Bourse, F-75002 Paris,\nFrance\nMaria Schuld\nUniversity of KwaZulu-Natal,\nDurban 4000, South Africa\nNational Institute for Theoretical Physics,\nKwaZulu-Natal, Durban 4000, South Africa,\nand Xanadu Quantum Computing,\n777 Bay Street, M5B 2H7 Toronto,\nCanada\nNaftali Tishby\nThe Hebrew University of Jerusalem,\nEdmond Safra Campus,\nJerusalem 91904,\nIsrael\nLeslie Vogt-Maranto\nDepartment of Chemistry,\nNew York University,\nNew York, NY 10003,\nUSA\nLenka Zdeborová\nInstitut de physique théorique,\nUniversité Paris Saclay, CNRS, CEA,\nF-91191 Gif-sur-Yvette,\nFrancey\nMachine learning encompasses a broad range of algorithms and modeling tools used for\na vast array of data processing tasks, which has entered most scientiﬁc disciplines in\nrecent years. We review in a selective way the recent research on the interface between\nmachine learning and physical sciences. This includes conceptual developments in ma-\nchine learning (ML) motivated by physical insights, applications of machine learning\ntechniques to several domains in physics, and cross-fertilization between the two ﬁelds.\nAfter giving basic notion of machine learning methods and principles, we describe ex-\namples of how statistical physics is used to understand methods in ML. We then movearXiv:1903.10563v2  [physics.comp-ph]  6 Dec 20192\nto describe applications of ML methods in particle physics and cosmology, quantum\nmany body physics, quantum computing, and chemical and material physics. We also\nhighlight research and development into novel computing architectures aimed at acceler-\nating ML. In ",
    "title": "Machine learning and the physical sciences",
    "abstract": "Machine learning (ML) encompasses a broad range of algorithms and modeling tools used for a vast array of data processing tasks, which has entered most scientific disciplines in recent years. This article reviews in a selective way the recent research on the interface between machine learning and the physical sciences. This includes conceptual developments in ML motivated by physical insights, applications of machine learning techniques to several domains in physics, and cross fertilization between the two fields. After giving a basic notion of machine learning methods and principles, examples are described of how statistical physics is used to understand methods in ML. This review then describes applications of ML methods in particle physics and cosmology, quantum many-body physics, quantum computing, and chemical and material physics. Research and development into novel computing architectures aimed at accelerating ML are also highlighted. Each of the sections describe recent successes as well as domain-specific methodology and challenges.",
    "link": "https://www.semanticscholar.org/paper/a9cbbef8f4426329d0687025b34287c35bdd8b38",
    "published": "2019-03-25"
  },
  "135": {
    "pdf_path": "data/pdfs/machine learning_paper_667.pdf",
    "text_excerpt": "Machine Learning, 12, 7-16 (1993)\n© 1993 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\nIntroduction: Cognitive Autonomy\nin Machine Discovery\nJAN M. ZYTKOW ZYTKOW@WISE.CS.TWSU.EDU\nDepartment of Computer Science, Wichita State University, Wichita, KS 67208\n1. How is discovery different from learning?\nThis special issue of Machine Learning on discovery raises the question: How is discovery\ndifferent from learning? In machine learning we typically distinguish between (1) learning\nas acquiring new knowledge in the form of concepts, taxonomies, regularities, and the like,\nand (2) learning as performance improvement and skill acquisition. \"Discovery\" applies\nto things that exist, such as the moons of Jupiter or the laws of nature, so in this section\nwe confront discovery with learning in the first sense, of acquisition of objective knowledge.\nIn the second section we discuss the distinction between discovery and invention. The third\nsection reviews the ways in which the articles in this issue contribute to the growing autonomy\nand integration of machine discoverers, and it mentions recent research in machine discovery\nnot represented in this issue.\n1.1. A discover must be autonomous\nThere is no difference between discovery and learning based on the type of knowledge to be\nacquired. Each piece of knowledge available to a learner must have been discovered earlier,\nand everything that has been discovered can become a subject of learning. The distinction\nbecomes clear when we focus on the source of knowledge. A learner depends upon a teacher\nor a knowledge source, while a discovery can be granted only if it has been made without\nhelp of anybody who already knows.\nLearning is easier than discovery, because a human teacher or any agent who knows a\ngiven piece of knowledge can guide the learner's cognitive process in many ways. Consider\nconcept learning from examples. The teacher selects a useful concept and prepares a col-\nlection of examples and counterex",
    "title": "Introduction: Cognitive Autonomy in Machine Discovery",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022874916090",
    "published": "2003-04-04T16:57:10Z"
  },
  "136": {
    "pdf_path": "data/pdfs/machine learning_paper_31.pdf",
    "text_excerpt": "Classifying medical notes into standard disease codes\nAmitabha Karmakar\nUniversity of California\nBerkeley, CA\namitkarmakar@berkeley.edu\nAbstract\nWe investigate the automatic classiﬁcation of\npatient discharge notes into standard disease\nlabels. We ﬁnd that Convolutional Neural Net-\nworks with Attention outperform previous al-\ngorithms used in this task, and suggest further\nareas for improvement.\n1 Introduction\nElectronic Health Records (EHRs) have grown signiﬁ-\ncantly over the years and now include an unprecedented\namount and variety of patient information, including\ndemographics, vital sign measurements, laboratory test\nresults, prescriptions, procedures performed, digitized\nnotes, imaging reports, mortality etc. They usually\ncontain both structured data (e.g. admission dates) as\nwell as unstructured data (e.g. notes written by doc-\ntors).\nProvided it can be processed, the information in\nthese records - especially the unstructured data - holds\nthe promise of new medical insights and improved\nmedical care, such as faster detection of epidemics,\nidentiﬁcation of symptoms, personalized treatment, or\na more detailed understanding of treatment outcomes.\nOne such gains is a more automated and accurate\nway to report diseases. Since 1967, the World Health\nOrganization (WHO) has developed an International\nClassiﬁcation of Diseases (ICD) to “monitor the inci-\ndence and prevalence of diseases, observe reimburse-\nments and resource allocation trends, and keep track\nof safety and quality guidelines”1. Currently this ICD\nlabeling is done manually by administrative personnel\nbased on deﬁnitions and is subject to interpretation and\nerrors2.\nIn this paper, we focus our efforts on the automatic\nlabeling of discharge notes from the MIMIC3Database\ninto ICD codes. This public database of EHRs contains\ndata points on about 41,000 patients from an intensive\ncare units between 2001 and 2012, including notes on\nclose ot 53,000 admissions. MIMIC has already been\n1http://www.who.int/classiﬁc",
    "title": "Classifying medical notes into standard disease codes using Machine\n  Learning",
    "abstract": "We investigate the automatic classification of patient discharge notes into\nstandard disease labels. We find that Convolutional Neural Networks with\nAttention outperform previous algorithms used in this task, and suggest further\nareas for improvement.",
    "link": "http://arxiv.org/abs/1802.00382v1",
    "published": "2018-02-01T16:46:00Z"
  },
  "137": {
    "pdf_path": "data/pdfs/Machine Learning for Clinical Predictive Analytics_paper_11.pdf",
    "text_excerpt": "Using Machine Learning to Develop Smart Reflex Testing Protocols  \nMatthew McDermott1,2+, Anand Dighe3,4,5, Peter Szolovits1, Yuan Luo6*, Jason Baron3,4* \n1. MIT Computer Science and Artificial Intelligence Lab, Boston, Massachusetts, USA.  \n2. Harvard Medical School, Department of Biomedical Informatics.  \n3. Department of Pathology, Massachusetts General Hospital, Boston, Massachusetts, USA.  \n4. Harvard Medical School, Boston, Massachusetts, USA.  \n5. MGB HealthCare System, Somerville, Massachusetts, USA.  \n6. Department  of Preventive Medicine, Northwestern University, Chicago, IL, USA.  \n*Drs. Baron and Luo are co-corresponding authors  \n+ Dr. McDermott was associated with the MIT affiliation during the time the majority of the work was \nperformed but had switched to the Harv ard Medical School affiliation by the time of manuscript revision \nand submission.   \n \nAddress reprint requests to : \nJason Baron, MD  \nMassachusetts General Hospital  \n55 Fruit Street  \nBoston, MA 02114  \n(314) 276 -0944  \njmbaron@partners.org  \n \nKey Words: Machine learning, Clinical decision support, Laboratory test ordering, Imputation, Missing \ndata, Ferritin, Computational Pathology  \n \nWord Count (main text): xx \n Tables:  4 \n \nFigures: 5 \nSupplemental Figures: 1  \n \n  ABSTRACT  \nObjective : Reflex testing protocols allow clinical laboratories to perform  second line diagnostic tests on       \nexisting specimens based on the results of initially o rdered tests.  Reflex testing can support optimal \nclinical laboratory test ordering and diagnosis.  In current clinical practice, reflex testing typically relies on \nsimple “if -then” rules; however, this limits their scope since most test ordering decisions  involve more      \ncomplexity than a simple rule will allow.  Here, using the analyte ferritin as an example , we propose an \nalternative machine learning -based approach to “smart” reflex testing with a wider scope and greater \nimpact than traditional rul e-based approaches.  \nMeth",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "138": {
    "pdf_path": "data/pdfs/machine learning_paper_365.pdf",
    "text_excerpt": "Machine learning phases of matter\nJuan Carrasquilla1and Roger G. Melko2, 1\n1Perimeter Institute for Theoretical Physics,\nWaterloo, Ontario N2L 2Y5, Canada\n2Department of Physics and Astronomy,\nUniversity of Waterloo, Ontario, N2L 3G1, Canada\nNeural networks can be used to identify phases and phase transitions in condensed\nmatter systems via supervised machine learning. Readily programmable through\nmodern software libraries, we show that a standard feed-forward neural network\ncan be trained to detect multiple types of order parameter directly from raw state\ncon\fgurations sampled with Monte Carlo. In addition, they can detect highly non-\ntrivial states such as Coulomb phases, and if modi\fed to a convolutional neural\nnetwork, topological phases with no conventional order parameter. We show that this\nclassi\fcation occurs within the neural network without knowledge of the Hamiltonian\nor even the general locality of interactions. These results demonstrate the power\nof machine learning as a basic research tool in the \feld of condensed matter and\nstatistical physics.arXiv:1605.01735v1  [cond-mat.str-el]  5 May 20162\nCondensed matter physics is the study of the collective behavior of massively complex\nassemblies of electrons, nuclei, magnetic moments, atoms or qubits [1]. This complexity is\nre\rected in the size of the classical or quantum state space, which grows exponentially with\nthe number of particles. This exponential growth is reminiscent of the \\curse of dimension-\nality\" commonly encountered in machine learning. That is, a target function to be learned\nrequires an amount of training data that grows exponentially in the dimension (e.g. the\nnumber of image features). Despite this curse, the machine learning community has devel-\noped a number of techniques with remarkable abilities to recognize, classify, and characterize\ncomplex sets of data. In light of this success, it is natural to ask whether such techniques\ncould be applied to the arena of condensed-matter physics",
    "title": "Machine learning phases of matter",
    "abstract": "The success of machine learning techniques in handling big data sets proves ideal for classifying condensed-matter phases and phase transitions. The technique is even amenable to detecting non-trivial states lacking in conventional order. Condensed-matter physics is the study of the collective behaviour of infinitely complex assemblies of electrons, nuclei, magnetic moments, atoms or qubits1. This complexity is reflected in the size of the state space, which grows exponentially with the number of particles, reminiscent of the ‘curse of dimensionality’ commonly encountered in machine learning2. Despite this curse, the machine learning community has developed techniques with remarkable abilities to recognize, classify, and characterize complex sets of data. Here, we show that modern machine learning architectures, such as fully connected and convolutional neural networks3, can identify phases and phase transitions in a variety of condensed-matter Hamiltonians. Readily programmable through modern software libraries4,5, neural networks can be trained to detect multiple types of order parameter, as well as highly non-trivial states with no conventional order, directly from raw state configurations sampled with Monte Carlo6,7.",
    "link": "https://www.semanticscholar.org/paper/dd308eb0d7be24e593fe355476057fc37ab5bf0e",
    "published": "2016-03-17"
  },
  "139": {
    "pdf_path": "data/pdfs/machine learning_paper_57.pdf",
    "text_excerpt": "Radiological images and machine learning: trends, perspectives,\nand prospects\nZhenwei Zhang, Ervin Sejdi\u0013 c∗\nAbstract\nThe application of machine learning to radiological images is an increasingly active research area\nthat is expected to grow in the next \fve to ten years. Recent advances in machine learning have\nthe potential to recognize and classify complex patterns from di\u000berent radiological imaging modalities\nsuch as x-rays, computed tomography, magnetic resonance imaging and positron emission tomography\nimaging. In many applications, machine learning based systems have shown comparable performance to\nhuman decision-making. The applications of machine learning are the key ingredients of future clinical\ndecision making and monitoring systems. This review covers the fundamental concepts behind various\nmachine learning techniques and their applications in several radiological imaging areas, such as medical\nimage segmentation, brain function studies and neurological disease diagnosis, as well as computer-\naided systems, image registration, and content-based image retrieval systems. Synchronistically, we will\nbrie\ry discuss current challenges and future directions regarding the application of machine learning in\nradiological imaging. By giving insight on how take advantage of machine learning powered applications,\nwe expect that clinicians can prevent and diagnose diseases more accurately and e\u000eciently.\nKeywords : deep learning, machine learning, imaging modalities, deep neural network\n1 Introduction\nRadiology is a branch of medicine that uses imaging techniques to detect, diagnose and treat diseases [1, 2, 3].\nDiagnostic radiology helps radiologists image internal body structures to diagnose the cause of symptoms,\nscreen for illnesses and detect the body's response to treatments. The most common radiology modalities\ninclude: plain X-ray, computed tomography (CT), magnetic resonance imaging (MRI), positron emission\ntomography (PET), and ultrasound imaging. Fig. 1 show",
    "title": "Radiological images and machine learning: trends, perspectives, and\n  prospects",
    "abstract": "The application of machine learning to radiological images is an increasingly\nactive research area that is expected to grow in the next five to ten years.\nRecent advances in machine learning have the potential to recognize and\nclassify complex patterns from different radiological imaging modalities such\nas x-rays, computed tomography, magnetic resonance imaging and positron\nemission tomography imaging. In many applications, machine learning based\nsystems have shown comparable performance to human decision-making. The\napplications of machine learning are the key ingredients of future clinical\ndecision making and monitoring systems. This review covers the fundamental\nconcepts behind various machine learning techniques and their applications in\nseveral radiological imaging areas, such as medical image segmentation, brain\nfunction studies and neurological disease diagnosis, as well as computer-aided\nsystems, image registration, and content-based image retrieval systems.\nSynchronistically, we will briefly discuss current challenges and future\ndirections regarding the application of machine learning in radiological\nimaging. By giving insight on how take advantage of machine learning powered\napplications, we expect that clinicians can prevent and diagnose diseases more\naccurately and efficiently.",
    "link": "http://arxiv.org/abs/1903.11726v1",
    "published": "2019-03-27T23:11:15Z"
  },
  "140": {
    "pdf_path": "data/pdfs/machine learning_paper_10.pdf",
    "text_excerpt": "arXiv:1911.06612v1  [cs.LG]  12 Nov 2019Position Paper: Towards Transparent Machine\nLearning\nDustin Juliano\nNov. 1, 2019\nAbstract\nTransparent machine learning is introduced as an alternati ve form of\nmachine learning, where both the model and the learning syst em are rep-\nresented in source code form. The goal of this project is to en able direct\nhuman understanding of machine learning models, giving us t he ability to\nlearn, verify, and reﬁne them as programs. If solved, this te chnology could\nrepresent a best-case scenario for the safety and security o f AI systems\ngoing forward.\n1 Introduction\nCurrent machine learning (ML) systems produce models that a re diﬃcult or\nimpossible to understand. This poses clear challenges with security, safety, and\nbias in these deployments. Opaque models also make it diﬃcul t to gain insight\ninto the automated decision-making process.\nAs a result of this, interpretable or explainable ML have bec ome active areas\nof research [ 1,2], with a notable interest from DARPA [ 3]. However, those\napproaches are focused on analyzing models that are inheren tly resistant to\nhuman understanding due to the way in which they are represen ted.\nTransparent machine learning (TML) is intended to solve the se problems by\nproducing models and data that we can understand. It would do this by repre-\nsenting and modifying source code representations. This, i n turn, would result\nin a potentially self-contained executable that could be de ployed directly.\nIn addition to the source code model, the TML system itself ma y be embedded\ninto the output program. This would give it the ability to con tinuously update\nitself. Embedding, however, is not a requirement; the learn er may be suppressed\nso that it is not emitted with the ﬁnal program. This could be f or safety purposes\nor to ensure stability of the deployment.\nA complex program may require auxiliary information, such a s labels, lookup\ntables, or a database of some kind. Transparent machine lear ning sys",
    "title": "Position Paper: Towards Transparent Machine Learning",
    "abstract": "Transparent machine learning is introduced as an alternative form of machine\nlearning, where both the model and the learning system are represented in\nsource code form. The goal of this project is to enable direct human\nunderstanding of machine learning models, giving us the ability to learn,\nverify, and refine them as programs. If solved, this technology could represent\na best-case scenario for the safety and security of AI systems going forward.",
    "link": "http://arxiv.org/abs/1911.06612v1",
    "published": "2019-11-12T10:49:55Z"
  },
  "141": {
    "pdf_path": "data/pdfs/machine learning_paper_418.pdf",
    "text_excerpt": "Machine Learning, 19, 29-43 (1995)\n© 1995 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\nMonotonicity Maintenance in Information-Theoretic\nMachine Learning Algorithms\nARIE BEN-DAVID msariebd@pluto.mscc.huji.ac.il\nManagement Information Systems, School of Business Administration, The Hebrew University of Jerusalem, Mount\nScopus, Jerusalem 91905, Israel\nEditor: Paul Utgoff\nAbstract. Decision trees that are based on information-theory are useful paradigms for learning from exam-\nples. However, in some real-world applications, known information-theoretic methods frequently generate non-\nmonotonic decision trees, in which objects with better attribute values are sometimes classified to lower classes\nthan objects with inferior values. This property is undesirable for problem solving in many application domains,\nsuch as credit scoring and insurance premium determination, where monotonicity of subsequent classifications\nis important. An attribute-selection metric is proposed here that takes both the error as well as monotonicity\ninto account while building decision trees. The metric is empirically shown capable of significantly reducing the\ndegree of non-monotonicity of decision trees without sacrificing their inductive accuracy.\nKeywords: information theory, monotonic decision trees, consistency, accuracy, monotonic classification\nproblems\n1 Introduction\nSuppose a college admissions committee decides to use decision trees to determine whom\nto admit based on standardized test scores and grades. For reasons such as fairness and\nliability, the college would not want to use a decision tree that admits an applicant with\ncertain scores, and then rejects another who scores as high or higher on each measure.\nSimilarly, a life insurance company would not wish to rely on a decision tree that quotes a\nyoung and healthy applicant a higher premium rate than one that has been quoted to an old\nand unhealthy person.\nThe classifications in both the school admissions a",
    "title": "Monotonicity Maintenance in Information-Theoretic Machine Learning Algorithms",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022655006810",
    "published": "2003-04-04T16:55:36Z"
  },
  "142": {
    "pdf_path": "data/pdfs/Machine Learning for Clinical Predictive Analytics_paper_1.pdf",
    "text_excerpt": "Machine Learning for Clinical Predictive Analytics\nWei-Hung Weng1\nLearning Objectives\n\u000fUnderstand the basics of machine learning\ntechniques and the reasons behind why they\nare useful for solving clinical prediction\nproblems.\n\u000fUnderstand the intuition behind some ma-\nchine learning models, including regression,\ndecision trees, and support vector machines.\n\u000fUnderstand how to apply these models to\nclinical prediction problems using publicly\navailable datasets via case studies.\n1. Machine Learning for Healthcare\n1.1. Introduction\nIn this chapter, we provide a brief overview of applying ma-\nchine learning techniques for clinical prediction tasks. We\nbegin with a quick introduction to the concepts of machine\nlearning, and outline some of the most common machine\nlearning algorithms. Next, we demonstrate how to apply\nthe algorithms with appropriate toolkits to conduct machine\nlearning experiments for clinical prediction tasks.\nThis chapter is composed of ﬁve sections. First, we will\nexplain why machine learning techniques are helpful for\nresearchers in solving clinical prediction problems (section\n1). Understanding the motivations behind machine learn-\ning approaches in healthcare are essential, since precision\nand accuracy are often critical in healthcare problems, and\neverything from diagnostic decisions to predictive clinical\nanalytics could dramatically beneﬁt from data-based pro-\ncesses with improved efﬁciency and reliability. In the sec-\nond section, we will introduce several important concepts in\nmachine learning in a colloquial manner, such as learning\nscenarios, objective/target function, error and loss function\nand metrics, optimization and model validation, and ﬁnally\na summary of model selection methods (section 2). These\ntopics will help us utilize machine learning algorithms in\nan appropriate way. Following that, we will introduce some\n1MIT CSAIL, Cambridge, MA, USA. Correspondence to: Wei-\nHung Weng <ckbjimmy@mit.edu >.popular machine learning algorithms for ",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "143": {
    "pdf_path": "data/pdfs/machine learning_paper_665.pdf",
    "text_excerpt": "Machin e Learning , 5, 117-12 0 (1990 )\n© 1990 Kluwe r Academi c Publishers , Boston . Manufacture d in The Netherlands .\nIntroduction : Specia l Issu e on Computationa l\nLearnin g Theor y\nThis specia l issu e of Machine  Learning  provide s a smal l samplin g of som e of the curren t\nresearc h in computationa l learnin g theory . The paper s wer e selecte d from amon g thos e\nthat appeare d in preliminar y form in the Proceeding s of the 1989 Worksho p on Computa -\ntiona l Learnin g Theor y (COL T '89), and were presente d at the worksho p in Sant a Cruz ,\nCA, in Augus t of 1989 .\nThe annua l COL T workshop s are rathe r broa d in scope , and encompas s a wide variet y\nof learnin g model s and protocols , learnin g tasks , and technique s for the analysi s of learnin g\nalgorithms . Perhap s the only commo n trait that the paper s from COL T '89 shar e is that\nthey all contai n forma l analyse s of theoretica l issue s in machin e learning . Mos t (althoug h\nnot all) of the COL T paper s focu s on the genera l proble m of concept  learning,  and it is\nthis area that has been evolvin g mos t rapidl y in the last few years . Muc h of the reaso n\nfor this progres s is the developmen t of variou s formalization s of the concep t learnin g proble m\nthat captur e man y of the characteristic s of wha t concep t learnin g mean s intuitively , whil e\nat the sam e time allow for successfu l applicatio n of rigorou s mathematica l technique s to\na variet y of specifi c learnin g problems . A main goal of computationa l learnin g theor y is\nthe suitabl e formalizatio n of othe r importan t aspect s of machin e learning . Hopefully , such\nformalization s will spaw n the quantit y and qualit y of theoreticall y oriente d result s that we\nhave witnesse d for concep t learning .\nThree basic forma l model s of concep t learnin g have emerge d as the most dominant . Thes e\nlearnin g protocol s are certainl y not the only ones that are relevan t to the investigation s\nappearin g in th",
    "title": "Introduction: Special Issue on Computational Learning Theory",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022615931710",
    "published": "2003-04-04T16:55:36Z"
  },
  "144": {
    "pdf_path": "data/pdfs/machine learning_paper_40.pdf",
    "text_excerpt": "Dex: Incremental Learning for Complex\nEnvironments in Deep Reinforcement Learning\nNick Erickson, Qi Zhao\nDepartment of Science and Engineering\nUniversity of Minnesota, Twin Cities\nMinneapolis, MN 55455\neric3068@umn.edu, qzhao@cs.umn.edu\nAbstract\nThis paper introduces Dex, a reinforcement learning environment toolkit special-\nized for training and evaluation of continual learning methods as well as general\nreinforcement learning problems. We also present the novel continual learning\nmethod of incremental learning, where a challenging environment is solved using\noptimal weight initialization learned from ﬁrst solving a similar easier environment.\nWe show that incremental learning can produce vastly superior results than standard\nmethods by providing a strong baseline method across ten Dexenvironments. We\nﬁnally develop a saliency method for qualitative analysis of reinforcement learning,\nwhich shows the impact incremental learning has on network attention.\n1 Introduction\nComplex environments such as Go,Starcraft , and many modern video-games present profound\nchallenges in deep reinforcement learning that have yet to be solved. They often require long, precise\nsequences of actions and domain knowledge in order to obtain reward, and have yet to be learned from\nrandom weight initialization. Solutions to these problems would mark a signiﬁcant breakthrough on\nthe path to artiﬁcial general intelligence.\nRecent works in reinforcement learning have shown that environments such as Atari games [ 2]\ncan be learned from pixel input to superhuman expertise [ 9]. The agents start with randomly\ninitialized weights, and learn largely from trial and error, relying on a reward signal to indicate\nperformance. Despite these successes, complex games, including those where rewards are sparse such\nasMontezuma’s Revenge , have been notoriously difﬁcult to learn. While methods such as intrinsic\nmotivation [ 3] have been used to partially overcome these challenges, we suspect this becomes\nintr",
    "title": "Dex: Incremental Learning for Complex Environments in Deep Reinforcement\n  Learning",
    "abstract": "This paper introduces Dex, a reinforcement learning environment toolkit\nspecialized for training and evaluation of continual learning methods as well\nas general reinforcement learning problems. We also present the novel continual\nlearning method of incremental learning, where a challenging environment is\nsolved using optimal weight initialization learned from first solving a similar\neasier environment. We show that incremental learning can produce vastly\nsuperior results than standard methods by providing a strong baseline method\nacross ten Dex environments. We finally develop a saliency method for\nqualitative analysis of reinforcement learning, which shows the impact\nincremental learning has on network attention.",
    "link": "http://arxiv.org/abs/1706.05749v1",
    "published": "2017-06-19T00:16:24Z"
  },
  "145": {
    "pdf_path": "data/pdfs/machine learning_paper_74.pdf",
    "text_excerpt": "Practical Attacks on Machine\nLearning: A Case Study on\nAdversarial Windows Malware\nLuca Demetrio\nUniversity of Cagliari, Italy, and Pluribus One\nBattista Biggio\nUniversity of Cagliari, Italy, and Pluribus One\nFabio Roli\nUniversity of Genova, Italy, and Pluribus One\nAbstract —While machine learning is vulnerable to adversarial examples, it still lacks systematic\nprocedures and tools for evaluating its security in different application contexts. In this article,\nwe discuss how to develop automated and scalable security evaluations of machine learning\nusing practical attacks, reporting a use case on Windows malware detection.\nIndex Terms: Machine learning, Invasive software\nINTRODUCTION\nMachine learning has recorded unprecedented\nsuccess in many applications, including computer\nvision and speech recognition. Even in the cyber-\nsecurity domain, many companies have recently\nbuilt machine learning models within their de-\ntection pipelines to improve their anti-malware\nsolutions [8]. However, it is now widely known\nthat machine learning models can be easily misled\nby carefully-crafted attacks, such as training data\npoisoning, backdooring, evasion, model stealing,\nand other privacy-related threats [3]. While many\nof these attacks can be successfully prevented,\nmachine learning models remain extremely vul-\nnerable to adversarial examples [2], [15], that are\ninputs presented at test time speciﬁcally designed\nto cause the model to make a mistake. Adversarial\nexamples are normally found by optimizing a\nperturbation against the target model either viagradient-based optimization, when white-box ac-\ncess to the model is given (the kind of model\nand its trained parameters are accessible), or via\ngradient-free optimizers, when only black-box\naccess to the model is provided (for instance,\nthe model can be queried using different inputs,\nand feedback on the corresponding predictions\nis observable). In the black-box setting, it is\nalso possible to stage transfer attacks , which\nare gra",
    "title": "Practical Attacks on Machine Learning: A Case Study on Adversarial\n  Windows Malware",
    "abstract": "While machine learning is vulnerable to adversarial examples, it still lacks\nsystematic procedures and tools for evaluating its security in different\napplication contexts. In this article, we discuss how to develop automated and\nscalable security evaluations of machine learning using practical attacks,\nreporting a use case on Windows malware detection.",
    "link": "http://arxiv.org/abs/2207.05548v1",
    "published": "2022-07-12T14:17:58Z"
  },
  "146": {
    "pdf_path": "data/pdfs/machine learning_paper_125.pdf",
    "text_excerpt": "Machine Learning for High-Speed\nCorner Detection\nEdward Rosten and Tom Drummond\nDepartment of Engineering,\nCambridge University, UK\n{er258, twd20 }@cam.ac.uk\nAbstract. Where feature points are used in real-time frame-rate appli-\ncations, a high-speed feature detector is necessary. Feature detectors such\nas SIFT (DoG), Harris and SUSAN are good methods which yield high\nquality features, however they are too computationally intensive for use\nin real-time applications of any complexity. Here we show that machine\nlearning can be used to derive a feature detector which can fully process\nlive PAL video using less than 7% of the available processing time. By\ncomparison neither the Harris detector (120%) nor the detection stage\nof SIFT (300%) can operate at full frame rate.\nClearly a high-speed detector is of limited use if the features produced\nare unsuitable for downstream processing. In particular, the same scene\nviewed from two diﬀerent positions should yield features which corre-\nspond to the same real-world 3D locations[1]. Hence the second contri-\nbution of this paper is a comparison corner detectors based on this crite-\nrion applied to 3D scenes. This comparison supports a number of claims\nmade elsewhere concerning existing corner detectors. Further, contrary\nto our initial expectations, we show that despite being principally con-\nstructed for speed, our detector signiﬁ cantly outperforms existing feature\ndetectors according to this criterion.\n1 Introduction\nCorner detection is used as the ﬁrst step of many vision tasks such as tracking,\nSLAM (simultaneous localisation and mapping), localisation, image matching\nand recognition. Hence, a large number of corner detectors e xist in the litera-\nture. With so many already available i t may appear unnecessary to present yet\nanother detector to the community; however, we have a strong interest in real-\ntime frame rate applications such as SLAM in which computational resources\nare at a premium. In particular, it is still tr",
    "title": "Machine Learning for High-Speed Corner Detection",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/e0408181bccb7e3754dd5e6785ec47d8beb8b6bd",
    "published": "2006-05-07"
  },
  "147": {
    "pdf_path": "data/pdfs/machine learning_paper_534.pdf",
    "text_excerpt": "Machine Learning 35, 57–90 (1999)\nc°1999 Kluwer Academic Publishers. Manufactured in The Netherlands.\nLearning to Take Actions*\nRONI KHARDON†roni@dcs.ed.ac.uk\nDivision of Informatics, University of Edinburgh, JCMB, King’s Buildings, Edinburgh EH9 3JZ, Scotland\nEditor:J. Shavlik\nAbstract. We formalize a model for supervised learning of action strategies in dynamic stochastic domains\nand show that PAC-learning results on Occam algorithms hold in this model as well. We then identify a class ofrule-based action strategies for which polynomial time learning is possible. The representation of strategies is ageneralization of decision lists; strategies include rules with existentially quantiﬁed conditions, simple recursivepredicates,andsmallinternalstate,butaresyntacticallyrestricted. Wealsostudythelearnabilityofhierarchicallycomposed strategies where a subroutine already acquired can be used as a basic action in a higher level strategy.Weprovesomepositiveresultsinthissetting,butalsoshowthatinsomecasesthehierarchicallearningproblemis computationally hard.\nKeywords: learning to act, stochastic domains, supervised learning, rule based systems, hierarchical learning,\nNP-complete\n1. Introduction\nWe formalize a model for supervised learning of action strategies in dynamic stochastic\ndomains,andstudythelearnabilityofstrategiesrepresentedbyrule-basedsystems.Inthismodel,thelearnerisgivenaccesstotracesofbehaviorofanotheragentandusingthesetracesit tries to reconstruct a strategy for behaving successfully in the same world. Followingprevious work on learning to reason (Khardon & Roth, 1995, 1997) the formalizationutilizes two general ideas. First, one can gain insights by focusing on learning that isdone for the purpose of performing well in a particular task. Second, when couplinglearning with the task, the competence required of the agent can be deﬁned relative to itslearning interface. This allows for relaxed deﬁnitions describing plausible scenarios thatadmit efﬁcient solutions.\n",
    "title": "Learning to Take Actions",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1007571119753",
    "published": "2002-12-22T05:04:10Z"
  },
  "148": {
    "pdf_path": "data/pdfs/machine learning_paper_454.pdf",
    "text_excerpt": "Machine Learning, 37, 115–130 (1999)\nc°1999 Kluwer Academic Publishers. Manufactured in The Netherlands.\nProjection Learning⁄\nLESLIE G. VALIANT valiant@deas.harvard.edu\nDivision of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138, USA\nEditor:Lisa Hellerstein\nAbstract. A method of combining learning algorithms is described that preserves attribute-efﬁciency. It yields\nlearningalgorithmsthatrequireanumberofexamplesthatispolynomialinthenumberofrelevantvariablesandlogarithmicinthenumberofirrelevantones. Thealgorithmsaresimpletoimplementandrealizableonnetworkswith a number of nodes linear in the total number of variables. They include generalizations of Littlestone’sWinnowalgorithm,andare,therefore,goodcandidatesforexperimentationondomainshavingverylargenumbersof attributes but where nonlinear hypotheses are sought.\nKeywords: computational learning, attribute-efﬁcient learning, irrelevant attributes, Winnow algorithm\n1. Introduction\nOneareaofstarkcontrastbetweencurrentmachinelearningpracticeandnaturalbiological\nlearning is that of data preparation. In machine learning it is often found that in order toensuresuccessatanewlearningtaskconsiderableefforthastobeputintocreatingtherightsetofvariables,andintoeliminatingredundantonesiftherearelargenumbersofthese. Inbiological learning, on the other hand, no explicit methods for achieving these ends havebeen identiﬁed. The learning process appears to have mechanisms for overcoming theseproblems implicitly.\nThisdichotomysuggeststheexistenceofausefulclassoflearningalgorithmsthathave\nyettobediscoveredandexploited. Indeedithasbeensuggestedthatalgorithmsthatﬁllthisgaparefundamentaltosystemsthatlearnandmaintainlargeknowledgebasesforcognitivecomputations (Valiant, 1998).\nA simple but persuasive formulation of the problem at hand is the following: one needs\nalgorithmsthatlearn,asefﬁcientlyaspossible,functionsthatdependonasmallnumber k\namongamuchlargernumber nofavailablevariables.Thenecessityforthisissuggestedby\nth",
    "title": "Projection Learning",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1007678005361",
    "published": "2002-12-22T05:54:50Z"
  },
  "149": {
    "pdf_path": "data/pdfs/machine learning_paper_555.pdf",
    "text_excerpt": "Machin e Learnin g 2: 229 246 , 1987\n© 198 7 Kluwe r Academi c Publishers , Bosto n - Manufacture d in The Netherland s\nLearnin g Decisio n List s\nRONAL D L. RIVES T (RIVEST@THEORY.LCS.MIT.EDU )\nLaboratory  for Computer  Science,  Massachusetts  Institute  of Technology,\nCambridge,  Massachusetts  02139,  U.S.A.\n(Received : Decembe r 29, 1986 )\n(Revised : Augus t 5, 1987 )\nKeywords : Learnin g from examples , decisio n lists , Boolea n formulae , polynomial-tim e\nidentification .\nAbstract . Thi s pape r introduce s a new representatio n for Boolea n functions , calle d\ndecision  lists,  and show s that they are efficientl y learnabl e from examples . Mor e precisely ,\nthis resul t is establishe d for k-D L the set of decisio n lists with conjunctiv e clause s of\nsize k at each decision . Sinc e k-DL  properl y include s othe r well-know n technique s for\nrepresentin g Boolea n function s suc h as k-CN F (formula e in conjunctiv e norma l form\nwith at mos t k literal s per clause) , k-DN F (formula e in disjunctiv e norma l form with\nat mos t k literal s per term) , and decisio n tree s of dept h k, our resul t strictl y increase s\nthe set of function s that are know n to be polynomiall y learnable , in the sens e of Valian t\n(1984) . Our proo f is constructive : we presen t an algorith m that can efficientl y construc t\nan elemen t of k-D L consisten t with a give n set of examples , if one exists .\n1. Introductio n\nOne goal of researc h in machin e learnin g is to identif y the larges t possibl e\nclass of concept s that are learnabl e from examples . In this pape r we restric t\nour attentio n to concept s that can be define d in term s of a give n set of n\nBoolea n attributes ; each assignmen t of true or false to the attribute s can\nbe classifie d as eithe r a positive  or a negative  instanc e of the concep t to be\nlearned .\nTo mak e precis e the notio n of \"learnabl e from examples, \" we adop t the\ndefinitio n of polynomial  learnability  pioneere d by Valian t ",
    "title": "Learning Decision Lists",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022607331053",
    "published": "2003-04-04T16:55:36Z"
  },
  "150": {
    "pdf_path": "data/pdfs/machine learning_paper_13.pdf",
    "text_excerpt": "Noname manuscript No.\n(will be inserted by the editor)\nData Pricing in Machine Learning Pipelines\nZicun Cong \u0001Xuan Luo \u0001Pei Jian \u0001\nFeida Zhu \u0001Yong Zhang\nReceived: date / Accepted: date\nAbstract Machine learning is disruptive. At the same time, machine learning\ncan only succeed by collaboration among many parties in multiple steps natu-\nrally as pipelines in an eco-system, such as collecting data for possible machine\nlearning applications, collaboratively training models by multiple parties and\ndelivering machine learning services to end users. Data is critical and pene-\ntrating in the whole machine learning pipelines. As machine learning pipelines\ninvolve many parties and, in order to be successful, have to form a construc-\ntive and dynamic eco-system, marketplaces and data pricing are fundamental\nin connecting and facilitating those many parties. In this article, we survey\nthe principles and the latest research development of data pricing in machine\nlearning pipelines. We start with a brief review of data marketplaces and pric-\ning desiderata. Then, we focus on pricing in three important steps in machine\nlearning pipelines. To understand pricing in the step of training data collection,\nwe review pricing raw data sets and data labels. We also investigate pricing\nin the step of collaborative training of machine learning models, and overview\nZicun Cong\nSimon Fraser University, Burnaby, Canada\nE-mail: zicun cong@cs.sfu.ca\nXuan Luo\nSimon Fraser University, Burnaby, Canada\nE-mail: xuan luo@cs.sfu.ca\nJian Pei\nSimon Fraser University, Burnaby, Canada\nE-mail: jpei@cs.sfu.ca\nFeida Zhu\nSingapore Management University, Singapore\nE-mail: fdzhu@smu.edu.sg\nYong Zhang\nHuawei Technologies Canada, Burnaby, Canada\nE-mail: yong.zhang3@huawei.comarXiv:2108.07915v1  [cs.LG]  18 Aug 20212 Zicun Cong et al.\npricing machine learning models for end users in the step of machine learning\ndeployment. We also discuss a series of possible future directions.\nKeywords Data Assets\u0001Data Pricing\u0001Data",
    "title": "Data Pricing in Machine Learning Pipelines",
    "abstract": "Machine learning is disruptive. At the same time, machine learning can only\nsucceed by collaboration among many parties in multiple steps naturally as\npipelines in an eco-system, such as collecting data for possible machine\nlearning applications, collaboratively training models by multiple parties and\ndelivering machine learning services to end users. Data is critical and\npenetrating in the whole machine learning pipelines. As machine learning\npipelines involve many parties and, in order to be successful, have to form a\nconstructive and dynamic eco-system, marketplaces and data pricing are\nfundamental in connecting and facilitating those many parties. In this article,\nwe survey the principles and the latest research development of data pricing in\nmachine learning pipelines. We start with a brief review of data marketplaces\nand pricing desiderata. Then, we focus on pricing in three important steps in\nmachine learning pipelines. To understand pricing in the step of training data\ncollection, we review pricing raw data sets and data labels. We also\ninvestigate pricing in the step of collaborative training of machine learning\nmodels, and overview pricing machine learning models for end users in the step\nof machine learning deployment. We also discuss a series of possible future\ndirections.",
    "link": "http://arxiv.org/abs/2108.07915v1",
    "published": "2021-08-18T00:57:06Z"
  },
  "151": {
    "pdf_path": "data/pdfs/machine learning_paper_351.pdf",
    "text_excerpt": "Machine Learning, 50, 5–43, 2003\nc/circlecopyrt2003 Kluwer Academic Publishers. Manufactured in The Netherlands.\nAn Introduction to MCMC for Machine Learning\nCHRISTOPHE ANDRIEU C.Andrieu@bristol.ac.uk\nDepartment of Mathematics, Statistics Group, University of Bristol, University Walk, Bristol BS8 1TW, UK\nNANDO DE FREITAS nando@cs.ubc.ca\nDepartment of Computer Science, University of British Columbia, 2366 Main Mall, Vancouver,BC V6T 1Z4, Canada\nARNAUD DOUCET doucet@ee.mu.oz.au\nDepartment of Electrical and Electronic Engineering, University of Melbourne, Parkville, Victoria 3052, Australia\nMICHAEL I. JORDAN jordan@cs.berkeley.edu\nDepartments of Computer Science and Statistics, University of California at Berkeley, 387 Soda Hall, Berkeley,CA 94720-1776, USA\nAbstract. This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with\nemphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chainMonte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly,it discusses new interesting research horizons.\nKeywords: Markov chain Monte Carlo, MCMC, sampling, stochastic algorithms\n1. Introduction\nA recent survey places the Metropolis algorithm among the ten algorithms that have had the\ngreatest inﬂuence on the development and practice of science and engineering in the 20thcentury (Beichl & Sullivan, 2000). This algorithm is an instance of a large class of samplingalgorithms, known as Markov chain Monte Carlo (MCMC). These algorithms have playeda signiﬁcant role in statistics, econometrics, physics and computing science over the lasttwo decades. There are several high-dimensional problems, such as computing the volumeof a convex body in ddimensions, for which MCMC simulation is the only known general\napproach for providing a solution within a reasonable time (polynomial in d) (Dyer, Frieze,\n& Kannan, 1991; Jerrum & Sinclair, 1996).\nWhile conval",
    "title": "An Introduction to MCMC for Machine Learning",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/7c8858eba8571d86abd90252e734a11e3a6dd73f",
    "published": null
  },
  "152": {
    "pdf_path": "data/pdfs/machine learning_paper_587.pdf",
    "text_excerpt": " \n \n \n*Corresponding author. Email:  Hadeel.mohammed@uoanbar.edu.iq  \n                      \n \n \n \nResea rch Article  \nDiagnosis of HIV ( AIDS) by  Using deep learning and machine learning  \nHadeel M Saleh 1,*,, Abdulrahman Kareem Oleiwi1, , Ahmed Abed Hwaidi Abed1,  \n \n1 Center For  Continuing Education, Anbar University, Iraq . \n \n \nA R T I C L E  I N F O  \n \nArticle  Histo ry \nReceived 23  Aug  2023  \nAccepted 22  Oct  2023  \nPublished 15 Nov 2023  \n \nKeywords  \nHIV  \nAIDS  \nMachine learning  \ndeep learning  A B S T R A C T   \n \nHuman Immunodeficiency  Virus (HIV) is a global health issue that can progress to Acquired \nImmunodeficiency Syndrome (HIV) if not diagnosed and treated early. The advent of Artificial \nIntelligence (AI), particularly in machine learning and deep learning, presents new opportunities for \nimproving the accuracy and efficiency of HIV diagnosis. This research explores the application of AI \ntechniques in diagnos ing HIV by reviewing previous studies and proposing a novel AI -based approach. \nThe proposed methodology leverages deep learning algorithms, such as convolutional neural networks \n(CNNs), along with advanced data preprocessing techniques to enhance diagnosti c accuracy, sensitivity, \nand specificity. The results of the proposed CNN -based model show an accuracy of 96.2%, sensitivity \nof 95.8%, specificity of 96.8%, and an AUC -ROC score of 0.965. Compared to Random Forest \n(accuracy: 92.1%), SVM (accuracy: 91.5%), and traditional methods (accuracy: 89.0%), the CNN model \noutperforms existing techniques significantly in terms of accuracy, sensitivity, and specificity. This \ndemonstrates the effectiveness of the proposed AI approach for enhancing early and accurate HIV \ndetection.\n1. INTRODUCTION  \nHuman Immunodeficiency Virus (HIV) is a global public health crisis that severely impacts the immune system by targeting \nCD4 cells, which are essential for immune defense. If untreated, the progression of HIV can lead to Acquired \nImmunode",
    "title": "Diagnosis of HIV (AIDS)  by Using deep learning and machine learning",
    "abstract": "<jats:p>Human Immunodeficiency Virus (HIV) is a global health issue that can progress to Acquired Immunodeficiency Syndrome (HIV) if not diagnosed and treated early. The advent of Artificial Intelligence (AI), particularly in machine learning and deep learning, presents new opportunities for improving the accuracy and efficiency of HIV diagnosis. This research explores the application of AI techniques in diagnosing HIV by reviewing previous studies and proposing a novel AI-based approach. The proposed methodology leverages deep learning algorithms, such as convolutional neural networks (CNNs), along with advanced data preprocessing techniques to enhance diagnostic accuracy, sensitivity, and specificity. The results of the proposed CNN-based model show an accuracy of 96.2%, sensitivity of 95.8%, specificity of 96.8%, and an AUC-ROC score of 0.965. Compared to Random Forest (accuracy: 92.1%), SVM (accuracy: 91.5%), and traditional methods (accuracy: 89.0%), the CNN model outperforms existing techniques significantly in terms of accuracy, sensitivity, and specificity. This demonstrates the effectiveness of the proposed AI approach for enhancing early and accurate HIV detection.\r\n </jats:p>",
    "link": "https://doi.org/10.58496/bjml/2023/012",
    "published": "2024-12-18T17:52:27Z"
  },
  "153": {
    "pdf_path": "data/pdfs/machine learning_paper_14.pdf",
    "text_excerpt": "Techniques for Automated Machine Learning\nYi-Wei Chen, Qingquan Song, Xia Hu\nDepartment of Computer Science and Engineering\nTexas A&M University\n{yiwei_chen, song_3134, xiahu}@tamu.edu\nABSTRACT\nAutomated machine learning (AutoML) aims to ﬁnd opti-\nmal machine learning solutions automatically given a ma-\nchine learning problem. It could release the burden of data\nscientists from the multifarious manual tuning process and\nenable the access of domain experts to the oﬀ-the-shelf ma-\nchine learning solutions without extensive experience. In\nthis paper, we review the current developments of AutoML\nin terms of three categories, automated feature engineer-\ning (AutoFE), automated model and hyperparameter learn-\ning (AutoMHL), and automated deep learning (AutoDL).\nState-of-the-art techniques adopted in the three categories\nare presented, including Bayesian optimization, reinforce-\nment learning, evolutionary algorithm, and gradient-based\napproaches. We summarize popular AutoML frameworks\nand conclude with current open challenges of AutoML.\n1. INTRODUCTION\nAutomated machine learning (AutoML) has emerged as\na prevailing research ﬁeld upon the ubiquitous adoption of\nmachine learning techniques. It aims at automatically de-\ntermininghigh-performancemachinelearningsolutionswith\na little workforce in reasonable time budget. For example,\nGoogle HyperTune, Amazon Model Tuning, and Microsoft\nAzureAutoMLallprovidecloudserviceswithAutoMLtools\nwhich cultivate oﬀ-the-shelf machine learning solutions for\nboth researchers and practitioners. Therefore, AutoML not\nonly liberates them from the time-consuming tuning process\nand tedious trial-and-error iterations but also facilitates the\ndevelopment of solving machine learning problems.\nAtraditionalmachinelearningpipelineisaniterativepro-\ncedure composed of feature engineering, model and hyper-\nparameter selection, and performance evaluation, as shown\nin Figure /one.oldstyle. Data scientists manually manipulate numerous\nfeatures, design models,",
    "title": "Techniques for Automated Machine Learning",
    "abstract": "Automated machine learning (AutoML) aims to find optimal machine learning\nsolutions automatically given a machine learning problem. It could release the\nburden of data scientists from the multifarious manual tuning process and\nenable the access of domain experts to the off-the-shelf machine learning\nsolutions without extensive experience. In this paper, we review the current\ndevelopments of AutoML in terms of three categories, automated feature\nengineering (AutoFE), automated model and hyperparameter learning (AutoMHL),\nand automated deep learning (AutoDL). State-of-the-art techniques adopted in\nthe three categories are presented, including Bayesian optimization,\nreinforcement learning, evolutionary algorithm, and gradient-based approaches.\nWe summarize popular AutoML frameworks and conclude with current open\nchallenges of AutoML.",
    "link": "http://arxiv.org/abs/1907.08908v1",
    "published": "2019-07-21T04:03:36Z"
  },
  "154": {
    "pdf_path": "data/pdfs/machine learning_paper_410.pdf",
    "text_excerpt": "Machine Learning 1: 363-366, 1986\n© 1986 Kluwer Academic Publishers, Boston - Manufactured in The Netherlands\nEditorial: Machine Learning and Discovery\nDiscovery as learning\nIn everyday language, the terms learning and discovery convey rather different\nmeanings. The former suggests a gradual process, while the latter suggests a more\nrapid mental event, often involving some form of insight. Learning may lead to\nan unconscious change in knowledge, while one is always aware that a discovery\nhas been made. The result of learning can be declarative or procedural, while the\nproduct of discovery is always declarative. Learning often involves a transfer of\nknowledge from teacher to student; in contrast, discovery involves acquiring\nknowledge from the environment without the aid of a tutor. Finally, all humans\nand most animals learn from experience, but we reserve the term discovery for\nthe accomplishments of a select few. These boundaries are admittedly vague, but\nthey exist nonetheless.\nDespite the natural distinction between these concepts, the field of machine\nlearning has always viewed discovery as one of its concerns. Undoubtedly, one\nreason for this interest is that, like learning, discovery often involves induction —\nthe act of reasoning from specific facts or data to general rules or laws which\nprovide a general characterization of those facts. Another reason (probably\nrelated to the first) is that, historically, researchers in machine discovery have also\nworked on learning problems and have applied related techniques to these tasks.\nThus, contemporary machine learning researchers tend to view discovery as a\ndifficult form of 'learning from observation' (Carbonell, Michalski, & Mitchell,\n1983).\nThe historical role of discovery within machine learning, together with recent\nprogress in automated discovery, suggested the need for a special treatment of\nthat topic, and the current issue of Machine Learning is the result. The three\npapers in this issue are representative",
    "title": "Editorial: Machine Learning and Discovery",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022814715297",
    "published": "2003-04-04T16:57:10Z"
  },
  "155": {
    "pdf_path": "data/pdfs/machine learning_paper_451.pdf",
    "text_excerpt": "Machine Learning, 38, 9–40, 2000.\nc°2000 Kluwer Academic Publishers. Printed in The Netherlands.\nLEARNABLE EVOLUTION MODEL:\nEvolutionary Processes Guidedby Machine Learning\nRYSZARD S. MICHALSKI michalski@gmu.edu\nMachine Learning and Inference Laboratory, George Mason University, Fairfax, VA andInstitute of Computer Science, Polish Academy of Sciences, Warsaw, Poland\nEditors: Floriana Esposito & Lorenza Saitta\nAbstract. Anewclassofevolutionarycomputationprocessesispresented,called LearnableEvolutionModel or\nLEM.IncontrasttoDarwinian-typeevolutionthatreliesonmutation,recombination,andselectionoperators,LEM\nemploysmachinelearningtogeneratenewpopulations.Speciﬁcally,in MachineLearningmode ,alearningsystem\nseeks reasons why certain individuals in a population (or a collection of past populations) are superior to othersinperformingadesignatedclassoftasks.Thesereasons,expressedasinductivehypotheses,areusedtogeneratenew populations. A remarkable property of LEM is that it is capable of quantum leaps (“insight jumps”) of theﬁtnessfunction,unlikeDarwinian-typeevolutionthattypicallyproceedsthroughnumerousslightimprovements.In our early experimental studies, LEM signiﬁcantly outperformed evolutionary computation methods used inthe experiments, sometimes achieving speed-ups of two or more orders of magnitude in terms of the number ofevolutionarysteps.LEMhasapotentialforawiderangeofapplications,inparticular,insuchdomainsascomplexoptimization or search problems, engineering design, drug design, evolvable hardware, software engineering,economics, data mining, and automatic programming.\nKeywords: multistrategy learning, genetic algorithms, evolution model, evolutionary computation\n1. Introduction\nRecent years have witnessed signiﬁcant progress in the development and applications of\nmachine learning methods, in particular, in scaling them up to cope with large datasets(e.g., Clark & Niblett, 1989; Cohen, 1995; Dietterich, 1997; Mitchell, 1997; Michalski,Bratko,&Kubat,1998).Therehasal",
    "title": "LEARNABLE EVOLUTION MODEL: Evolutionary Processes Guided by Machine Learning",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1007677805582",
    "published": "2002-12-22T05:54:50Z"
  },
  "156": {
    "pdf_path": "data/pdfs/machine learning_paper_546.pdf",
    "text_excerpt": "Machine Learning, 42, 321, 2001\nc°2001 Kluwer Academic Publishers. Manufactured in The Netherlands.\nErrata\nThepublisherapologizesandherebytakesallresponsibilityforanerrorwhichoccurred\ninVolume41,Number3,ofthejournal, MachineLearning . Onthefrontcover,thetitleof\nthearticlewrittenbyKristinP.Bennett,NelloCristianini,JohnShawe-TaylorandDonghuiWuisincorrect. Thecorrecttitleis“EnlargingtheMarginsinPerceptronDecisionTrees.”",
    "title": "Errata",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1017306117718",
    "published": "2002-12-29T16:40:04Z"
  },
  "157": {
    "pdf_path": "data/pdfs/machine learning_paper_115.pdf",
    "text_excerpt": "Page 1 \n Applications of machine learning to machine fault diagnosis: A 1 \nreview and roadmap 2 \nYaguo Lei a,∗, Bin Yang a, Xinwei Jiang a, Feng Jia a, Naipeng Li a, Asoke K. Nandi b 3 \na Key Laboratory of Education Ministry for Modern Design and Rotor -Bearing System, Xi ’an Jiaotong 4 \nUniversity, Xi’ an 710049, China  5 \nb Department of Electronic and Computer Engineering, Brunel University London, Uxbridge UB8 3PH, 6 \nUnited Kingdom  7 \nAbstract 8 \nIntelligent fault diagnosis (IFD) refers to applications of machine learning theories to machine fault 9 \ndiagnosis. This is a promising way to release the contribution from human labor and automatically recognize 10 \nthe health states of machines, thus it has attracte d much attention in the last two or three decades. Although 11 \nIFD has achieved a considerable number of successes, a review still leaves a blank space to systematically 12 \ncover the development of IFD from the cradle to the bloom, and rarely provides potential guidelines for the 13 \nfuture development. To bridge the gap, this paper presents a review and roadmap to systematically cover the 14 \ndevelopment of IFD following the progress of machine learning theories and offer a future perspective. In the 15 \npast, traditional m achine learning theories began to weak the contribution of human labor and brought the era 16 \nof artificial intelligence to machine fault diagnosis. Over the recent years, the advent of deep learning theories 17 \nhas reformed IFD in further releasing the artifici al assistance since the 2010s, which encourages to construct 18 \nan end -to-end diagnosis process. It means to directly bridge the relationship between the increasingly -grown 19 \nmonitoring data and the health states of machines. In the future, transfer learning theories attempt to use the 20 \ndiagnosis knowledge from one or multiple diagnosis tasks to other related ones, which prospectively 21 \novercomes the obstacles in applications of IFD to engineering scenarios. Fi",
    "title": "Applications of machine learning to machine fault diagnosis: A review and roadmap",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/7ae2783a9196fb4bc2a610ae812d19722daddce5",
    "published": "2020-04-01"
  },
  "158": {
    "pdf_path": "data/pdfs/machine learning_paper_235.pdf",
    "text_excerpt": "arXiv:1806.00069v3  [cs.AI]  3 Feb 2019Explaining Explanations: An Overview of\nInterpretability of Machine Learning\nLeilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Mich ael Specter and Lalana Kagal\nComputer Science and Artiﬁcial Intelligence Laboratory\nMassachusetts Institute of Technology\nCambridge, MA 02139\n{lgilpin, davidbau, bzy, abajwa, specter, lkagal }@ mit.edu\nAbstract —There has recently been a surge of work in ex-\nplanatory artiﬁcial intelligence (XAI). This research are a tackles\nthe important problem that complex machines and algorithms\noften cannot provide insights into their behavior and thoug ht\nprocesses. XAI allows users and parts of the internal system to\nbe more transparent, providing explanations of their decis ions\nin some level of detail. These explanations are important to\nensure algorithmic fairness, identify potential bias/pro blems in\nthe training data, and to ensure that the algorithms perform as\nexpected. However, explanations produced by these systems is\nneither standardized nor systematically assessed. In an ef fort to\ncreate best practices and identify open challenges, we desc ribe\nfoundational concepts of explainability and show how they c an\nbe used to classify existing literature. We discuss why curr ent\napproaches to explanatory methods especially for deep neur al\nnetworks are insufﬁcient. Finally, based on our survey, we c on-\nclude with suggested future research directions for explan atory\nartiﬁcial intelligence.\nI. I NTRODUCTION\nAs autonomous machines and black-box algorithms begin\nmaking decisions previously entrusted to humans, it become s\nnecessary for these mechanisms to explain themselves. Desp ite\ntheir success in a broad range of tasks including advertisin g,\nmovie and book recommendations, and mortgage qualiﬁcation ,\nthere is general mistrust about their results. In 2016, Angw in et\nal. [1] analyzed Correctional Offender Management Proﬁlin g\nfor Alternative Sanctions (COMPAS), a widely used criminal\nrisk assessm",
    "title": "Explaining Explanations: An Overview of Interpretability of Machine Learning",
    "abstract": "There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.",
    "link": "https://www.semanticscholar.org/paper/d7701e78e0bfc92b03a89582e80cfb751ac03f26",
    "published": "2018-05-31"
  },
  "159": {
    "pdf_path": "data/pdfs/machine learning_paper_372.pdf",
    "text_excerpt": "This copy is for personal use only.  \nTo order printed copies, contact reprints@rsna.orgThis copy is for personal use only.  \nTo order printed copies, contact reprints@rsna.org\nREVIEWS AND COMMENTARY  • REVIEW\nRecent advances in machine learning offer promise in nu -\nmerous industries and applications, including medical \nimaging (1). Within the innovations of data science, ma -\nchine learning is a class of techniques and area of research \nthat is enabling computers to learn like humans and to ex -\ntract or classify patterns. Machines may further be able to \nanalyze more data sets and extract features from data that \nhumans may not be able to do (2). Recent research and \ndevelopments are enabling technologies that hold promise \nnow and in the future for diagnostic imaging (3). In this \nreview article, we will first define what is meant by “ma -\nchine learning” at broad and granular levels, providing an \nintroduction into how such techniques can be developed \nand applied to imaging interpretation. Second, we will \nprovide examples of applications of machine learning in \ndiagnostic radiology. Third, we will discuss the key barriers \nand challenges in clinical application of machine learning \ntechniques. Finally, we will discuss the future direction and \nnatural extension of machine learning in radiology and be -\nyond radiology in medicine.\nFundamentals of Machine Learning\nDefinition of Machine Learning\nMachine learning is a method of data science that pro -\nvides computers with the ability to learn without being \nprogrammed with explicit rules (2). Machine learning \nenables the creation of algorithms that can learn and \nmake predictions. In contrast to rules-based algorithms, \nmachine learning takes advantage of increased exposure \nto large and new data sets and has the ability to improve \nand learn with experience (3,4).\nMachine Learning Categories\nMachine learning tasks are typically classified into  \nthree broad categories (5), depending on the type of task: su -\npe",
    "title": "Current Applications and Future Impact of Machine Learning in Radiology.",
    "abstract": "Recent advances and future perspectives of machine learning techniques offer promising applications in medical imaging. Machine learning has the potential to improve different steps of the radiology workflow including order scheduling and triage, clinical decision support systems, detection and interpretation of findings, postprocessing and dose estimation, examination quality control, and radiology reporting. In this article, the authors review examples of current applications of machine learning and artificial intelligence techniques in diagnostic radiology. In addition, the future impact and natural extension of these techniques in radiology practice are discussed.",
    "link": "https://www.semanticscholar.org/paper/5fdc2223709079ba5c0f78661cdf66cec2173258",
    "published": "2018-06-26"
  },
  "160": {
    "pdf_path": "data/pdfs/machine learning_paper_25.pdf",
    "text_excerpt": "When Machine Learning Meets Privacy: A Survey and\nOutlook\nBO LIU∗,University of Technology Sydney, Australia\nMING DING, Data61, CSIRO, Australia\nSINA SHAHAM, The University of Sydney, Australia\nWENNY RAHAYU, La Trobe University, Australia\nFARHAD FAROKHI, The University of Melbourne, Australia\nZIHUAI LIN, The University of Sydney, Australia\nThe newly emerged machine learning (e.g. deep learning) methods have become a strong driving force to\nrevolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance\nsystems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence\nera. It is important to note that the problem of privacy preservation in the context of machine learning is\nquite different from that in traditional data privacy protection, as machine learning can act as both friend and\nfoe. Currently, the work on the preservation of privacy and machine learning (ML) is still in an infancy stage,\nas most existing solutions only focus on privacy problems during the machine learning process. Therefore,\na comprehensive study on the privacy preservation problems and machine learning is required. This paper\nsurveys the state of the art in privacy issues and solutions for machine learning. The survey covers three\ncategories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine\nlearning aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection\nschemes. The current research progress in each category is reviewed and the key challenges are identified.\nFinally, based on our in-depth analysis of the area of privacy and machine learning, we point out future\nresearch directions in this field.\nCCS Concepts: •Security and privacy →Privacy protections ;Social network security and privacy .\nAdditional Key Words and Phrases: machine learning, privacy, deep learning, differential privacy\nACM Reference Format:\nBo Li",
    "title": "When Machine Learning Meets Privacy: A Survey and Outlook",
    "abstract": "The newly emerged machine learning (e.g. deep learning) methods have become a\nstrong driving force to revolutionize a wide range of industries, such as smart\nhealthcare, financial technology, and surveillance systems. Meanwhile, privacy\nhas emerged as a big concern in this machine learning-based artificial\nintelligence era. It is important to note that the problem of privacy\npreservation in the context of machine learning is quite different from that in\ntraditional data privacy protection, as machine learning can act as both friend\nand foe. Currently, the work on the preservation of privacy and machine\nlearning (ML) is still in an infancy stage, as most existing solutions only\nfocus on privacy problems during the machine learning process. Therefore, a\ncomprehensive study on the privacy preservation problems and machine learning\nis required. This paper surveys the state of the art in privacy issues and\nsolutions for machine learning. The survey covers three categories of\ninteractions between privacy and machine learning: (i) private machine\nlearning, (ii) machine learning aided privacy protection, and (iii) machine\nlearning-based privacy attack and corresponding protection schemes. The current\nresearch progress in each category is reviewed and the key challenges are\nidentified. Finally, based on our in-depth analysis of the area of privacy and\nmachine learning, we point out future research directions in this field.",
    "link": "http://arxiv.org/abs/2011.11819v1",
    "published": "2020-11-24T00:52:49Z"
  },
  "161": {
    "pdf_path": "data/pdfs/machine learning_paper_593.pdf",
    "text_excerpt": "Machine Learning, 27, 139–172 (1997)\nc°1997 Kluwer Academic Publishers. Manufactured in The Netherlands.\nPruning Algorithms for Rule Learning\nJOHANNES F ˜URNKRANZ jufﬁ@ai.univie.ac.at\nAustrian Research Institute for Artiﬁcial Intelligence, Schottengasse 3, A-1010 Vienna, Austria\nEditor:Raymond J. Mooney\nAbstract. Pre-pruningandPost-pruningaretwostandardtechniquesforhandlingnoiseindecisiontreelearning.\nPre-pruningdealswithnoiseduringlearning,whilepost-pruningaddressesthisproblemafteranoverﬁttingtheoryhasbeenlearned. Weﬁrstreviewseveraladaptationsofpre-andpost-pruningtechniquesforseparate-and-conquerrulelearningalgorithmsanddiscusssomefundamentalproblems. Theprimarygoalofthispaperistoshowhowto solve these problems with two new algorithms that combine and integrate pre- and post-pruning.\nKeywords: Pruning, Noise Handling, Inductive Rule Learning, Inductive Logic Programming\n1. Introduction\nSeparate-and-conquer rule-learning systems have recently gained popularity through the\nsuccess of the Inductive Logic Programming algorithm Foil(Quinlan, 1990, Quinlan &\nCameron-Jones, 1995). In this paper, we will analyze different pruning techniques forthis type of inductive rule learning algorithm and discuss some of their problems. Itsmain contributions are two new algorithms: Top-Down Pruning (TDP), an approach that\ncombines pre- and post-pruning, and Incremental Reduced Error Pruning (I-REP), a very\nefﬁcient integration of pre-and post-pruning.\nPruningis the common framework for avoiding the problem of overﬁtting noisy data.\nThe basic idea is to incorporate a bias towards simpler theories in order to avoid complexruleswithlowcoveragethatcontainirrelevantliteralsthathaveonlybeenaddedtoexcludenoisy examples.\nPre-pruning methods deal with noise during learning. Instead of trying to ﬁnd a theory\nthatiscompleteandconsistentwiththegiventrainingdata,heuristics(i.e., stoppingcriteria )\nare used to relax this constraint by stopping the learning process although some positiveexamplesmayn",
    "title": "Pruning Algorithms for Rule Learning",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1007329424533",
    "published": "2002-12-22T04:48:21Z"
  },
  "162": {
    "pdf_path": "data/pdfs/machine learning_paper_102.pdf",
    "text_excerpt": "On Hyperparameter Optimization of Machine Learning\nAlgorithms: Theory and Practice\nLi Yang and Abdallah Shami\nDepartment of Electrical and Computer Engineering, University of Western Ontario,\n1151 Richmond St, London, Ontario, Canada N6A 3K7\nAbstract\nMachine learning algorithms have been used widely in various applications\nand areas. To \ft a machine learning model into di\u000berent problems, its hyper-\nparameters must be tuned. Selecting the best hyper-parameter con\fguration\nfor machine learning models has a direct impact on the model's performance.\nIt often requires deep knowledge of machine learning algorithms and appro-\npriate hyper-parameter optimization techniques. Although several automatic\noptimization techniques exist, they have di\u000berent strengths and drawbacks\nwhen applied to di\u000berent types of problems. In this paper, optimizing the\nhyper-parameters of common machine learning models is studied. We in-\ntroduce several state-of-the-art optimization techniques and discuss how to\napply them to machine learning algorithms. Many available libraries and\nframeworks developed for hyper-parameter optimization problems are pro-\nvided, and some open challenges of hyper-parameter optimization research\nare also discussed in this paper. Moreover, experiments are conducted on\nbenchmark datasets to compare the performance of di\u000berent optimization\nmethods and provide practical examples of hyper-parameter optimization.\nThis survey paper will help industrial users, data analysts, and researchers\nto better develop machine learning models by identifying the proper hyper-\nparameter con\fgurations e\u000bectively.1\nKeywords: Hyper-parameter optimization, machine learning, Bayesian\nEmail address: {lyang339, abdallah.shami}@uwo.ca (Li Yang and Abdallah\nShami)\n1General Hyperparameter Optimization Code and Tutorials: https://github.com/\nLiYangHart/Hyperparameter-Optimization-of-Machine-Learning-Algorithms\nPreprint submitted to Neurocomputing October 6, 2022arXiv:2007.15745v3  [cs.LG]  5 Oct 202",
    "title": "On Hyperparameter Optimization of Machine Learning Algorithms: Theory and Practice",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/2e5d2f2dc01b150dffc163a9f457848e9b5b5c38",
    "published": "2020-07-30"
  },
  "163": {
    "pdf_path": "data/pdfs/machine learning_paper_199.pdf",
    "text_excerpt": "MATHEMATICS  FOR \nMACHINE LEAR NING\nMarc Peter Deisenroth\nA. Aldo Faisal\nCheng Soon On g\nMATHEMATICS FOR MACHINE LEARNING DEISENROTH ET AL.\nThe fundamental mathematical tools needed to understand machine \nlearning include linear algebra, analytic geometry, matrix decompositions, vector calculus, optimization, probability and statistics. These topics are traditionally taught in disparate courses, making it hard for data science or computer science students, or professionals, to efﬁ  ciently learn the mathematics. This self-contained textbook bridges the gap between mathematical and machine learning texts, introducing the mathematical concepts with a minimum of prerequisites. It uses these concepts to derive four central machine learning methods: linear regression, principal component analysis, Gaussian mixture models and support vector machines. For students and others with a mathematical background, these derivations provide a starting point to machine learning texts. For those learning the mathematics for the ﬁ  rst time, the methods help build intuition and practical experience with applying mathematical concepts. Every chapter includes worked examples and exercises to test understanding. Programming tutorials are offered on the book’s web site.\nMARC PETER DEISENROTH  is Senior Lecturer in Statistical Machine \nLearning at the Department of Computing, Împerial College London.\nA. ALDO FAISAL  leads the Brain & Behaviour Lab at Imperial College \nLondon, where he is also Reader in Neurotechnology at the Department of Bioengineering and the Department of Computing.\nCHENG SOON ONG  is Principal Research Scientist at the Machine Learning \nResearch Group, Data61, CSIRO. He is also Adjunct Associate Professor at Australian National University.\nCover image courtesy of Daniel Bosma / Moment / Getty Images\nCover design by Holly Johnson\nDeisenrith et al. 9781108455145 Cover. C M Y KContents\nForeword 1\nPart I Mathematical Foundations 9\n1 Introduction and Motivation 11\n1.1 Findi",
    "title": "Mathematics for Machine Learning",
    "abstract": "Machine learning is a way to study the algorithm and statistical model that is used by computer to perform a specific task through pattern and deduction [1]. It builds a mathematical model from a sample data which may come under either supervised or unsupervised learning. It is closely\n related to computational statistics which is an interface between statistics and computer science. Also, linear algebra and probability theory are two tools of mathematics which form the basis of machine learning. In general, statistics is a science concerned with collecting, analysing, interpreting\n the data. Data are the facts and figure that can be classified as either quantitative or qualitative. From the given set of data, we can predict the expected observation, difference between the outcome of two observations and how data look like which can help in better decision making process\n [2]. Descriptive and inferential statistics are the two methods of data analysis. Descriptive statistics summarize the raw data into information through which common expectation and variation of data can be taken. It also provides graphical methods that can be used to visualize the sample\n of data and qualitative understanding of observation whereas inferential statistics refers to drawing conclusions from data. Inferences are made under the framework of probability theory. So, understanding of data and interpretation of result are two important aspects of machine learning.\n In this paper, we have reviewed the different methods of ML, mathematics behind ML, its application in day to day life and future aspects.",
    "link": "https://www.semanticscholar.org/paper/4f97e87512eb8bf48ce695443e958725c54908b6",
    "published": "2020-02-01"
  },
  "164": {
    "pdf_path": "data/pdfs/machine learning_paper_427.pdf",
    "text_excerpt": "Machine Learning, 39, 169–202, 2000.\nc°2000 Kluwer Academic Publishers. Printed in The Netherlands.\nMachine Learning for Information Extraction\nin Informal Domains\nDAYNE FREITAG dayne@justresearch.com\nJustsystem Pittsburgh Research Center, 4616 Henry Street, Pittsburgh, PA 15213, USA\nEditor:William Cohen\nAbstract. We consider the problem of learning to perform information extraction in domains where linguistic\nprocessing is problematic, such as Usenet posts, email, and ﬁnger plan ﬁles. In place of syntactic and semanticinformation, other sources of information can be used, such as term frequency, typography, formatting, andmark-up. We describe four learning approaches to this problem, each drawn from a different paradigm: a rotelearner, a term-space learner based on Naive Bayes, an approach using grammatical induction, and a relationalrule learner. Experiments on 14 information extraction problems deﬁned over four diverse document collectionsdemonstratetheeffectivenessoftheseapproaches.Finally,wedescribeamultistrategyapproachwhichcombinestheselearnersandyieldsperformancecompetitivewithorbetterthanthebestofthem.Thistechniqueismodularand ﬂexible, and could ﬁnd application in other machine learning problems.\nKeywords: information extraction, multistrategy learning\n1. Introduction\nIf I were in the market for a bargain computer, then I would beneﬁt from a program that\nmonitors newsgroups where computers are offered for sale until it ﬁnds a suitable one forme.Thedesignofsuchaprogramisessentiallyan informationextraction (IE)problem.We\nknowwhateachdocument(post)saysingeneralterms;itdescribesacomputer.Informationextraction is the problem of summarizing the essential details particular to a given docu-ment. An individual summary produced by our program will take the form of a templatewithtypedslots,inwhicheachslotisﬁlledbyafragmentoftextfromthedocument(e.g.,type: “Pentium”; speed: “200 MHz.”; disksize: “1.5Gig”; etc.).\nExisting work in IE can give us some good ideas about how",
    "title": "Machine Learning for Information Extraction in Informal Domains",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1007601113994",
    "published": "2002-12-22T05:54:50Z"
  },
  "165": {
    "pdf_path": "data/pdfs/machine learning_paper_460.pdf",
    "text_excerpt": "Machin e Learning , 18, 109-114(1995 )\n© 1995 Kluwe r Academi c Publishers , Boston . Manufacture d in The Netherlands .\nResearch  Note\nClassificatio n Accuracy : Machin e Learnin g\nvs. Explici t Knowledg e Acquisitio n\nARIE BEN-DAVI D AN D JANIC E MANDE L msariebd@pluto.mscc.huji.ac.i l\nManagement  Information  Systems,  The Hebrew  University  of Jerusalem,  Mount  Scopus,  Jerusalem  91905  Israel\nEditor : Bruc e Porte r\nAbstract . This empirica l stud y provide s evidenc e that machin e learnin g model s can provid e bette r classificatio n\naccurac y than explici t knowledg e acquisitio n techniques . Th e finding s sugges t that the mai n contributio n of\nmachin e learnin g to exper t system s is not just cost reduction , but rathe r the provisio n of tools for the developmen t\nof bette r exper t systems .\nKeywords : Exper t systems , machin e learning , explici t vs. implici t knowledg e acquisition , classificatio n accurac y\n1. Introductio n\nCan machin e learnin g offer anythin g to exper t systems ? B.G . Buchana n (1989 ) raise d this\nimportan t questio n in an interestin g articl e unde r this title in Machine  Learning.  \"Th e\ncommercia l worl d of exper t system s at large seem s unconvince d that machin e learnin g has\nanythin g to offer yet. I strongl y disagree, \" he wrot e five year s ago. Despit e of the progres s\nwhic h has been mad e since , Buchanan' s observation s are still quit e valid . Perhap s the\nmain reaso n for the skepti c attitud e towar d machin e learnin g (ML ) in commercia l circle s\nstems from the fact that the academi c worl d has not presente d yet sufficien t evidenc e whic h\njustifie s abandonin g older , relativel y well established , method s in favo r of ML models .\nSurprisingl y man y firm s are involve d now in exper t system s (Ansar i & Modarress , 1990) .\nHow can one convinc e a compan y whic h currentl y develop s or uses exper t system s (ES)\nthat ML can be of substantia l benefit ? Developer s of exper t system s m",
    "title": "Classification Accuracy: Machine Learning vs. Explicit Knowledge Acquisition",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022826724635",
    "published": "2003-04-04T16:57:10Z"
  },
  "166": {
    "pdf_path": "data/pdfs/machine learning_paper_658.pdf",
    "text_excerpt": "Machine Learning, 35, 5–23 (1999)\nc°1999 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\nAn Experimental Evaluation of Integrating Machine\nLearning with Knowledge Acquisition\nGEOFFREY I. WEBB webb@deakin.edu.au\nJASON WELLS wells@deakin.edu.au\nZIJIAN ZHENG zijian@deakin.edu.au\nSchool of Computing and Mathematics, Deakin University, Geelong, Victoria 3217, Australia\nEditor:Pat Langley\nAbstract. Machine learning and knowledge acquisition from experts have distinct capabilities that appear to\ncomplementoneanother. Wereportastudythatdemonstratestheintegrationoftheseapproachescanbothimprovethe accuracy of the developed knowledge base and reduce development time. In addition, we found that usersexpected the expert systems created through the integrated approach to have higher accuracy than those createdwithout machine learning and rated the integrated approach less difﬁcult to use. They also provided favorableevaluations of both the speciﬁc integrated software, a system called The Knowledge Factory , and of the general\nvalue of machine learning for knowledge acquisition.\nKeywords: Integrated learning and knowledge acquisition; classiﬁcation learning; evaluation of knowledge\nacquisition techniques\n1. Introduction\nMachine learning1and knowledge acquisition from experts provide different and, on the\nface of it, complementary means of developing knowledge-based systems. The apparentmannerinwhichthestrengthsofonematchtheweaknessesoftheotherhasledtointegrationofthetwoapproaches. Thisintegrationisexpectedtobesynergisticineffect,theresultingcombinedapproachbeingmoreeffectivethaneitherofitscomponents. However,althoughtherehavebeencasestudiesdocumentingsuccessfulapplicationsoftheseintegratedtech-niques(Buntine&Stirling,1991;Morik,Wrobel,Kietz,&Emde,1993;Nedellec,Correia,Ferreira, & Costa, 1994; Webb, 1996), no previous research has provided comparativeevaluation of the relative merits of integrated approaches as opposed to either constituentapproachonitsown. Inp",
    "title": "An Experimental Evaluation of Integrating Machine Learning with Knowledge Acquisition",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1007504102006",
    "published": "2002-12-22T05:04:10Z"
  },
  "167": {
    "pdf_path": "data/pdfs/machine learning_paper_109.pdf",
    "text_excerpt": "1\nMultimodal Machine Learning:\nA Survey and Taxonomy\nTadas Baltru ˇsaitis, Chaitanya Ahuja, and Louis-Philippe Morency\nAbstract —Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste ﬂavors.\nModality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when\nit includes multiple such modalities. In order for Artiﬁcial Intelligence to make progress in understanding the world around us, it needs\nto be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate\ninformation from multiple modalities. It is a vibrant multi-disciplinary ﬁeld of increasing importance and with extraordinary potential.\nInstead of focusing on speciﬁc multimodal applications, this paper surveys the recent advances in multimodal machine learning itself\nand presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader\nchallenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This\nnew taxonomy will enable researchers to better understand the state of the ﬁeld and identify directions for future research.\nIndex Terms —Multimodal, machine learning, introductory, survey.\nF\n1 I NTRODUCTION\nTHEworld surrounding us involves multiple modalities\n— we see objects, hear sounds, feel texture, smell odors,\nand so on. In general terms, a modality refers to the way in\nwhich something happens or is experienced. Most people\nassociate the word modality with the sensory modalities\nwhich represent our primary channels of communication\nand sensation, such as vision or touch. A research problem\nor dataset is therefore characterized as multimodal when it\nincludes multiple such modalities. In this paper we focus\nprimarily, but not exclusively, on three modalities: natural\nlanguage which can be both written or",
    "title": "Multimodal Machine Learning: A Survey and Taxonomy",
    "abstract": "Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.",
    "link": "https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91",
    "published": "2017-05-26"
  },
  "168": {
    "pdf_path": "data/pdfs/machine learning_paper_371.pdf",
    "text_excerpt": "This draft was prepared using the LaTeX style ﬁle belonging to the Journal of Fluid Mechanics 1\nSuper-resolution reconstruction of turbulent\nﬂows with machine learning\nKai Fukami1,2,3y, Koji Fukagata1and Kunihiko Taira2,3\n1Department of Mechanical Engineering, Keio University, Yokohama, 223-8522, Japan\n2Department of Mechanical Engineering, Florida State University, Tallahassee, FL 32310, USA\n3Department of Mechanical and Aerospace Engineering, University of California, Los Angeles,\nCA 90095, USA\n(Received xx; revised xx; accepted xx)\nWe use machine learning to perform super-resolution analysis of grossly under-resolved\nturbulent ﬂow ﬁeld data to reconstruct the high-resolution ﬂow ﬁeld. Two machine-\nlearning models are developed; namely the convolutional neural network (CNN) and the\nhybrid Downsampled Skip-Connection Multi-Scale (DSC/MS) models. These machine-\nlearning models are applied to two-dimensional cylinder wake as a preliminary test and\nshow remarkable ability to reconstruct laminar ﬂow from low-resolution ﬂow ﬁeld data.\nWe further assess the performance of these models for two-dimensional homogeneous\nturbulence. The CNN and DSC/MS models are found to reconstruct turbulent ﬂows\nfrom extremely coarse ﬂow ﬁeld images with remarkable accuracy. For the turbulent ﬂow\nproblem, the machine-leaning based super-resolution analysis can greatly enhance the\nspatial resolution with as little as 50 training snapshot data, holding great potential to\nreveal subgrid-scale physics of complex turbulent ﬂows. With the growing availability\nof ﬂow ﬁeld data from high-ﬁdelity simulations and experiments, the present approach\nmotivates the development of eﬀective super-resolution models for a variety of ﬂuid ﬂows.\nKey words: Machine learning; Computational methods; Turbulent ﬂows.\n1. Introduction\nThe quest for high-resolution ﬂow data has been one of the major pursuits in both\nexperimental and computational ﬂuid dynamics. The miniaturization of hot wires and\nadvancement in partic",
    "title": "Super-resolution reconstruction of turbulent flows with machine learning",
    "abstract": "We use machine learning to perform super-resolution analysis of grossly under-resolved turbulent flow field data to reconstruct the high-resolution flow field. Two machine learning models are developed, namely, the convolutional neural network (CNN) and the hybrid downsampled skip-connection/multi-scale (DSC/MS) models. These machine learning models are applied to a two-dimensional cylinder wake as a preliminary test and show remarkable ability to reconstruct laminar flow from low-resolution flow field data. We further assess the performance of these models for two-dimensional homogeneous turbulence. The CNN and DSC/MS models are found to reconstruct turbulent flows from extremely coarse flow field images with remarkable accuracy. For the turbulent flow problem, the machine-leaning-based super-resolution analysis can greatly enhance the spatial resolution with as little as 50 training snapshot data, holding great potential to reveal subgrid-scale physics of complex turbulent flows. With the growing availability of flow field data from high-fidelity simulations and experiments, the present approach motivates the development of effective super-resolution models for a variety of fluid flows.",
    "link": "https://www.semanticscholar.org/paper/a0390b8d4a82daa1d24bba341b317aa710e4ce4d",
    "published": "2018-11-28"
  },
  "169": {
    "pdf_path": "data/pdfs/machine learning_paper_6.pdf",
    "text_excerpt": "Introduction to Machine Learning\n67577 - Fall, 2008\nAmnon Shashua\nSchool of Computer Science and Engineering\nThe Hebrew University of Jerusalem\nJerusalem, IsraelarXiv:0904.3664v1  [cs.LG]  23 Apr 2009Contents\n1 Bayesian Decision Theory page 1\n1.1 Independence Constraints 5\n1.1.1 Example: Coin Toss 7\n1.1.2 Example: Gaussian Density Estimation 7\n1.2 Incremental Bayes Classi\fer 9\n1.3 Bayes Classi\fer for 2-class Normal Distributions 10\n2 Maximum Likelihood/ Maximum Entropy Duality 12\n2.1 ML and Empirical Distribution 12\n2.2 Relative Entropy 14\n2.3 Maximum Entropy and Duality ML/MaxEnt 15\n3 EM Algorithm: ML over Mixture of Distributions 19\n3.1 The EM Algorithm: General 21\n3.2 EM with i.i.d. Data 24\n3.3 Back to the Coins Example 24\n3.4 Gaussian Mixture 26\n3.5 Application Examples 27\n3.5.1 Gaussian Mixture and Clustering 27\n3.5.2 Multinomial Mixture and \"bag of words\" Application 27\n4 Support Vector Machines and Kernel Functions 30\n4.1 Large Margin Classi\fer as a Quadratic Linear Programming 31\n4.2 The Support Vector Machine 34\n4.3 The Kernel Trick 36\n4.3.1 The Homogeneous Polynomial Kernel 37\n4.3.2 The non-homogeneous Polynomial Kernel 38\n4.3.3 The RBF Kernel 39\n4.3.4 Classifying New Instances 39\niiiiv Contents\n5 Spectral Analysis I: PCA, LDA, CCA 41\n5.1 PCA: Statistical Perspective 42\n5.1.1 Maximizing the Variance of Output Coordinates 43\n5.1.2 Decorrelation: Diagonalization of the Covariance\nMatrix 46\n5.2 PCA: Optimal Reconstruction 47\n5.3 The Case n>>m 49\n5.4 Kernel PCA 49\n5.5 Fisher's LDA: Basic Idea 50\n5.6 Fisher's LDA: General Derivation 52\n5.7 Fisher's LDA: 2-class 54\n5.8 LDA versus SVM 54\n5.9 Canonical Correlation Analysis 55\n6 Spectral Analysis II: Clustering 58\n6.1 K-means Algorithm for Clustering 59\n6.1.1 Matrix Formulation of K-means 60\n6.2 Min-Cut 62\n6.3 Spectral Clustering: Ratio-Cuts and Normalized-Cuts 63\n6.3.1 Ratio-Cuts 64\n6.3.2 Normalized-Cuts 65\n7 The Formal (PAC) Learning Model 69\n7.1 The Formal Model 69\n7.2 The Rectangle Learning Problem 73\n7.3 Learn",
    "title": "Introduction to Machine Learning: Class Notes 67577",
    "abstract": "Introduction to Machine learning covering Statistical Inference (Bayes, EM,\nML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering),\nand PAC learning (the Formal model, VC dimension, Double Sampling theorem).",
    "link": "http://arxiv.org/abs/0904.3664v1",
    "published": "2009-04-23T11:40:57Z"
  },
  "170": {
    "pdf_path": "data/pdfs/machine learning_paper_153.pdf",
    "text_excerpt": "Nature | Vol 594 | 10 June 2021 | 265\nArticleSwarm Learning for decentralized and \nconfidential clinical machine learning\nStefanie Warnat-Herresthal1,2,127, Hartmut Schultze3,127, Krishnaprasad Lingadahalli Shastry3,127, \nSathyanarayanan Manamohan3,127, Saikat Mukherjee3,127, Vishesh Garg3,4,127, \nRavi Sarveswara3,127, Kristian Händler1,5,127, Peter Pickkers6,127, N. Ahmad Aziz7,8,127, \nSofia Ktena9,127, Florian Tran10,11, Michael Bitzer12, Stephan Ossowski13,14, Nicolas Casadei13,14, \nChristian Herr15, Daniel Petersheim16, Uta Behrends17, Fabian Kern18, Tobias Fehlmann18, \nPhilipp Schommers19, Clara Lehmann19,20,21, Max Augustin19,20,21, Jan Rybniker19,20,21, \nJanine Altmüller22, Neha Mishra11, Joana P. Bernardes11, Benjamin Krämer23, \nLorenzo Bonaguro1,2, Jonas Schulte-Schrepping1,2, Elena De Domenico1,5, Christian Siever3, \nMichael Kraut1,5, Milind Desai3, Bruno Monnet3, Maria Saridaki9, Charles Martin Siegel3, \nAnna Drews1,5, Melanie Nuesch-Germano1,2, Heidi Theis1,5, Jan Heyckendorf23, \nStefan Schreiber10, Sarah Kim-Hellmuth16, COVID-19 Aachen Study (COVAS)*, \nJacob Nattermann24,25, Dirk Skowasch26, Ingo Kurth27, Andreas Keller18,28, Robert Bals15, \nPeter Nürnberg22, Olaf Rieß13,14, Philip Rosenstiel11, Mihai G. Netea29,30, Fabian Theis31, \nSach Mukherjee32, Michael Backes33, Anna C. Aschenbrenner1,2,5,29, Thomas Ulas1,2,  \nDeutsche COVID-19 Omics Initiative (DeCOI)*, Monique M. B. Breteler7,34 ,128,  \nEvangelos J. Giamarellos-Bourboulis9,128, Matthijs Kox6,128, Matthias Becker1,5,128, \nSorin Cheran3,128, Michael S. Woodacre3,128, Eng Lim Goh3,128 & Joachim L. Schultze1,2,5 ,128 ✉\nFast and reliable detection of patients with severe and heterogeneous illnesses is a \nmajor goal of precision medicine1,2. Patients with leukaemia can be identified using \nmachine learning on the basis of their blood transcriptomes3. However, there is an \nincreasing divide between what is technically possible and what is allowed, because of \nprivacy legislation4,5. Here, to facilitate",
    "title": "Swarm Learning for decentralized and confidential clinical machine learning",
    "abstract": "Fast and reliable detection of patients with severe and heterogeneous illnesses is a major goal of precision medicine1,2. Patients with leukaemia can be identified using machine learning on the basis of their blood transcriptomes3. However, there is an increasing divide between what is technically possible and what is allowed, because of privacy legislation4,5. Here, to facilitate the integration of any medical data from any data owner worldwide without violating privacy laws, we introduce Swarm Learning—a decentralized machine-learning approach that unites edge computing, blockchain-based peer-to-peer networking and coordination while maintaining confidentiality without the need for a central coordinator, thereby going beyond federated learning. To illustrate the feasibility of using Swarm Learning to develop disease classifiers using distributed data, we chose four use cases of heterogeneous diseases (COVID-19, tuberculosis, leukaemia and lung pathologies). With more than 16,400 blood transcriptomes derived from 127 clinical studies with non-uniform distributions of cases and controls and substantial study biases, as well as more than 95,000 chest X-ray images, we show that Swarm Learning classifiers outperform those developed at individual sites. In addition, Swarm Learning completely fulfils local confidentiality regulations by design. We believe that this approach will notably accelerate the introduction of precision medicine. Swarm Learning is a decentralized machine learning approach that outperforms classifiers developed at individual sites for COVID-19 and other diseases while preserving confidentiality and privacy.",
    "link": "https://www.semanticscholar.org/paper/24d21ecaeb2d2ecc20e26a5e3f5128247704ccfe",
    "published": "2021-05-26"
  },
  "171": {
    "pdf_path": "data/pdfs/machine learning_paper_160.pdf",
    "text_excerpt": "Universal Differential Equations for Scienti\u0000c\nMachine Learning\nChristopher Rackauckas  (  crackauc@mit.edu )\nMassachusetts Institute of Technology https://orcid.org/0000-0001-5850-0663\nYingbo Ma \nJulia Computing\nJulius Martensen \nUniversity of Bremen https://orcid.org/0000-0003-4143-3040\nCollin Warner \nMassachusetts Institute of Technology\nKirill Zubov \nSaint Petersburg State University https://orcid.org/0000-0003-0441-449X\nRohit Supekar \nMassachusetts Institute of Technology\nDominic Skinner \nMassachusetts Institute of Technology https://orcid.org/0000-0002-2698-041X\nAli Ramadhan \nMassachusetts Institute of Technology\nAlan Edelman \nMassachusetts Institute of Technology\nArticle\nKeywords:  machine learning, generalized approaches, modeling  methodology\nPosted Date:  August 31st, 2020\nDOI:  https://doi.org/10.21203/rs.3.rs-55125/v1\nLicense:    This work is licensed under a Creative Commons Att ribution 4.0 International License.  \nRead Full LicenseUniversal Diﬀerential Equations for Scientiﬁc\nMachine Learning\nChristopher Rackauckasa,b, Yingbo Mac, Julius Martensend, Collin\nWarnera, Kirill Zubove, Rohit Supekara, Dominic Skinnera, Ali\nRamadhana, and Alan Edelmana\naMassachusetts Institute of Technology\nbUniversity of Maryland, Baltimore\ncJulia Computing\ndUniversity of Bremen\neSaint Petersburg State University\nAugust 26, 2020\nAbstract 1\nIn the context of science, the well-known adage “a picture is w orth a 2\nthousand words” might well be “a model is worth a thousand datas ets.” 3\nScientiﬁc models, such as Newtonian physics or biological g ene regula- 4\ntory networks, are human-driven simpliﬁcations of complex phen omena 5\nthat serve as surrogates for the countless experiments that valid ated the 6\nmodels. Recently, machine learning has been able to overcome t he in- 7\naccuracies of approximate modeling by directly learning the ent ire set of 8\nnonlinear interactions from data. However, without any predeterm ined 9\nstructure from the scientiﬁc basis behind the problem,",
    "title": "Universal Differential Equations for Scientific Machine Learning",
    "abstract": "\n In the context of science, the well-known adage “a picture is worth a thousand words” might well be “a model is worth a thousand datasets.” Scientific models, such as Newtonian physics or biological gene regulatory networks, are human-driven simplifications of complex phenomena that serve as surrogates for the countless experiments that validated the models. Recently, machine learning has been able to overcome the inaccuracies of approximate modeling by directly learning the entire set of nonlinear interactions from data. However, without any predetermined structure from the scientific basis behind the problem, machine learning approaches are flexible but data-expensive, requiring large databases of homogeneous labeled training data. A central challenge is reconciling data that is at odds with simplified models without requiring \"big data\". In this work demonstrate how a mathematical object, which we denote universal differential equations (UDEs), can be utilized as a theoretical underpinning to a diverse array of problems in scientific machine learning to yield efficient algorithms and generalized approaches. The UDE model augments scientific models with machine-learnable structures for scientifically-based learning. We show how UDEs can be utilized to discover previously unknown governing equations, accurately extrapolate beyond the original data, and accelerate model simulation, all in a time and data-efficient manner. This advance is coupled with open-source software that allows for training UDEs which incorporate physical constraints, delayed interactions, implicitly-defined events, and intrinsic stochasticity in the model. Our examples show how a diverse set of computationally-difficult modeling issues across scientific disciplines, from automatically discovering biological mechanisms to accelerating the training of physics-informed neural networks and large-eddy simulations, can all be transformed into UDE training problems that are efficiently solved by a single software methodology.",
    "link": "https://www.semanticscholar.org/paper/696b388ee6221c6dbcfd647a06883b2bfee773d9",
    "published": "2020-01-13"
  },
  "172": {
    "pdf_path": "data/pdfs/machine learning_paper_567.pdf",
    "text_excerpt": "Machine Learning, 39, 99–101, 2000.\nc°2000 Kluwer Academic Publishers. Printed in The Netherlands.\nSpecial Issue of Machine Learning on Information\nRetrieval Introduction\nJAIME CARBONELL, YIMING YANG\nLanguage Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213\nWILLIAM COHEN\nAT&T Labs—Research, Shannon Laboratory, 180 Park Ave, Florham Park, NJ 07922\nAstheﬁeldofmachinelearning(ML)hasmatured,ithasreachedouttoseveralrelatedﬁelds,\nbothforchallengingapplicationsandproblems(e.g.,robotics),andfornewtechniquesandmethods(e.g.,statisticsanddatabases).Oneespeciallyinterestingsourceofbothproblemsand methods for ML is the ﬁeld of Information Retrieval (IR).\nIR has witnessed a boom in activity and attention in the 1990’s, fueled in large part\non the commercial side by web-based search engines, and in the scientiﬁc side by large-scale, repeatable evaluations on common tasks, as illustrated by the TREC, MUC, TDT,and SUMMAC conferences. One important trend in information retrieval has been to-ward exploration of new tasks. Most early IR research focused on document retrieval forsame-languagead-hocqueries.Morerecentworkhasexploredcross-languageinformationretrieval,inwhichthedocumentsretrievedarewritteninadifferentlanguagethantheusers’query; automated text categorization; text summarization; fact extraction; topic trackingand detection; and multi-media IR. As part of this trend, IR has necessarily reached out torelatedcommunities,suchasthenaturallanguageprocessingcommunity,thespeechrecog-nition community, and the machine learning (ML) community. ML techniques have beenappliedtoseveralIRproblems,includingtextcategorization,topictrackinganddetection,and trainable cross-language IR.\nAlthough IR and ML have both beneﬁted from intellectual cross-pollination in recent\nyears,theMLandIRscientiﬁccommunitiesstilllargelymoveindifferentorbits.Differentterminologyisoftenused;existingresultsaresometimesinadvertentlyre-invented;differ-ent evaluation metrics are used; and the two",
    "title": "Special Issue of Machine Learning on Information Retrieval Introduction",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1007676028106",
    "published": "2002-12-22T05:54:50Z"
  },
  "173": {
    "pdf_path": "data/pdfs/machine learning_paper_58.pdf",
    "text_excerpt": "A Game of Dice: Machine Learning and the\nQuestion Concerning Art\nPaul Todorov\npaul.todorov@hec.edu\nAbstract\nBy this art you may contemplate the variation of the 26 letters.\nWe review some practical and philosophical questions raised by the\nuse of machine learning in creative practice. Beyond the obvious problems\nregarding plagiarism and authorship, we argue that the novelty in AI\nArt relies mostly on a narrow machine learning contribution : manifold\napproximation. Nevertheless, this contribution creates a radical shift in\nthe way we have to consider this movement. Is this omnipotent tool a\nblessing or a curse for the artists?\n1 Plagiarism and Authorship\nPlagiarism is not a very well deﬁned term in the ﬁeld of art. Appropriation\n(inspiration, collage, tribute) has always been at the heart of the creative pro-\ncess, yet, the use of machine learning brings additional diﬃculties. It may\nseem paradoxical or ironical since there is also hope that deep learning could\nactually help ﬁght against art forgery [1]. Most of the AI art practitioners use\nalgorithms derived from generative adversarial networks [2]. In this type of\nwork, the artist has to (1) deﬁne a training set, (2) choose the algorithm (GAN\nvariant), choose the hyper parameters and the optimization method, (3) select\nsamples from a latent space, (4) generate outputs and possibly post-process them.\nWhy is plagiarism a tricky issue with AI Art? We can trivially show that\nevery painting is a geometric transformation of another painting. Consequently,\nthe problem for AI art isn’t that they can be obtained as a geometric transfor-\nmation of other paintings, but that it is precisely the way they are created. By\ndoing so, it confronts us to a threshold phenomenon. Everyone would agree that\noutputs from an algorithm that is able to perfectly memorize and reproduce\na training set, or that do not deteriorate at all a copyrighted input, would be\nplagiarism. But what if we imagine the following simple example: training an\nau",
    "title": "A Game of Dice: Machine Learning and the Question Concerning Art",
    "abstract": "We review some practical and philosophical questions raised by the use of\nmachine learning in creative practice. Beyond the obvious problems regarding\nplagiarism and authorship, we argue that the novelty in AI Art relies mostly on\na narrow machine learning contribution : manifold approximation. Nevertheless,\nthis contribution creates a radical shift in the way we have to consider this\nmovement. Is this omnipotent tool a blessing or a curse for the artists?",
    "link": "http://arxiv.org/abs/1904.01957v1",
    "published": "2019-04-02T09:37:44Z"
  },
  "174": {
    "pdf_path": "data/pdfs/machine learning_paper_320.pdf",
    "text_excerpt": "Machine learning-assisted directed protein evolution\nwith combinatorial libraries\nZachary Wua, S. B. Jennifer Kana, Russell D. Lewisb, Bruce J. Wittmannb, and Frances H. Arnolda,b,1\naDivision of Chemistry and Chemical Engineering, California Institute of Technology, Pasadena, CA 91125; andbDivision of Biology and Bioengineering,\nCalifornia Institute of Technology, Pasadena, CA 91125\nContributed by Frances H. Arnold, March 18, 2019 (sent for review February 4, 2019; reviewed by Marc Ostermeier and Justin B. Siegel)\nTo reduce experimental effort associated with directed protein\nevolution and to explore the sequence space encoded by mutating\nmultiple positions simultaneously, we incorporate machine learn-\ning into the directed evolution workflow. Combinatorial sequence\nspace can be quite expensive to sample experimentally, but\nmachine-learning models trained on tested variants provide a fast\nmethod for testing sequence space computationally. We validated\nthis approach on a large published empirical fitness landscape for\nhuman GB1 binding protein, demonstrating that machine learning-\nguided directed evolution finds variants with higher fitness than\nthose found by other directed evolution approaches. We then\nprovide an example application in evolving an enzyme to produce\neach of the two possible product enantiomers (i.e., stereodiver-\ngence) of a new-to-nature carbene Si –H insertion reaction. The\napproach predicted libraries enriched in functional enzymes and\nfixed seven mutations in two rounds of evolution to identify var-\niants for selective catalysis with 93% and 79% ee(enantiomeric\nexcess). By greatly increasing throughput with in silico modeling,\nmachine learning enhances the quality and diversity of sequence\nsolutions for a protein engineering problem.\nprotein engineering |machine learning |directed evolution |enzyme |\ncatalysis\nNature provides countless proteins with untapped potential\nfor technological applications. Rarely optimal for their\nenvisioned human uses",
    "title": "Machine learning-assisted directed protein evolution with combinatorial libraries",
    "abstract": "Significance Proteins often function poorly when used outside their natural contexts; directed evolution can be used to engineer them to be more efficient in new roles. We propose that the expense of experimentally testing a large number of protein variants can be decreased and the outcome can be improved by incorporating machine learning with directed evolution. Simulations on an empirical fitness landscape demonstrate that the expected performance improvement is greater with this approach. Machine learning-assisted directed evolution from a single parent produced enzyme variants that selectively synthesize the enantiomeric products of a new-to-nature chemical transformation. By exploring multiple mutations simultaneously, machine learning efficiently navigates large regions of sequence space to identify improved proteins and also produces diverse solutions to engineering problems. To reduce experimental effort associated with directed protein evolution and to explore the sequence space encoded by mutating multiple positions simultaneously, we incorporate machine learning into the directed evolution workflow. Combinatorial sequence space can be quite expensive to sample experimentally, but machine-learning models trained on tested variants provide a fast method for testing sequence space computationally. We validated this approach on a large published empirical fitness landscape for human GB1 binding protein, demonstrating that machine learning-guided directed evolution finds variants with higher fitness than those found by other directed evolution approaches. We then provide an example application in evolving an enzyme to produce each of the two possible product enantiomers (i.e., stereodivergence) of a new-to-nature carbene Si–H insertion reaction. The approach predicted libraries enriched in functional enzymes and fixed seven mutations in two rounds of evolution to identify variants for selective catalysis with 93% and 79% ee (enantiomeric excess). By greatly increasing throughput with in silico modeling, machine learning enhances the quality and diversity of sequence solutions for a protein engineering problem.",
    "link": "https://www.semanticscholar.org/paper/89f88f324bb3775f63f87cec90a4283a3522ab44",
    "published": "2019-02-20"
  },
  "175": {
    "pdf_path": "data/pdfs/Machine Learning for Clinical Predictive Analytics_paper_8.pdf",
    "text_excerpt": "Stabilizing Sparse Cox Model using Clinical Structures in Electronic\nMedical Records\nShivapratap Gopakumar, Truyen Tran, Dinh Phung, Svetha Venkatesh\nCenter for Pattern Recognition and Data Analytics , Deakin University, Australia\nfsgopakum, truyen.tran, dinh.phung, svetha.venkatesh g@deakin.edu.au\nAbstract\nStability in clinical prediction models is crucial for\ntransferability between studies, yet has received lit-\ntle attention. The problem is paramount in high-\ndimensional data which invites sparse models with fea-\nture selection capability. We introduce an effective\nmethod to stabilize sparse Cox model of time-to-events\nusing clinical structures inherent in Electronic Medical\nRecords (EMR). Model estimation is stabilized using\na feature graph derived from two types of EMR struc-\ntures: temporal structure of disease and intervention re-\ncurrences, and hierarchical structure of medical knowl-\nedge and practices. We demonstrate the efﬁcacy of the\nmethod in predicting time-to-readmission of heart fail-\nure patients. On two stability measures – the Jaccard\nindex and the Consistency index – the use of clinical\nstructures signiﬁcantly increased feature stability with-\nout hurting discriminative power. Our model reported\na competitive AUC of 0.64 (95% CIs: [0.58,0.69]) for 6\nmonths prediction.\n1 Introduction\nHeart failure is a serious illness which demands fre-\nquent hospitalization. It is estimated that 50% of heart\nfailure patients are readmitted within 6 months after\ntheir discharge [4]. A signiﬁcant amount of these read-\nmissions can be predicted and prevented. Existing read-\nmission prediction models use diverse subsets of clini-\ncal variables from prior hypotheses or medical litera-\nture [9], making it hard to reach a consensus of what\nare predictive and what are not. With the emergence\nof electronic medical records (EMR), it is now possible\nto obtain data on all aspects of patient care over time.\nA typical EMR database contains full details about de-\nmographics, h",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "176": {
    "pdf_path": "data/pdfs/machine learning_paper_326.pdf",
    "text_excerpt": "PC71CH16_Noe ARjats.cls April 9,2020 14:5\nAnnualReviewofPhysicalChemistry\nMachine Learning for\nMolecular Simulation\nFrank Noé,1,2,3Alexandre Tkatchenko,4\nKlaus-Robert Müller,5,6,7and Cecilia Clementi1,3,8\n1Department of Mathematics and Computer Science,Freie Universität Berlin,14195 Berlin,\nGermany; email: frank.noe@fu-berlin.de\n2Department of Physics,Freie Universität Berlin,14195 Berlin,Germany\n3Department of Chemistry and Center for Theoretical Biological Physics,Rice University,\nHouston,Texas 77005,USA; email: cecilia@rice.edu\n4Physics and Materials Science Research Unit,University of Luxembourg,1511 Luxembourg,\nLuxembourg; email: alexandre.tkatchenko@uni.lu\n5Department of Computer Science,Technical University Berlin,10587 Berlin,Germany;\nemail: klaus-robert.mueller@tu-berlin.de\n6Max-Planck-Institut für Informatik,66123 Saarbrücken,Germany\n7Department of Brain and Cognitive Engineering,Korea University,Seoul 136-713,\nSouth Korea\n8Department of Physics,Rice University,Houston,Texas 77005,USA\nAnnu.Rev.Phys.Chem.2020.71:361–90\nFirst published as a Review in Advance on\nFebruary 24,2020\nTheAnnualReviewofPhysicalChemistry is online at\nphyschem.annualreviews.org\nhttps://doi.org/10.1146/annurev-physchem-042018-\n052331\nCopyright © 2020 by Annual Reviews.\nAll rights reservedKeywords\nmachine learning,neural networks,molecular simulation,quantum\nmechanics,coarse graining,kinetics\nAbstract\nMachinelearning(ML)istransformingallareasofscience.Thecomplexand\ntime-consuming calculations in molecular simulations are particularly suit-\nableforanMLrevolutionandhavealreadybeenprofoundlyaffectedbythe\napplication of existing ML methods. Here we review recent ML methods\nfor molecular simulation, with particular focus on (deep) neural networks\nfor the prediction of quantum-mechanical energies and forces, on coarse-\ngrained molecular dynamics, on the extraction of free energy surfaces and\nkinetics, and on generative network approaches to sample molecular equi-\nlibrium structures and comput",
    "title": "Machine learning for molecular simulation",
    "abstract": "Machine learning (ML) is transforming all areas of science. The complex and time-consuming calculations in molecular simulations are particularly suitable for an ML revolution and have already been profoundly affected by the application of existing ML methods. Here we review recent ML methods for molecular simulation, with particular focus on (deep) neural networks for the prediction of quantum-mechanical energies and forces, on coarse-grained molecular dynamics, on the extraction of free energy surfaces and kinetics, and on generative network approaches to sample molecular equilibrium structures and compute thermodynamics. To explain these methods and illustrate open methodological problems, we review some important principles of molecular physics and describe how they can be incorporated into ML structures. Finally, we identify and describe a list of open challenges for the interface between ML and molecular simulation. Expected final online publication date for the Annual Review of Physical Chemistry, Volume 71 is April 20, 2020. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.",
    "link": "https://www.semanticscholar.org/paper/b674a7aee72e9b9cc5390eca13f9c5c7812f2ba0",
    "published": "2019-11-07"
  },
  "177": {
    "pdf_path": "data/pdfs/machine learning_paper_63.pdf",
    "text_excerpt": "Integrating Machine Learning with Physics-Based Modeling\nWeinan E1,2, Jiequn Han1, and Linfeng Zhang2\n1Department of Mathematics, Princeton University\n2Program in Applied and Computational Mathematics, Princeton University\nAbstract\nMachine learning is poised as a very powerful tool that can drastically improve\nour ability to carry out scienti\fc research. However, many issues need to be ad-\ndressed before this becomes a reality. This article focuses on one particular issue of\nbroad interest: How can we integrate machine learning with physics-based modeling\nto develop new interpretable and truly reliable physical models? After introducing\nthe general guidelines, we discuss the two most important issues for developing ma-\nchine learning-based physical models: Imposing physical constraints and obtaining\noptimal datasets. We also provide a simple and intuitive explanation for the fun-\ndamental reasons behind the success of modern machine learning, as well as an\nintroduction to the concurrent machine learning framework needed for integrating\nmachine learning with physics-based modeling. Molecular dynamics and moment\nclosure of kinetic equations are used as examples to illustrate the main issues dis-\ncussed. We end with a general discussion on where this integration will lead us to,\nand where the new frontier will be after machine learning is successfully integrated\ninto scienti\fc modeling.\n1 Fundamental laws and practical methodologies\nPhysics is centered on two main themes: the search for fundamental laws and the solution\nof practical problems. The former has resulted in Newton's laws, Maxwell equations,\nthe theory of relativity and quantum mechanics. The latter has been the foundation of\nmodern technology, ranging from automobiles, airplanes, computers to cell phones. In\n1929, after quantum mechanics was just discovered, Paul Dirac made the following claim\n[1]:\n\\The underlying physical laws necessary for the mathematical theory of a large part\nof physics and the whole o",
    "title": "Integrating Machine Learning with Physics-Based Modeling",
    "abstract": "Machine learning is poised as a very powerful tool that can drastically\nimprove our ability to carry out scientific research. However, many issues need\nto be addressed before this becomes a reality. This article focuses on one\nparticular issue of broad interest: How can we integrate machine learning with\nphysics-based modeling to develop new interpretable and truly reliable physical\nmodels? After introducing the general guidelines, we discuss the two most\nimportant issues for developing machine learning-based physical models:\nImposing physical constraints and obtaining optimal datasets. We also provide a\nsimple and intuitive explanation for the fundamental reasons behind the success\nof modern machine learning, as well as an introduction to the concurrent\nmachine learning framework needed for integrating machine learning with\nphysics-based modeling. Molecular dynamics and moment closure of kinetic\nequations are used as examples to illustrate the main issues discussed. We end\nwith a general discussion on where this integration will lead us to, and where\nthe new frontier will be after machine learning is successfully integrated into\nscientific modeling.",
    "link": "http://arxiv.org/abs/2006.02619v1",
    "published": "2020-06-04T02:35:10Z"
  },
  "178": {
    "pdf_path": "data/pdfs/machine learning_paper_35.pdf",
    "text_excerpt": "Efﬁcient Deep Learning on Multi-Source Private Data\nNick Hynes Raymond Cheng Dawn Song\nUC Berkeley\n{nhynes, ryscheng, dawnsong}@berkeley.edu\nAbstract —Machine learning models beneﬁt from large and\ndiverse datasets. Using such datasets, however, often requires\ntrusting a centralized data aggregator. For sensitive applications\nlike healthcare and ﬁnance this is undesirable as it could compro-\nmise patient privacy or divulge trade secrets. Recent advances\nin secure and privacy-preserving computation, including trusted\nhardware enclaves and differential privacy, offer a way for\nmutually distrusting parties to efﬁciently train a machine learning\nmodel without revealing the training data. In this work, we\nintroduce Myelin, a deep learning framework which combines\nthese privacy-preservation primitives, and use it to establish a\nbaseline level of performance for fully private machine learning.\nI. I NTRODUCTION\nMachine learning (ML) has enabled a variety of applications\nfrom smart-homes to personal assistants. Such success is largely\ndue to recent algorithmic breakthroughs, increased availability\nof computational resources, and access to vast quantities of\ndata which enable training complex models. However, such\ndatasets often contain sensitive information and therefore\nraise several privacy concerns. For instance, it has recently\nbeen demonstrated that personally identifying information can\nbe inferred from even rough estimates of an ad campaign’s\naudience size [29].\nAdditionally, machine learning models can beneﬁt from\nmultiple providers’ shared data. Examples include clinical\nresearchers training a model on patient information from several\ngeographically distributed clinics, and banks pooling data to\ntrain a higher quality fraud detection model. In both cases,\ndirectly sharing data is unacceptable: the clinics must protect\ntheir patients’ privacy and banks desire to protect their trade\nsecrets. Thus, protecting the conﬁdentiality of the data, the\nmodel, and the computatio",
    "title": "Efficient Deep Learning on Multi-Source Private Data",
    "abstract": "Machine learning models benefit from large and diverse datasets. Using such\ndatasets, however, often requires trusting a centralized data aggregator. For\nsensitive applications like healthcare and finance this is undesirable as it\ncould compromise patient privacy or divulge trade secrets. Recent advances in\nsecure and privacy-preserving computation, including trusted hardware enclaves\nand differential privacy, offer a way for mutually distrusting parties to\nefficiently train a machine learning model without revealing the training data.\nIn this work, we introduce Myelin, a deep learning framework which combines\nthese privacy-preservation primitives, and use it to establish a baseline level\nof performance for fully private machine learning.",
    "link": "http://arxiv.org/abs/1807.06689v1",
    "published": "2018-07-17T22:18:19Z"
  },
  "179": {
    "pdf_path": "data/pdfs/machine learning_paper_673.pdf",
    "text_excerpt": "Machin e Learnin g 2: 343-370 , 1988\n© 198 8 Kluwe r Academi c Publishers , Bosto n - Manufacture d in The Netherland s\nLearnin g From Nois y Example s\nDAN A ANGLUI N (ANGLUIN@YALE.EDU )\nDepartment  of Computer  Science,  Yale  University,  P.O.  Box  2158,\nNew Haven,  CT 06520,  U.S.A.\nPHILI P LAIR D (LAIRD%PLU@IO.ARC.NASA.GOV )\nNASA  Ames  Research  Center,  MS 244-17,  Moffett  Field,  CA 94035,  U.S.A.\n(Received : Jun e 1, 1987 )\n(Revised : Novembe r 20, 1987 )\nKeywords : Concep t learning , learnin g from examples , probabl y approximatel y correc t\nlearning , nois y data , theoretica l limitation s\nAbstract . The  basic questio n addresse d in this pape r is: how  can a learnin g algorith m\ncope with incorrec t trainin g examples ? Specifically , how can algorithm s that produc e\nan \"approximatel y correct \" identificatio n with \"hig h probability \" for reliabl e data be\nadapte d to handl e nois y data ? We show that whe n the teache r may mak e independen t\nrando m error s in classifyin g the exampl e data , the strateg y of selectin g the mos t con-\nsisten t rule for the sampl e is sufficient , and usuall y require s a feasibl y smal l numbe r of\nexamples , provide d nois e affect s less than half the example s on average . In this settin g\nwe are able to estimat e the rate of nois e usin g only the knowledg e that the rate is less\nthan one half . The basi c idea s exten d to othe r type s of rando m nois e as well . We also\nshow that the searc h proble m associate d wit h this strateg y is intractabl e in general .\nHowever , for particula r classe s of rules the targe t rule may be efficientl y identifie d if we\nuse technique s specifi c to that class . For an importan t clas s of formula s - the k-CN F\nformula s studie d by Valian t - we presen t a polynomial-tim e algorith m that identifie s\nconcept s in this form whe n the rate of classificatio n error s is less than one half .\n1. Introductio n\nThe abilit y to form genera l concept s on the basis of part",
    "title": "Learning From Noisy Examples",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022873112823",
    "published": "2003-04-04T16:57:10Z"
  },
  "180": {
    "pdf_path": "data/pdfs/machine learning_paper_495.pdf",
    "text_excerpt": "Machin e Learning , 20, 5-22 (1995 )\n& 199 5 Kluwe r Academi c Publishers , Boston . Manufacture d in The Netherlands ,\nEvaluatio n and Selectio n of Biase s in Machin e\nLearnin g\nDIAN A F. GORDO N gordon@aic.nrl.navy.mi l\nNaval  Research  Laboratory\nMARI E DESJARDIN S marie@erg.sri.co m\nSRI International\nEditor : Thoma s G. Dietteric h\nAbstract . In this introduction , we defin e the term bias  as it is used in machin e learnin g systems . We motivat e\nthe importanc e of automate d method s for evaluatin g and selectin g biase s usin g a framewor k of bias selectio n\nas searc h in bias and meta-bia s spaces . Recen t researc h in the field of machin e learnin g bias is summarized .\nKeywords : bias , concep t learnin g\n1. Introductio n\nThis specia l issue of Machine  Learning  focuse s on the evaluatio n and selectio n of biases .\nThe paper s in this issue describ e method s by whic h intelligen t system s automaticall y eval -\nuate and selec t thei r own biases , and tools for analyzin g and testin g variou s approache s to\nbias selection . In this paper , we motivat e the importanc e of this topic . Sinc e mos t reader s\nwill be familia r with supervise d concep t learning , we phras e our discussio n withi n that\nframework . However , bias as we presen t it here is a part of ever y type of learning .\nWe outlin e a framewor k for treatin g bias selectio n as a proces s of designin g appropriat e\nsearc h method s over the bias and meta-bia s spaces . Thi s framewor k has two essentia l\nfeatures : it divide s bias into representationa l and procedura l components , and it char -\nacterize s learnin g as searc h withi n multipl e tiers . The source s of bias withi n a syste m\ncan thus be identifie d and analyze d with respec t to thei r influenc e on this multi-tiere d\nsearc h process , and bias shift become s searc h at the bias level . The framewor k provide s\nan analyti c tool with whic h to compar e differen t system s (includin g those not develope d\nwithi n the ",
    "title": "Evaluation and Selection of Biases in Machine Learning",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022630017346",
    "published": "2003-04-04T16:55:36Z"
  },
  "181": {
    "pdf_path": "data/pdfs/Machine Learning for Clinical Predictive Analytics_paper_6.pdf",
    "text_excerpt": "© 2024 IJRAR  October 2024, Vol ume 11, Issue 4                    www.ijrar.org ( E-ISSN 2348 -1269, P - ISSN 2349 -5138 ) \nIJRAR24D12 53 International Journal of Research and Analytical Reviews (IJRAR)  164 \n Advancements In Heart Disease Prediction: A \nMachine Learning Approach For Early Detection \nAnd Risk Assessment  \n1Balaji Shesharao Ingole, 2Vishnu Ramineni, 3Nikhil Bangad, 4Koushik Kumar Ganeeb, \n5Priyankkumar Patel,  \n1Researcher, 2Senior Staff Software Engineer, 3Data Engineer, 4Researcher, 5Researcher  \n1Indepedent Research,  \n1IEEE Org, Evans, GA, USA  \n \nAbstract: The primary aim of the paper is to comprehend, assess, and analyze the role, relevance, and efficiency of machine \nlearning models in anticipating heart disease risks using clinical data. While the essentiality of heart disease risk predict ion can’t be \nemphasized more, the us age of machine learning (ML) in the identification and assessment of the effect of its multiple features on \nthe division of patients with and without heart disease, generating a reliable clinical dataset, is equally important. The pa per relies \nessentially on cross -sectional clinical data. The ML approach is designed potentially to strengthen various clinical features in the \nheart disease prognosis process. Some features turn out to be strong predictors adding potential values. The paper entails se ven ML \nclassifiers Logistic Regression, Random Forest, Decision Tree, Naive Bayes, k -nearest Neighbors, Neural Networks, and Support \nVector Machine (SVM). The evaluation of the performance of each model is done based on accuracy metrics. Interestingly, the \nSupport V ector Machine (SVM) demonstrates the highest accuracy percentage i.e. 91.51%, proving its worth among the evaluated \nmodels in the realm of predictive ability. The overall findings of the research demonstrate the superiority of advanced compu tational \nmethod ologies in the evaluation, prediction, improvement, and management of cardiovascular risks. In ot",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "182": {
    "pdf_path": "data/pdfs/machine learning_paper_26.pdf",
    "text_excerpt": "A METHOD TO BENCHMARK HIGH-DIMENSIONAL PROCESS DRIFT\nDETECTION\nEDGAR WOLF∗AND TOBIAS WINDISCH\nUniversity of Applied Sciences Kempten, Germany\nAbstract. Process curves are multivariate finite time series data coming from manufac-\nturing processes. This paper studies machine learning that detect drifts in process curve\ndatasets. A theoretic framework to synthetically generate process curves in a controlled way\nis introduced in order to benchmark machine learning algorithms for process drift detection.\nAn evaluation score, called the temporal area under the curve, is introduced, which allows\nto quantify how well machine learning models unveil curves belonging to drift segments. Fi-\nnally, a benchmark study comparing popular machine learning approaches on synthetic data\ngenerated with the introduced framework is presented that shows that existing algorithms\noften struggle with datasets containing multiple drift segments.\n1.Introduction\nManufacturing lines typically consist of processes arranged sequentially, each using tech-\nniques like casting, forming, or joining to shape components to their final specifications.\nAdvanced sensor technology enables precise monitoring of key performance indicators, like\nforce, pressure, or temperature, over time. IoT-enabled systems now commonly store the data\nobtained, called process curves , facilitating analysis across both single components and entire\nproduction sequences [1]. Issues like anomalous batches, tool wear, or miscalibrations can de-\ngrade performance, often subtly, by causing gradual shifts in process curves. Thus, detecting\nprocess drifts is key to keep unplanned downtimes and scrap parts at bay. In high-volume\nproduction, this is particularly challenging due to the rapid data generation and complexity\nof multi-variable curves and hence these settings have been an ideal application for machine\nlearning methods [2, 3, 4, 5, 6, 7, 8]. Although process curves are multivariate time-series,\nprocess drift detection should not",
    "title": "A method to benchmark high-dimensional process drift detection",
    "abstract": "Process curves are multivariate finite time series data coming from\nmanufacturing processes. This paper studies machine learning that detect drifts\nin process curve datasets. A theoretic framework to synthetically generate\nprocess curves in a controlled way is introduced in order to benchmark machine\nlearning algorithms for process drift detection. An evaluation score, called\nthe temporal area under the curve, is introduced, which allows to quantify how\nwell machine learning models unveil curves belonging to drift segments.\nFinally, a benchmark study comparing popular machine learning approaches on\nsynthetic data generated with the introduced framework is presented that shows\nthat existing algorithms often struggle with datasets containing multiple drift\nsegments.",
    "link": "http://arxiv.org/abs/2409.03669v2",
    "published": "2024-09-05T16:23:07Z"
  },
  "183": {
    "pdf_path": "data/pdfs/machine learning_paper_547.pdf",
    "text_excerpt": "Machine Learning, 21, 125-150 (1995)\n© 1995 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\nMachine Discovery of Protein Motifs\nDARRELL CONKLIN * conklin@zgi.com\nZymoGenetics Inc., 1201 Eastlake Avenue East, Seattle, WA 98102\nEditors: Jude Shavlik, Lawrence Hunter, and David Searls\nAbstract. The investigation of relations between protein tertiary structure and amino acid sequence is a topic\nof tremendous importance in molecular biology. The automated discovery of recurrent patterns of structure\nand sequence is an essential part of this investigation. These patterns, known as protein motifs, are abstractions\nof fragments drawn from proteins of known sequence and tertiary structure. This paper has two objectives.\nThe first is to introduce and define protein motifs, and provide a survey of previous research on protein motif\ndiscovery. The second is to present and apply a novel approach to protein motif representation and discovery,\nwhich is based on a spatial description logic and the symbolic machine learning paradigm of structured concept\nformation. A large database of protein fragments is processed using this approach, and several interesting and\nsignificant protein motifs are discovered.\nKeywords: protein tertiary structure, machine discovery, relational learning, knowledge representation, de-\nscription logics, information retrieval, knowledge discovery in databases\n1. Introduction and definitions\nA task of growing importance in the management of biochemical and crystallographic\ndata is the ability to perform generalization and abstraction over large sets of related\nphysical observations. There exist recurrent patterns and rules of structural biochemistry\nhidden in the Protein Data Bank (Bernstein et al., 1977); machine discovery techniques\ncan help to uncover these patterns and rules. Generalized patterns can facilitate efficient\ninformation retrieval and data incorporation, providing a conceptual framework to which\nnew structural data can be ",
    "title": "Machine Discovery of Protein Motifs",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022673832367",
    "published": "2003-04-04T16:55:36Z"
  },
  "184": {
    "pdf_path": "data/pdfs/machine learning_paper_181.pdf",
    "text_excerpt": " 1Title: All-Optical Machine Learning Using Diffracti ve Deep Neural Networks \nAuthors: Xing Lin 1,2,3, †, Yair Rivenson 1,2,3, †, Nezih T. Yardimci 1,3 , Muhammed Veli 1,2,3 , Mona Jarrahi 1,3  \nand Aydogan Ozcan 1,2,3,4,* \nAffiliations: \n1Electrical and Computer Engineering Department, Uni versity of California, Los Angeles, CA, 90095, USA \n2Bioengineering Department, University of California , Los Angeles, CA, 90095, USA \n3California NanoSystems Institute (CNSI), University  of California, Los Angeles, CA, 90095, USA \n4Department of Surgery, David Geffen School of Medic ine, University of California, Los Angeles, CA, 900 95, \nUSA \n*Correspondence to: ozcan@ucla.edu   \n†These authors contributed equally to this work. \nAbstract: We introduce an all-optical Diffractive Deep Neural  Network (D 2NN) architecture that can learn to \nimplement various functions after deep learning-bas ed design of passive diffractive layers that work c ollectively. \nWe experimentally demonstrated the success of this framework by creating 3D-printed D 2NNs that learned to \nimplement handwritten digit classification and the function of an imaging lens at terahertz spectrum. With the \nexisting plethora of 3D-printing and other lithogra phic fabrication methods as well as spatial-light-m odulators, \nthis all-optical deep learning framework can perfor m, at the speed of light, various complex functions  that \ncomputer-based neural networks can implement, and w ill find applications in all-optical image analysis , feature \ndetection and object classification, also enabling new camera designs and optical components that can learn to \nperform unique tasks using D2NNs. \n \n   2Main Text \nDeep learning is one of the fastest-growing machine  learning methods of this decade ( 1), and it uses multi-\nlayered artificial neural networks implemented in a  computer to digitally learn data representation an d \nabstraction, and perform advanced tasks, comparable  to or even superior than the performance of h",
    "title": "All-optical machine learning using diffractive deep neural networks",
    "abstract": "All-optical deep learning Deep learning uses multilayered artificial neural networks to learn digitally from large datasets. It then performs advanced identification and classification tasks. To date, these multilayered neural networks have been implemented on a computer. Lin et al. demonstrate all-optical machine learning that uses passive optical components that can be patterned and fabricated with 3D-printing. Their hardware approach comprises stacked layers of diffractive optical elements analogous to an artificial neural network that can be trained to execute complex functions at the speed of light. Science, this issue p. 1004 All-optical deep learning can be implemented with 3D-printed passive optical components. Deep learning has been transforming our ability to execute advanced inference tasks using computers. Here we introduce a physical mechanism to perform machine learning by demonstrating an all-optical diffractive deep neural network (D2NN) architecture that can implement various functions following the deep learning–based design of passive diffractive layers that work collectively. We created 3D-printed D2NNs that implement classification of images of handwritten digits and fashion products, as well as the function of an imaging lens at a terahertz spectrum. Our all-optical deep learning framework can perform, at the speed of light, various complex functions that computer-based neural networks can execute; will find applications in all-optical image analysis, feature detection, and object classification; and will also enable new camera designs and optical components that perform distinctive tasks using D2NNs.",
    "link": "https://www.semanticscholar.org/paper/5c7e5248d9eb7f373f10277410bf8506160907ea",
    "published": "2018-04-14"
  },
  "185": {
    "pdf_path": "data/pdfs/machine learning_paper_252.pdf",
    "text_excerpt": "arXiv:1811.06128v2  [cs.LG]  12 Mar 2020Machine Learning for Combinatorial Optimization:\na Methodological Tour d’Horizon∗\nYoshua Bengio2,3, Andrea Lodi1,3, and Antoine Prouvost1,3\nyoshua.bengio@mila.quebec\n{andrea.lodi, antoine.prouvost }@polymtl.ca\n1Canada Excellence Research Chair in Data Science for Decision\nMaking, ´Ecole Polytechnique de Montr´ eal\n2Department of Computer Science and Operations Research,\nUniversit´ e de Montr´ eal\n3Mila, Quebec Artiﬁcial Intelligence Institute\nAbstract\nThis paper surveys the recent attempts, both from the machine\nlearning and operations research communities, at leveraging machin e\nlearning to solve combinatorial optimization problems. Given the hard\nnature of these problems, state-of-the-art algorithms rely on h and-\ncraftedheuristicsformakingdecisionsthat areotherwisetooexp ensive\nto compute or mathematically not well deﬁned. Thus, machine learn-\ning looks like a natural candidate to make such decisions in a more\nprincipled and optimized way. We advocate for pushing further the\nintegration of machine learning and combinatorial optimization and\ndetail a methodology to do so. A main point of the paper is seeing\ngeneric optimization problems as data points and inquiring what is\nthe relevant distribution of problems to use for learning on a given\ntask.\n1 Introduction\nOperations research, also referred to as prescriptive anal ytics, started in the\nsecond world war as an initiative to use mathematics and comp uter science\n∗Accepted to the European Journal of Operations Research. c/circlecopyrt2020. Licensed under\nthe Creative Commons cbnd\n1to assist military planners in their decisions (Fortun and S chweber, 1993).\nNowadays, it is widely used in the industry, including but no t limited to\ntransportation, supply chain, energy, ﬁnance, and schedul ing. In this pa-\nper, we focus on discrete optimization problems formulated as integer con-\nstrained optimization, i.e., with integral or binary variables (called decision\nvariables). While ",
    "title": "Machine Learning for Combinatorial Optimization: a Methodological Tour d'Horizon",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/3f13a5148f7caa51ea946193d261d4f8ed32d81a",
    "published": "2018-11-15"
  },
  "186": {
    "pdf_path": "data/pdfs/Machine Learning for Clinical Predictive Analytics_paper_5.pdf",
    "text_excerpt": " Corresponding author : Sydney Anuyah  \nCopyright ©  2024 Author(s) retain the copyright of this article.  This  article  is published  under  the terms  of the Creative Commons Attribution Liscense 4.0 . \nAdvancing clinical trial outcomes using deep learning and predictive modelling: \nbridging precision medicine and patient -centered care  \nSydney Anuyah  1, *, Mallika K Singh  2 and Hope Nyavor  3\n1 PhD Student, Luddy School of Informatics, Computing  and Engineering, IU Indianapolis, USA . \n2 Public Health Researcher, New York Medical College, New York, USA . \n3 Department of Nanoscience and Nano -Engineering, University of North Carolina  Greensboro,  USA . \nWorld Journal of Advanced Research and Reviews , 202 4, 24(03), 001 –025  \nPublication history: Received on 22 October 2024; revised on 30 November 2024; accepted on 02 December 2024  \nArticle DOI: https://doi.org/10.30574/wjarr.2024.24.3.3671  \nAbstract  \nThe integration of artificial intelligence [AI] into clinical trials has revolutionized the process of drug development and \npersonalized medicine. Among these advancements, deep learning and predictive modelling have emerged as \ntransformative tools for opt imizing clinical trial design, patient recruitment, and real -time monitoring. This study \nexplores the application of deep learning techniques, such as convolutional neural networks [CNNs] and transformer -\nbased models, to stratify patients, forecast adverse  events, and personalize treatment plans. Furthermore, predictive \nmodelling approaches, including survival analysis and time -series forecasting, are employed to predict trial outcomes, \nenhancing efficiency and reducing trial failure rates. To address chall enges in analysing unstructured clinical data, such \nas patient notes and trial protocols, natural language processing [NLP] techniques are utilized for extracting actionable \ninsights. A custom dataset comprising structured patient demographics, genomic dat a, and unstructured text is cura",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "187": {
    "pdf_path": "data/pdfs/machine learning_paper_53.pdf",
    "text_excerpt": "ML-Schema: Exposing the Semantics of Machine\nLearning with Schemas and Ontologies\nGustavo Correa Publio\nAKSW Group\nUniversity of Leipzig, GermanyDiego Esteves\nSDA Research\nUniversity of Bonn, Germany\nAgnieszka Ławrynowicz\nPoznan University of Technology\nPolandPanˇce Panov\nJožef Stefan Institute\nLjubljana, SloveniaLarisa Soldatova\nBrunel University, UK\nTommaso Soru\nAKSW Group\nUniversity of Leipzig, GermanyJoaquin Vanschoren\nEindhoven\nUniversity of Technology\nThe NetherlandsHamid Zafar\nSDA Research\nUniversity of Bonn, Germany\nAbstract\nThe ML-Schema, proposed by the W3C Machine Learning Schema Community\nGroup, is a top-level ontology that provides a set of classes, properties, and re-\nstrictions for representing and interchanging information on machine learning\nalgorithms, datasets, and experiments. It can be easily extended and specialized\nand it is also mapped to other more domain-speciﬁc ontologies developed in the\narea of machine learning and data mining. In this paper we overview existing\nstate-of-the-art machine learning interchange formats and present the ﬁrst release\nof ML-Schema, a canonical format resulted of more than seven years of experience\namong different research institutions. We argue that exposing semantics of machine\nlearning algorithms, models, and experiments through a canonical format may pave\nthe way to better interpretability and to realistically achieve the full interoperability\nof experiments regardless of platform or adopted workﬂow solution.\n1 Introduction\nComplex machine learning models have recently achieved great successes in many predictive tasks.\nDespite their successes, a major problem is that they are often hard to interpret, which may affect\ntheir safeness and the level of trust of their users. The problem of interpretability is one of the key\nresearch issues in the area of knowledge engineering and the Semantic Web community, which deals\nwith making the semantics of various phenomena explicit. In this community, the problem of deali",
    "title": "ML-Schema: Exposing the Semantics of Machine Learning with Schemas and\n  Ontologies",
    "abstract": "The ML-Schema, proposed by the W3C Machine Learning Schema Community Group,\nis a top-level ontology that provides a set of classes, properties, and\nrestrictions for representing and interchanging information on machine learning\nalgorithms, datasets, and experiments. It can be easily extended and\nspecialized and it is also mapped to other more domain-specific ontologies\ndeveloped in the area of machine learning and data mining. In this paper we\noverview existing state-of-the-art machine learning interchange formats and\npresent the first release of ML-Schema, a canonical format resulted of more\nthan seven years of experience among different research institutions. We argue\nthat exposing semantics of machine learning algorithms, models, and experiments\nthrough a canonical format may pave the way to better interpretability and to\nrealistically achieve the full interoperability of experiments regardless of\nplatform or adopted workflow solution.",
    "link": "http://arxiv.org/abs/1807.05351v1",
    "published": "2018-07-14T08:07:31Z"
  },
  "188": {
    "pdf_path": "data/pdfs/machine learning_paper_668.pdf",
    "text_excerpt": "Machine Learning 35, 191–192 (1999)\nc°1999 Kluwer Academic Publishers. Manufactured in The Netherlands.\nIntroducing the Special Issue of Machine\nLearning Selected from Papers Presentedat the 1997 Conference on ComputationalLearning Theory, COLT’97\nJOHN SHAWE-TAYLOR\nRoyal Holloway, University of London\nThe aim of the COLT special issues is to provide a forum for COLT conference papers\nwhich though theoretical have been judged to have signiﬁcant practical ramiﬁcations andhence to be of potential interest to a broader spectrum of Machine Learning readers.\nThe papers included in this special issue cover a broad range of topics, their practical\nsigniﬁcancebeinginsomecasesoftheform‘howtodoit’,whileinothersitisoftheform‘this could be hard’. The paper most clearly aiming at a particular practical application isbyBlumandKalaientitled,‘Universalportfolioswithandwithouttransactioncosts’. Thepaper considers the problem of rebalancing an investment portfolio on-line in such a waythat the result is competitive with the best constant rebalancing determined in hindsight.They extend Cover’s universal algorithm to take into account transaction costs, and at thesametimegivearandomizedimplementationwhichissigniﬁcantlyfasterinpractice. Otherapplications of the approach are also described.\nDalmau’s paper ‘A dichotomy theorem for learning quantiﬁed Boolean formulas’ is in\nthe ‘this could be hard’ category. It studies the question of learning quantiﬁed Booleanformulasconstructedfromaﬁnitesetofbasicfunctionsusingconjunctions,constantsandﬁnally existential or universal quantiﬁcation. He shows how the sets of basic functionscan be identiﬁed as being in one of two categories. For sets in the ﬁrst category learningis ‘easy’ and can be achieved by known algorithms and techniques, while for sets in thesecond category learning is not possible under standard cryptographic assumptions.\nThepaper‘Estimationoftime-varyingparametersinstatisticalmodels;anoptimization\napproach’ by Bertsimas, Gamarnik and ",
    "title": "Introducing the Special Issue of Machine Learning Selected from Papers Presented at the 1997 Conference on Computational Learning Theory, COLT '97",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1007540111909",
    "published": "2002-12-22T05:04:10Z"
  },
  "189": {
    "pdf_path": "data/pdfs/Machine Learning for Clinical Predictive Analytics_paper_12.pdf",
    "text_excerpt": "Disparate Model Performance and Stability in Machine Learning Clinical\nSupport for Diabetes and Heart Diseases\nIoannis Bilionis, MSc1,2, Ricardo C. Berrios1, Luis Fernandez-Luque, PhD1,\nCarlos Castillo, PhD2,3\n1Adhera Health, Santa Cruz, USA;2Universitat Pompeu Fabra, Barcelona, Spain;\n3ICREA, Catalonia, Spain\nAbstract\nMachine Learning (ML) algorithms are vital for supporting clinical decision-making in biomedical informatics. How-\never, their predictive performance can vary across demographic groups, often due to the underrepresentation of his-\ntorically marginalized populations in training datasets. The investigation reveals widespread sex- and age-related\ninequities in chronic disease datasets and their derived ML models. Thus, a novel analytical framework is introduced,\ncombining systematic arbitrariness with traditional metrics like accuracy and data complexity. The analysis of data\nfrom over 25,000 individuals with chronic diseases revealed mild sex-related disparities, favoring predictive accuracy\nfor males, and significant age-related differences, with better accuracy for younger patients. Notably, older patients\nshowed inconsistent predictive accuracy across seven datasets, linked to higher data complexity and lower model per-\nformance. This highlights that representativeness in training data alone does not guarantee equitable outcomes, and\nmodel arbitrariness must be addressed before deploying models in clinical settings.\nIntroduction\nIn the realm of biomedicine, Artificial Intelligence (AI) methodologies, particularly Machine Learning (ML) models,\nare used as clinical support tools to systematically discern patterns and interdependencies among factors and outcomes\nwithin large datasets. ML has the potential to enhance healthcare provision by complementing, rather than supplant-\ning, clinical judgment. It has demonstrated efficacy in the detection of skin cancer and diabetic retinopathy, among\nmany other medical conditions [1, 2]. A paramount objective whe",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "190": {
    "pdf_path": "data/pdfs/machine learning_paper_475.pdf",
    "text_excerpt": "Machine Learning, 34, 5–9 (1999)\nc°1999 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.\nGuest Editors’ Introduction:\nMachine Learning and Natural Language\nCLAIRE CARDIE cardie@cs.cornell.edu\nDepartment of Computer Science, Cornell University, Ithaca, NY 14853-7501\nRAYMOND J. MOONEY mooney@cs.utexas.edu\nDepartment of Computer Sciences, Taylor Hall 2.124, University of Texas, Austin, TX 78712-1188\nThe application of machine learning techniques to natural language processing (NLP) has\nincreased dramatically in recent years under the name of “corpus-based,” “statistical,” or“empirical” methods. However, most of this research has been conducted outside thetraditionalmachinelearningresearchcommunity. Thisspecialissueattemptstobridgethisdivide by assembling an interesting variety of recent research papers on various aspectsof natural language learning – many from authors who do not generally publish in thetraditional machine learning literature – and presenting them to the readers of MachineLearning.\nInthelastﬁvetotenyearstherehasbeenadramaticshiftincomputationallinguisticsfrom\nmanuallyconstructinggrammarsandknowledgebasestopartiallyortotallyautomatingthisprocessbyusingstatisticallearningmethodstrainedonlargeannotatedorunannotatednat-ural language corpora. The success of statistical methods in speech recognition (Stolcke,1997;Jelinek,1998)hasbeenparticularlyinﬂuentialinmotivatingtheapplicationofsimilarmethods to other aspects of natural language processing. There is now a variety of workon applying learning methods to almost all other aspects of language processing as well(Brill&Mooney,1997),includingmorphologicalandsyntacticanalysis(Charniak,1997),semantic disambiguation and interpretation (Ng & Zelle, 1997), discourse processing andinformationextraction(Cardie,1997),andmachinetranslation(Knight,1997). Somecon-cretepublicationstatisticsclearlyillustratetheextentoftherevolutioninnaturallanguageresearch. According to data recently collected by Hirschber",
    "title": "Guest Editors' Introduction: Machine Learning and Natural Language",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1007580931600",
    "published": "2002-12-22T05:04:10Z"
  },
  "191": {
    "pdf_path": "data/pdfs/machine learning_paper_396.pdf",
    "text_excerpt": "Machin e Learnin g 2: 281-284 , 1988\n© 198 8 Kluwe r Academi c Publishers , Bosto n - Manufacture d in The Netherland s\nGUES T EDITORIA L\nNew Theoretica l Direction s in Machin e Learnin g\nLookin g back over the last decad e of work in machin e learning , one thin g\nis apparent : ther e is an art to designin g efficien t and effectiv e learnin g\nalgorithms . Too muc h of an art. Man y researcher s have argue d that if the\nfield is ever to fully mature , we must find a soun d and adequat e theoretica l\nbasis on whic h to buil d a scienc e of machin e learning .\nA learnin g algorith m is an algorithm . Althoug h it woul d be unwis e to\nsubsum e machin e learnin g entirel y unde r the genera l theor y of algorithm s\nand computationa l complexity , the technique s and perspective s from this\nfield are pertinen t to developin g a theoretica l basi s for machin e learning .\nThis specia l issu e of Machine  Learning  present s som e recen t progres s in\napplyin g thes e technique s and perspective s to the computationa l problem s\ninheren t in learning .\nEarly effort s alon g thes e line s were base d primaril y on the framewor k\nintroduce d by Gol d (1967) . Thi s earl y wor k is reviewe d in the excellen t\nsurve y by Anglui n and Smit h (1983) . Mor e recently , Valian t (1984 ) has\nintroduce d a probabilisti c framewor k for the stud y of learnin g algorithms .\nKearns , Li, Pitt , and Valian t (1987 ) and Haussle r (1987 ) hav e surveye d\napplication s of this framewor k to AI concept-learnin g problems ; Rives t\n(1987 ) has also carrie d out recen t wor k in this paradigm .\nThe Valian t framewor k has been mor e successfu l than the Gol d frame -\nwork in addressin g som e of the requirement s that are typicall y place d on a\nlearnin g algorith m in practice . To allow greate r emphasi s on computationa l\nefficiency , it require s only that a good approximatio n to the targe t concep t\nbe foun d with high probability , rathe r than requirin g exac t identificatio n\nof t",
    "title": "New Theoretical Directions In Machine Learning",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022817027844",
    "published": "2003-04-04T16:57:10Z"
  },
  "192": {
    "pdf_path": "data/pdfs/Machine Learning for Clinical Predictive Analytics_paper_15.pdf",
    "text_excerpt": "Explainable AI for Mental Health Emergency Returns: Integrating LLMs with Predictive Modeling  Abdulaziz Ahmed1, 2*, Mohammad Saleem1, Mohammed Alzeen1, Badari Birur3, Rachel E Fargason3, Bradley G Burk3,4,  Ahmed Alhassan3, Mohammed Ali Al-Garadi5 1Department of Health Services Administration, School of Health Professions, University of Alabama at Birmingham, Birmingham, AL United States. 2Department of Biomedical Informatics and Data Science, Heersink School of Medicine, University of Alabama at Birmingham, Birmingham, AL 35233, USA 3Department of Psychiatry and Behavioral Neurobiology, University of Alabama at Birmingham, Birmingham, AL, United States 4Department of Pharmacy, University of Alabama at Birmingham, Birmingham, AL, United States 5Department of Biomedical Informatics, School of Medicine, Vanderbilt University Medical Center, Nashville, TN,  United States  ABSTRACT Importance: Emergency department (ED) returns for mental health conditions represent a significant healthcare burden. Traditional machine learning (ML) models for predicting these returns often lack interpretability for clinical implementation. Objective: To evaluate whether integrating large language models (LLMs) with traditional ML approaches improves both the predictive accuracy and clinical interpretability of ED mental health returns models. Methods: This retrospective cohort study analyzed 42,464 ED visits for 27,904 unique mental health patients at an Academic Medical Center in the deep South of the U.S. between January 2018 and December 2022.  Main Outcomes and Measures: Two outcomes were evaluated: (1) 30-day ED return prediction accuracy and (2) model interpretability through a novel LLM-enhanced explainability framework integrating SHAP (SHapley Additive exPlanations) values with contextual clinical knowledge. Results: For chief complaint classification, Llama 3 (8B) with 10-shot learning outperformed traditional models, achieving 0.882 accuracy and 0.86 F1-score. In SDoH classif",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "193": {
    "pdf_path": "data/pdfs/machine learning_paper_374.pdf",
    "text_excerpt": "iml: An R package for Interpretable Machine Learning\nChristoph Molnar1, Giuseppe Casalicchio1, and Bernd Bischl1\n1Department of Statistics, LMU Munich DOI: 10.21105/joss.00786\nSoftware\n• Review\n• Repository\n• Archive\nSubmitted: 19 June 2018\nPublished: 27 June 2018\nLicense\nAuthors of papers retain copyright\nand release the work under a Cre-\native Commons Attribution 4.0 In-\nternational License ( CC-BY ).Summary\nComplex, non-parametric models, which are typically used in machine learning, have\nproven to be successful in many prediction tasks. But these models usually operate as\nblack boxes: While they are good at predicting, they are often not interpretable. Many\ninherently interpretable models have been suggested, which come at the cost of losing\npredictive power. Another option is to apply interpretability methods to a black box\nmodel after model training. Given the velocity of research on new machine learning\nmodels, it is preferable to have model-agnostic tools which can be applied to a random\nforest as well as to a neural network. T ools for model-agnostic interpretability methods\nshould improve the adoption of machine learning.\niml is an R package (R Core T eam 2016) that offers a general toolbox for making machine\nlearning models interpretable. It implements many model-agnostic methods which work\nfor any type of machine learning model. The package covers following methods:\n• Partial dependence plots (F riedman 2001): Visualizing the learned relationship be-\ntween features and predictions.\n• Individual conditional expectation (Goldstein et al. 2015): Visualizing the learned\nrelationship between features and predictions for individual instances of the data.\n• F eature importance (Fisher, Rudin, and Dominici 2018): Scoring features by con-\ntribution to predictive performance.\n• Global surrogate tree: Approximating the black box model with an interpretable\ndecision tree.\n• Local surrogate models (Ribeiro, Singh, and Guestrin 2016): Explaining single pre-\ndictions b",
    "title": "iml: An R package for Interpretable Machine Learning",
    "abstract": "Complex, non-parametric models, which are typically used in machine learning, have proven to be successful in many prediction tasks. But these models usually operate as black boxes: While they are good at predicting, they are often not interpretable. Many inherently interpretable models have been suggested, which come at the cost of losing predictive power. Another option is to apply interpretability methods to a black box model after model training. Given the velocity of research on new machine learning models, it is preferable to have model-agnostic tools which can be applied to a random forest as well as to a neural network. Tools for model-agnostic interpretability methods should improve the adoption of machine learning.",
    "link": "https://www.semanticscholar.org/paper/ea7887fadc666d6faf92e569d4a10d994ee91297",
    "published": "2018-06-27"
  },
  "194": {
    "pdf_path": "data/pdfs/machine learning_paper_328.pdf",
    "text_excerpt": "ARTICLE\nMachine-learning reprogrammable metasurface\nimager\nLianlin Li1, Hengxin Ruan1, Che Liu2, Ying Li3, Ya Shuang1, Andrea Alù4,5,6, Cheng-Wei Qiu3&\nTie Jun Cui2\nConventional microwave imagers usually require either time-consuming data acquisition, or\ncomplicated reconstruction algorithms for data post-processing, making them largely inef-fective for complex in-situ sensing and monitoring. Here, we experimentally report a real-timedigital-metasurface imager that can be trained in-situ to generate the radiation patternsrequired by machine-learning optimized measurement modes. This imager is electronicallyreprogrammed in real time to access the optimized solution for an entire data set, realizingstorage and transfer of full-resolution raw data in dynamically varying scenes. High-accuracyimage coding and recognition are demonstrated in situ for various image sets, including hand-written digits and through-wall body gestures, using a single physical hardware imager,reprogrammed in real time. Our electronically controlled metasurface imager opens newvenues for intelligent surveillance, fast data acquisition and processing, imaging at variousfrequencies, and beyond.https://doi.org/10.1038/s41467-019-09103-2 OPEN\n1State Key Laboratory of Advanced Optical Communication Systems and Networks, Department of Electronics, Peking University, 100871 Beijing, China .\n2State Key Laboratory of Millimeter Waves, Southeast University, 210096 Nanjing, China.3Department of Electrical and Computer Engineering, National\nUniversity of Singapore, 4 Engineering Drive 3, Singapore 117583, Singapore.4Photonics Initiative, Advanced Science Research Center, City University of New\nYork, 85 St. Nicholas Terrace, New York, NY 10031, USA.5Physics Program, The Graduate Center, City University of New York, 365 Fifth Avenue, New York,\nNY 10016, USA.6Department of Electrical Engineering, City College of New York, New York, NY 10031, USA. These authors contributed equally: Lianlin Li,\nHengxin Ruan, Che",
    "title": "Machine-learning reprogrammable metasurface imager",
    "abstract": "Conventional microwave imagers usually require either time-consuming data acquisition, or complicated reconstruction algorithms for data post-processing, making them largely ineffective for complex in-situ sensing and monitoring. Here, we experimentally report a real-time digital-metasurface imager that can be trained in-situ to generate the radiation patterns required by machine-learning optimized measurement modes. This imager is electronically reprogrammed in real time to access the optimized solution for an entire data set, realizing storage and transfer of full-resolution raw data in dynamically varying scenes. High-accuracy image coding and recognition are demonstrated in situ for various image sets, including hand-written digits and through-wall body gestures, using a single physical hardware imager, reprogrammed in real time. Our electronically controlled metasurface imager opens new venues for intelligent surveillance, fast data acquisition and processing, imaging at various frequencies, and beyond. Conventional imagers require time-consuming data acquisition, or complicated reconstruction algorithms for data post-processing. Here, the authors demonstrate a real-time digital-metasurface imager that can be trained in-situ to show high accuracy image coding and recognition for various image sets.",
    "link": "https://www.semanticscholar.org/paper/4f2b9cb774489c1a600c224c75edb8da07a24064",
    "published": "2019-03-06"
  },
  "195": {
    "pdf_path": "data/pdfs/machine learning_paper_358.pdf",
    "text_excerpt": "Delayed Impact of Fair Machine Learning\u0003\nLydia T. Liu ,Sarah Dean ,Esther Rolf ,Max Simchowitz and Moritz Hardt\nDepartment of Electrical Engineering and Computer Sciences, University of California at Berkeley\nflydiatliu, dean sarah, esther rolf, msimchow, hardtg@berkeley.edu\nAbstract\nStatic classiﬁcation has been the predominant focus\nof the study of fairness in machine learning. While\nmost models do not consider how decisions change\npopulations over time, it is conventional wisdom\nthat fairness criteria promote the long-term well-\nbeing of groups they aim to protect. This work\nstudies the interaction of static fairness criteria with\ntemporal indicators of well-being. We show a sim-\nple one-step feedback model in which common cri-\nteria do not generally promote improvement over\ntime, and may in fact cause harm. Our results high-\nlight the importance of temporal modeling in the\nevaluation of fairness criteria, suggesting a range\nof new challenges and trade-offs.\n1 Introduction\nMachine learning commonly considers static objectives de-\nﬁned on a snapshot of the population at one instant in time;\nconsequential decisions, in contrast, reshape the population\nover time. Lending practices, for example, can shift the dis-\ntribution of debt and wealth in the population. Job adver-\ntisements allocate opportunity. School admissions shape the\nlevel of education in a community.\nExisting scholarship on fairness in automated decision-\nmaking criticizes unconstrained machine learning for its po-\ntential to harm historically underrepresented or disadvan-\ntaged groups in the population [Executive Ofﬁce of the Presi-\ndent, 2016; Barocas and Selbst, 2016 ]. Consequently, a vari-\nety of fairness criteria have been proposed as constraints on\nstandard learning objectives. Even though, in each case, these\nconstraints are clearly intended to protect the disadvantaged\ngroup by an appeal to intuition, a rigorous argument to that\neffect is often lacking.\nIn this work, we formally examine under ",
    "title": "Delayed Impact of Fair Machine Learning",
    "abstract": "Static classification has been the predominant focus of the study of fairness in machine learning. While most models do not consider how decisions change populations over time, it is conventional wisdom that fairness criteria promote the long-term well-being of groups they aim to protect. This work studies the interaction of static fairness criteria with temporal indicators of well-being. We show a simple one-step feedback model in which common criteria do not generally promote improvement over time, and may in fact cause harm. Our results highlight the importance of temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.",
    "link": "https://www.semanticscholar.org/paper/4f2baff3195b6fc43a38e3e869496dab9fe9dbc3",
    "published": "2018-03-12"
  },
  "196": {
    "pdf_path": "data/pdfs/machine learning_paper_193.pdf",
    "text_excerpt": "Articles\nhttps:/ / doi.org/10.1038/s42256-020-00287-71Department of Methodology and Statistics, Faculty of Social and Behavioral Sciences, Utrecht University, Utrecht, the Netherlands. 2Department of \nResearch and Data Management Services, Information T echnology Services, Utrecht University, Utrecht, the Netherlands. 3Utrecht University Library, \nUtrecht University, Utrecht, the Netherlands. 4Department of T est and Quality Services, Information T echnology Services, Utrecht University, Utrecht, the \nNetherlands. 5School of Governance, Faculty of Law, Economics and Governance, Utrecht University, Utrecht, the Netherlands. 6Department of Biostatistics, \nData management and Data Science, Julius Center, University Medical Center Utrecht, Utrecht, the Netherlands. ✉e-mail: a.g.j.vandeschoot@uu.nlWith the emergence of online publishing, the number \nof scientific manuscripts on many topics is skyrocket -\ning1. All of these textual data present opportunities to \nscholars and practitioners while simultaneously confronting them with new challenges. Scholars often develop systematic reviews and \nmeta-analyses to develop comprehensive overviews of the relevant \ntopics\n2. The process entails several explicit and, ideally, reproduc -\nible steps, including identifying all likely relevant publications in a standardized way, extracting data from eligible studies and synthe\n-\nsizing the results. Systematic reviews differ from traditional litera -\nture reviews in that they are more replicable and transparent3,4. Such \nsystematic overviews of literature on a specific topic are pivotal not only for scholars, but also for clinicians, policy-makers, journalists \nand, ultimately, the general public\n5–7.\nGiven that screening the entire research literature on a given \ntopic is too labour intensive, scholars often develop quite narrow searches. Developing a search strategy for a systematic review is \nan iterative process aimed at balancing recall and precision\n8,9; that \nis, including as ma",
    "title": "An open source machine learning framework for efficient and transparent systematic reviews",
    "abstract": "To help researchers conduct a systematic review or meta-analysis as efficiently and transparently as possible, we designed a tool to accelerate the step of screening titles and abstracts. For many tasks—including but not limited to systematic reviews and meta-analyses—the scientific literature needs to be checked systematically. Scholars and practitioners currently screen thousands of studies by hand to determine which studies to include in their review or meta-analysis. This is error prone and inefficient because of extremely imbalanced data: only a fraction of the screened studies is relevant. The future of systematic reviewing will be an interaction with machine learning algorithms to deal with the enormous increase of available text. We therefore developed an open source machine learning-aided pipeline applying active learning: ASReview. We demonstrate by means of simulation studies that active learning can yield far more efficient reviewing than manual reviewing while providing high quality. Furthermore, we describe the options of the free and open source research software and present the results from user experience tests. We invite the community to contribute to open source projects such as our own that provide measurable and reproducible improvements over current practice. It is a challenging task for any research field to screen the literature and determine what needs to be included in a systematic review in a transparent way. A new open source machine learning framework called ASReview, which employs active learning and offers a range of machine learning models, can check the literature efficiently and systemically.",
    "link": "https://www.semanticscholar.org/paper/223846b7b56e250e1b6f521997b4c1b809cc0da7",
    "published": "2020-06-22"
  },
  "197": {
    "pdf_path": "data/pdfs/machine learning_paper_134.pdf",
    "text_excerpt": " \n1 A guide to machine learning for biologists  \n \nJoe G. Greener*  \nDept of Computer Science, University College London, Gower Street, London WC1E 6BT, \nUnited Kingdom  \nj.greener@ucl.ac.uk  \n \nShaun M. Kandathil*  \nDept of Computer Science, University College London, Gower Street, London WC1E 6BT, \nUnited Kingdom  \ns.kandathil@ucl.ac.uk  \n \nLewis Moffat  \nDept of Computer Science, University College London, Gower Street, London WC1E 6BT, \nUnited Kingdom  \nlewis.moffat@cs.ucl.ac.uk  \n \nDavid T. Jones † \nDept of Computer Science , University College London, Gower Street, London WC1E 6BT, \nUnited Kingdom  \nd.t.jones@ucl.ac.uk  \n \n* These authors contributed equally  \n† Correspondence should be addressed to: d.t.jones@ucl.ac.uk  \n \n \n \nThe expanding scale and inherent complexity of biological data have encouraged a growing \nuse of machine learning in biology to build informative and predictive models of the \nunderlying biological processes. All machine learning techniques fit models to dat a, however, \nthe specific methodologies are quite varied and can at first glance seem bewildering. In this \narticle, we aim to provide readers with a gentle introduction to a few key machine learning \ntechniques, including those based on the most recently dev eloped and widely used techniques \ninvolving deep neural networks. We describe how different techniques may be suited to \nspecific types of biological data, and also discuss some best practices and points to consider \nwhen embarking on experiments involving m achine learning. Some emerging directions in \nmachine learning methodology will also be discussed.  \n   \n2 Introduction  \n \nHumans make sense of the world around them by observing it, and  learning to predict what might \nhappen next. Consider a child learning to catch a ball: the child (usually) knows nothing about the \nphysical laws that govern the motion of a thrown ball, yet, by a process of observation, trial and \nerror, the child adjusts their understanding of the bal",
    "title": "A guide to machine learning for biologists",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/80d9f0eb47b712988d19cbe29a7bfa63f2a175d0",
    "published": "2021-09-13"
  },
  "198": {
    "pdf_path": "data/pdfs/machine learning_paper_294.pdf",
    "text_excerpt": "ZK[KF ZIN FZ\\OIS K\nTkmstzo wok~ztzr kwr{~ttsy vkwtnktt{z wtts k\nwtytton €ky|wo €tzo\nFzn~t�� ^klkwk� OJ\n4*.Kyyk M{�oz5.Kwwoz W{wtkv{qq5.Fwo�kzno~ Q1Ik��{z4\n4Tk�o~tkw �.Jo�tmo� kzn[���oy� Jt�t�t{z. [ms{{w {qKwom�~t mkwkznKwom�~{ztm Kzrtzoo~tzr .\\so ]zt�o~�t�� {q\nTkzmso� �o~.Tkzmso��o~. Kzrwkzn. ]zt�on Rtzrn{y .5[ms{{w {qGt{w{rtm kw[mtozmo�. \\so ]zt�o~�t�� {q\nTkzmso� �o~.Tkzmso��o~. Kzrwkzn. ]zt�on Rtzrn{y\n*kzn~t�� 1�klkwk�Ey kzmso��o~ 1km1�v\nFl€t~kmt\nFn�kzmo� tzzo�~{tykrtzr. roz{ytm. y{�t{z �~kmvtzr. o�o/�~kmvtzr kznykz� {�so~ �omsz{w/\n{r�/lk�on nk�k m{wwom�t{z yo�s{n� sk�o won�{k�{~~oz� {qstrs ntyoz�t{zkw nk�k�o��. �stms\nm{yy{zw� sk�o k�ykww z�ylo~ {q�ky|wo� lomk��o {q�sotz�~tz�tm strs m{�� {qnk�k m{wwom/\n�t{ztz�{w�tzr s�ykz |k~�tmt|kz��1 Ntrs ntyoz�t{z kwnk�k �t�s k�ykww z�ylo~ {q�ky|wo� t�\n{qm~t�tmkw ty|{~�kzmo q{~tnoz�tq�tzr lt{yk~vo~� kznm{zn�m�tzr qok�tltwt�� kzn|tw{� �{~v.\ns{�o�o~ t�mkzwokn �{ltk�on ykmstzo wok~ztzr *TS+ |o~q{~ykzmo o��tyk�o�1 V�~ ~o�to� {q\n���nto� �stms sk�o k||wton TS�{|~ontm� k��t��tm q~{y z{z/k��t��tm tznt�tn�kw� �s{�on �sk�\n�ykww �ky|wo �t�o t�k��{mtk�on �t�s strso~ ~o|{~�on mwk��tqtmk�t{z kmm�~km�1 \\s��. �osk�o\ntz�o��trk�on �so�so~ �st�ltk� m{�wn lomk��on l��so��o{q�kwtnk�t{z yo�s{n� �stms n{\nz{���qqtmtoz�w� m{z�~{w {�o~qt��tzr1 V�~�ty�wk�t{z� �s{� �sk�R/q{wn I~{��/^kwtnk�t{z *I^+\n|~{n�mo� ��~{zrw� ltk�on |o~q{~ykzmo o��tyk�o� �t�s �ykww �ky|wo �t�o�. kzn�soltk� t�\n��twwo�tnoz� �t�s �ky|wo �t�o {q43331 Uo��on I^kzn�~ktz2�o�� �|wt� k||~{kmso �|~{n�mo\n~{l��� kzn�zltk�on |o~q{~ykzmo o��tyk�o� ~ork~nwo�� {q�ky|wo �t�o1 _okw�{ �s{� �sk�\nqok��~o �owom�t{z tq|o~q{~yon {z|{{won �~ktztzr kzn�o��tzr nk�k t�m{z�~tl��tzr �{ltk� m{z/\n�tno~klw� y{~o �skz |k~kyo�o~ ��ztzr1 Ozknnt�t{z. �som{z�~tl��t{z �{ltk� l�nk�k\nntyoz�t{zkwt��. s�|o~/|k~kyo�o~ �|kmo kznz�ylo~ {qI^q{wn� �k� o�|w{~on. kzn�kwtnk/\n�t{zyo�s{n� �o~o m{y|k~on �t�s nt�m~tytzklwo nk�k1 \\so ~o��w�� ��rro�� s{� �{no�trz\n~{l��� �o��tzr yo�s{n{w{r to��soz �{~vtzr �t�s �ykww nk�k�o�� kzns{� �{tz�o~",
    "title": "Machine learning algorithm validation with a limited sample size",
    "abstract": "Advances in neuroimaging, genomic, motion tracking, eye-tracking and many other technology-based data collection methods have led to a torrent of high dimensional datasets, which commonly have a small number of samples because of the intrinsic high cost of data collection involving human participants. High dimensional data with a small number of samples is of critical importance for identifying biomarkers and conducting feasibility and pilot work, however it can lead to biased machine learning (ML) performance estimates. Our review of studies which have applied ML to predict autistic from non-autistic individuals showed that small sample size is associated with higher reported classification accuracy. Thus, we have investigated whether this bias could be caused by the use of validation methods which do not sufficiently control overfitting. Our simulations show that K-fold Cross-Validation (CV) produces strongly biased performance estimates with small sample sizes, and the bias is still evident with sample size of 1000. Nested CV and train/test split approaches produce robust and unbiased performance estimates regardless of sample size. We also show that feature selection if performed on pooled training and testing data is contributing to bias considerably more than parameter tuning. In addition, the contribution to bias by data dimensionality, hyper-parameter space and number of CV folds was explored, and validation methods were compared with discriminable data. The results suggest how to design robust testing methodologies when working with small datasets and how to interpret the results of other studies based on what validation method was used.",
    "link": "https://www.semanticscholar.org/paper/fbf9812f29156024ec693b4633a21303eead309d",
    "published": "2019-11-07"
  },
  "199": {
    "pdf_path": "data/pdfs/machine learning_paper_44.pdf",
    "text_excerpt": "Model-Agnostic Interpretability of Machine Learning\nMarco Tulio Ribeiro MARCOTCR @CS.UW.EDU\nSameer Singh SAMEER @CS.UW.EDU\nCarlos Guestrin GUESTRIN @CS.UW.EDU\nUniversity of Washington Seattle, WA 98195 USA\nAbstract\nUnderstanding why machine learning models\nbehave the way they do empowers both system\ndesigners and end-users in many ways: in model\nselection, feature engineering, in order to trust\nand act upon the predictions, and in more intuitive\nuser interfaces. Thus, interpretability has become\na vital concern in machine learning, and work\nin the area of interpretable models has found re-\nnewed interest. In some applications, such models\nare as accurate as non-interpretable ones, and thus\nare preferred for their transparency. Even when\nthey are not accurate, they may still be preferred\nwhen interpretability is of paramount importance.\nHowever, restricting machine learning to inter-\npretable models is often a severe limitation. In this\npaper we argue for explaining machine learning\npredictions using model-agnostic approaches. By\ntreating the machine learning models as black-\nbox functions, these approaches provide crucial\nﬂexibility in the choice of models, explanations,\nand representations, improving debugging, com-\nparison, and interfaces for a variety of users and\nmodels. We also outline the main challenges for\nsuch methods, and review a recently-introduced\nmodel-agnostic explanation approach (LIME) that\naddresses these challenges.\n1. Introduction\nAs machine learning becomes a crucial component of an\never-growing number of user-facing applications, inter-\npretable machine learning has become an increasingly\nimportant area of research for a number of reasons. First,\nas humans are the ones who train, deploy, and often use the\npredictions of machine learning models in the real world, it\nis of utmost importance for them to be able to trust the model.\n2016 ICML Workshop on Human Interpretability in Machine\nLearning (WHI 2016) , New York, NY , USA. Copyright by the\naut",
    "title": "Model-Agnostic Interpretability of Machine Learning",
    "abstract": "Understanding why machine learning models behave the way they do empowers\nboth system designers and end-users in many ways: in model selection, feature\nengineering, in order to trust and act upon the predictions, and in more\nintuitive user interfaces. Thus, interpretability has become a vital concern in\nmachine learning, and work in the area of interpretable models has found\nrenewed interest. In some applications, such models are as accurate as\nnon-interpretable ones, and thus are preferred for their transparency. Even\nwhen they are not accurate, they may still be preferred when interpretability\nis of paramount importance. However, restricting machine learning to\ninterpretable models is often a severe limitation. In this paper we argue for\nexplaining machine learning predictions using model-agnostic approaches. By\ntreating the machine learning models as black-box functions, these approaches\nprovide crucial flexibility in the choice of models, explanations, and\nrepresentations, improving debugging, comparison, and interfaces for a variety\nof users and models. We also outline the main challenges for such methods, and\nreview a recently-introduced model-agnostic explanation approach (LIME) that\naddresses these challenges.",
    "link": "http://arxiv.org/abs/1606.05386v1",
    "published": "2016-06-16T23:39:41Z"
  },
  "200": {
    "pdf_path": "data/pdfs/machine learning_paper_198.pdf",
    "text_excerpt": "HAL Id: hal-03243917\nhttps://hal.science/hal-03243917v1\nSubmitted on 31 May 2021\nHAL is a multi-disciplinary open access\narchive for the deposit and dissemination of sci-\nentific research documents, whether they are pub-\nlished or not. The documents may come from\nteaching and research institutions in F rance or\nabroad, or from public or private research centers.L’archive ouverte pluridisciplinaire HAL , est\ndestinée au dépôt et à la diffusion de documents\nscientifiques de niveau recherche, publiés ou non,\némanant des établissements d’enseignement et de\nrecherche français ou étrangers, des laboratoires\npublics ou privés.\nBest practices in machine learning for chemistry\nNongnuch Artrith, Keith T Butler, F rançois-Xavier Coudert, Seungwu Han,\nOlexandr Isayev, Anubhav Jain, Aron W alsh\nT o cite this version:\nNongnuch Artrith, Keith T Butler, F rançois-Xavier Coudert, Seungwu Han, Olexandr Isayev, et\nal.. Best practices in machine learning for chemistry . Nature Chemistry , 2021, 13, pp.505-508.\n￿10.1038/s41557-021-00716-z￿. ￿hal-03243917￿ \n 1Best practices in machine learning for chemistry \n \nNongnuch Artrith1,2, Keith T. Butler3, François-Xavier Coudert4, Seungwu Han5, Olexandr \nIsayev6,7, Anubhav Jain8, Aron Walsh9,10 \n \nAs statistical tools based on machine learning become integrated into chemistry research \nworkflows, we discuss the elements necessary  to train and report reliable, repeatable, and \nreproducible models. \n \nChemistry has long benefited fr om the power of applying model s to interpret patterns in \ndata. Standard relations range from the Eyring eq uation in chemical kinetics, the scales of \nelectronegativity to describe chemical stability an d reactivity, to the ligand-field approaches \nthat connect molecular structur e and spectroscopy. Such models are typically in the form of \nreproducible closed-form equations and remain relevant over the course of decades. \nHowever, the rules of chemistry are often limited to specific cl asses of systems (e.g. \nelec",
    "title": "Best practices in machine learning for chemistry",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/0c8c500cec9b74ebc7be44c52b79d2bd78234605",
    "published": "2021-05-31"
  },
  "201": {
    "pdf_path": "data/pdfs/Machine Learning for Clinical Predictive Analytics_paper_10.pdf",
    "text_excerpt": "Multiclass Disease Predictions Based on Integrated Clinical and Genomics Datasets\nMoeez M. Subhani\nCollege of Engineering and Technology\nUniversity of Derby\nDerby, England\nEmail: m.subhani@derby.ac.ukAshiq Anjum\nCollege of Engineering and Technology\nUniversity of Derby\nDerby, England\nEmail: a.anjum@derby.ac.uk\nAbstract —Clinical predictions using clinical data by computa-\ntional methods are common in bioinformatics. However, clinical\npredictions using information from genomics datasets as well is\nnot a frequently observed phenomenon in research. Precision\nmedicine research requires information from all available datasets\nto provide intelligent clinical solutions. In this paper, we have\nattempted to create a prediction model which uses information\nfrom both clinical and genomics datasets. We have demonstrated\nmulticlass disease predictions based on combined clinical and\ngenomics datasets using machine learning methods. We have\ncreated an integrated dataset, using a clinical (ClinVar) and a\ngenomics (gene expression) dataset, and trained it using instance-\nbased learner to predict clinical diseases. We have used an\ninnovative but simple way for multiclass classiﬁcation, where the\nnumber of output classes is as high as 75. We have used Principal\nComponent Analysis for feature selection. The classiﬁer predicted\ndiseases with 73% accuracy on the integrated dataset. The\nresults were consistent and competent when compared with other\nclassiﬁcation models. The results show that genomics information\ncan be reliably included in datasets for clinical predictions and\nit can prove to be valuable in clinical diagnostics and precision\nmedicine.\nKeywords –Clinical; Genomics; Data Integration; Machine\nLearning; Disease Prediction; Classiﬁcation; Bioinformatics.\nI. I NTRODUCTION\nThe medical science is rich with various types of datasets\nranging from clinical to genomics datasets. The clinical\ndatasets are diverse in terms of their nature, format and the\ninformation they contain. On th",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "202": {
    "pdf_path": "data/pdfs/machine learning_paper_54.pdf",
    "text_excerpt": "XXX-X-XXXX-XXXX-X/XX/$XX.00 ©20XX IEEE Characterizing machine learning process: A maturity framework Rama Akkiraju, Vibha Sinha, Anbang Xu, Jalal Mahmud, Pritam Gundecha, Zhe Liu, Xiaotong Liu, John Schumacher IBM Watson, IBM Almaden Research Center, San Jose, California, USA {akkiraju, vibha.sinha, anbangxu, jumahmud, psgundec, liuzh, Xiaotong.Liu, jfs}@us.ibm.com  Abstract—Academic literature on machine learning modeling fails to address how to make machine learning models work for enterprises. For example, existing machine learning processes cannot address how to define business use cases for an AI application, how to convert business requirements from offering managers into data requirements for data scientists, and how to continuously improve AI applications in term of accuracy and fairness, how to customize general purpose machine learning models with industry, domain, and use case specific data to make them more accurate for specific situations etc. Making AI work for enterprises requires special considerations, tools, methods and processes. In this paper we present a maturity framework for machine learning model lifecycle management for enterprises. Our framework is a re-interpretation of the software Capability Maturity Model (CMM) for machine learning model development process. We present a set of best practices from authors’ personal experience of building large scale real-world machine learning models to help organizations achieve higher levels of maturity independent of their starting point. Keywords—machine learning models, maturity model, maturity framework, AI model life cycle management. I. INTRODUCTION  Software and Services development has gone through various phases of maturity in the past few decades. The community has evolved lifecycle management theories and practices to disseminate best practices to developers, companies and consultants alike. For example, in software field, Software Development Life Cycle (SDLC) Management, capability maturi",
    "title": "Characterizing machine learning process: A maturity framework",
    "abstract": "Academic literature on machine learning modeling fails to address how to make\nmachine learning models work for enterprises. For example, existing machine\nlearning processes cannot address how to define business use cases for an AI\napplication, how to convert business requirements from offering managers into\ndata requirements for data scientists, and how to continuously improve AI\napplications in term of accuracy and fairness, and how to customize general\npurpose machine learning models with industry, domain, and use case specific\ndata to make them more accurate for specific situations etc. Making AI work for\nenterprises requires special considerations, tools, methods and processes. In\nthis paper we present a maturity framework for machine learning model lifecycle\nmanagement for enterprises. Our framework is a re-interpretation of the\nsoftware Capability Maturity Model (CMM) for machine learning model development\nprocess. We present a set of best practices from our personal experience of\nbuilding large scale real-world machine learning models to help organizations\nachieve higher levels of maturity independent of their starting point.",
    "link": "http://arxiv.org/abs/1811.04871v1",
    "published": "2018-11-12T17:32:24Z"
  },
  "203": {
    "pdf_path": "data/pdfs/machine learning_paper_70.pdf",
    "text_excerpt": "New Trends in Quantum Machine Learning\nLorenzo Bu\u000boni1, 2and Filippo Caruso1, 3\n1Dipartimento di Fisica e Astronomia, Universit\u0013 a di Firenze, I-50019 Sesto Fiorentino, Italy\n2Dipartimento di Ingegneria dell'Informazione, Universit\u0013 a di Firenze, I-50139 Firenze, Italy\n3LENS, QSTAR and CNR-INO, I-50019 Sesto Fiorentino, Italy\nHere we will give a perspective on new possible interplays between Machine Learning and Quantum\nPhysics, including also practical cases and applications. We will explore the ways in which machine\nlearning could bene\ft from new quantum technologies and algorithms to \fnd new ways to speed up\ntheir computations by breakthroughs in physical hardware, as well as to improve existing models\nor devise new learning schemes in the quantum domain. Moreover, there are lots of experiments\nin quantum physics that do generate incredible amounts of data and machine learning would be\na great tool to analyze those and make predictions, or even control the experiment itself. On\ntop of that, data visualization techniques and other schemes borrowed from machine learning can\nbe of great use to theoreticians to have better intuition on the structure of complex manifolds or\nto make predictions on theoretical models. This new research \feld, named as Quantum Machine\nLearning, is very rapidly growing since it is expected to provide huge advantages over its classical\ncounterpart and deeper investigations are timely needed since they can be already tested on the\nalready commercially available quantum machines.\nI. INTRODUCTION\nMachine learning (ML) [1{4] is a broad \feld of study,\nwith multifaceted applications of cross-disciplinary\nbreadth. ML ultimately aims at developing computer\nalgorithms that improve automatically through experi-\nence. The core idea of arti\fcial intelligence (AI) tech-\nnology is that systems can learn from data, so as to\nidentify distinctive patterns and make consequently de-\ncisions, with minimal human intervention. The range of\napplications of ML met",
    "title": "New Trends in Quantum Machine Learning",
    "abstract": "Here we will give a perspective on new possible interplays between Machine\nLearning and Quantum Physics, including also practical cases and applications.\nWe will explore the ways in which machine learning could benefit from new\nquantum technologies and algorithms to find new ways to speed up their\ncomputations by breakthroughs in physical hardware, as well as to improve\nexisting models or devise new learning schemes in the quantum domain. Moreover,\nthere are lots of experiments in quantum physics that do generate incredible\namounts of data and machine learning would be a great tool to analyze those and\nmake predictions, or even control the experiment itself. On top of that, data\nvisualization techniques and other schemes borrowed from machine learning can\nbe of great use to theoreticians to have better intuition on the structure of\ncomplex manifolds or to make predictions on theoretical models. This new\nresearch field, named as Quantum Machine Learning, is very rapidly growing\nsince it is expected to provide huge advantages over its classical counterpart\nand deeper investigations are timely needed since they can be already tested on\nthe already commercially available quantum machines.",
    "link": "http://arxiv.org/abs/2108.09664v1",
    "published": "2021-08-22T08:23:30Z"
  },
  "204": {
    "pdf_path": "data/pdfs/machine learning_paper_672.pdf",
    "text_excerpt": "Machine Learning 1: 287-316, 1986\n© 1986 Kluwer Academic Publishers, Boston - Manufactured in The Netherlands\nLearning at the Knowledge Level\nTHOMAS G. DIETTERICH (DIETTERICH%OREGON-STATE@CSNET-RELAY)\nDepartment of Computer Science, Oregon State University, Corvallis, OR 97331, U.S.A.\n(Received January 10, 1986)\n(Revised April 15, 1986)\nKey words: inductive learning, deductive learning, knowledge level, learning theory\nAbstract. When Newell introduced the concept of the knowledge level as a useful level of description for\ncomputer systems, he focused on the representation of knowledge. This paper applies the knowledge\nlevel notion to the problem of knowledge acquisition. Two interesting issues arise. First, some existing\nmachine learning programs appear to be completely static when viewed at the knowledge level. These\nprograms improve their performance without changing their 'knowledge.' Second, the behavior of some\nother machine learning programs cannot be predicted or described at the knowledge level. These\nprograms take unjustified inductive leaps. The first programs are called symbol level learning (SLL)\nprograms; the second, nondeductive knowledge level learning (NKLL) programs. The paper analyzes\nboth of these classes of learning programs and speculates on the possibility of developing coherent\ntheories of each. A theory of symbol level learning is sketched, and some reasons are presented for\nbelieving that a theory of NKLL will be difficult to obtain.\n1. Introduction\nIn his AAAI President's Address, Allen Newell (1981) defined a level of computer\nsystem description called the 'knowledge level.' As with other levels of description\n(e.g., the register-transfer level, the circuit level), the purpose of introducing the\nknowledge level is to provide a succinct and efficient means of describing and\npredicting the behavior of a computer system. In particular, Newell was attempting\nto systematize and justify the everyday use that AI researchers make of notions\nsuch a",
    "title": "Learning at the Knowledge Level",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022858530318",
    "published": "2003-04-04T16:57:10Z"
  },
  "205": {
    "pdf_path": "data/pdfs/machine learning_paper_18.pdf",
    "text_excerpt": "arXiv:1212.2686v1  [stat.ML]  12 Dec 2012Joint Training of Deep Boltzmann Machines for Classiﬁcatio n\nIan J. Goodfellow Aaron Courville Yoshua Bengio\nUniversit´ e de Montr´ eal\nAbstract\nWe introduce a new method for training deep\nBoltzmann machines jointly. Prior methods\nrequireaninitial learningpassthat trains the\ndeep Boltzmann machine greedily, one layer\nat a time, or do not perform well on classiﬁ-\ncation tasks.\n1 Deep Boltzmann machines\nA deep Boltzmann machine\n(Salakhutdinov and Hinton, 2009) is a probabilistic\nmodel consisting of many layers of random variables,\nmost of which are latent. Typically, a DBM contains\na set ofDinput features vthat are called the visible\nunitsbecause they are always observed during both\ntraining and evaluation. The DBM is usually applied\nto classiﬁcation problems and thus often represents\nthe class label with a one-of- kcode in the form of\na discrete-valued label unit y.yis observed (on\nexamples for which it is available) during training.\nThe DBM also contains several hidden units, which\nare usually organized into Llayersh(i)of size\nNi,i= 1,...,L,with each unit in a layer conditionally\nindependent of the other units in the layer given the\nneighboring layers. These conditional independence\nproperties allow fast Gibbs sampling because an entire\nlayer of units can be sampled at a time. Likewise,\nmean ﬁeld inference with ﬁxed point equations is fast\nbecause each ﬁxed point equation gives a solution to\nan entire layer of variational parameters.\nA DBM deﬁnes a probability distribution by exponen-\ntiating and normalizing an energy function\nP(v,h,y) =1\nZexp(−E(v,h,y))\nPreliminary work presented to Bruno Olshausen’s lab and\nGoogle Brain, December 2012.where\nZ=/summationdisplay\nv′,h′,y′exp(−E(v′,h′,y′)).\nZ, the partition function, is intractable, due to the\nsummation over all possible states. Maximum like-\nlihood learning requires computing the gradient of\nlogZ. Fortunately, the gradient can be estimated us-\ning an MCMC procedure (Younes, 19",
    "title": "Joint Training of Deep Boltzmann Machines",
    "abstract": "We introduce a new method for training deep Boltzmann machines jointly. Prior\nmethods require an initial learning pass that trains the deep Boltzmann machine\ngreedily, one layer at a time, or do not perform well on classifi- cation\ntasks.",
    "link": "http://arxiv.org/abs/1212.2686v1",
    "published": "2012-12-12T01:59:27Z"
  },
  "206": {
    "pdf_path": "data/pdfs/machine learning_paper_36.pdf",
    "text_excerpt": "Journal of Machine Learning Research X (XXXX) X-X Submitted 06/19; Published XX/XX\nPymc-learn: Practical Probabilistic Machine Learning in\nPython\nDaniel Emaasit demaasit@haystax.com\nData Science Team\nHaystax Technology\nMcLean, VA 22102, USA\nEditor: XXX\nAbstract\nPymc-learn is a Python package providing a variety of state-of-the-art probabilistic models\nfor supervised and unsupervised machine learning. It is inspired by scikit-learn and focuses\non bringing probabilistic machine learning to non-specialists. It uses a general-purpose\nhigh-level language that mimics scikit-learn . Emphasis is put on ease of use, productivity,\n\rexibility, performance, documentation, and an API consistent with scikit-learn . It depends\nonscikit-learn andpymc3 and is distributed under the new BSD-3 license, encouraging its\nuse in both academia and industry. Source code, binaries, and documentation are available\non http://github.com/pymc-learn/pymc-learn.\nKeywords: Probabilistic modeling, scikit-learn, PyMC3, probabilistic programming,\nsupervised learning, unsupervised learning\n# Linear regression in Pymc-learn\nfrom pmlearn.linear_model \\\nimport LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nlr.score(X_test, y_test)\nlr.predict(X_test)\nlr.save(\"path/to/saved-model\")\n# Gaussian process regression in Pymc-learn\nfrom pmlearn.gaussian_process \\\nimport GaussianProcessRegressor()\ngpr = GaussianProcessRegressor()\ngpr.fit(X_train, y_train)\ngpr.score(X_test, y_test)\ngpr.predict(X_test)\ngpr.save(\"path/to/saved-model\")# Linear regression in Scikit-learn\nfrom sklearn.linear_model \\\nimport LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nlr.score(X_test, y_test)\nlr.predict(X_test)\nlr.save(\"path/to/saved-model\")\n# Gaussian process regression in Scikit-learn\nfrom sklearn.gaussian_process \\\nimport GaussianProcessRegressor()\ngpr = GaussianProcessRegressor()\ngpr.fit(X_train, y_train)\ngpr.score(X_test, y_test)\ngpr.predict(X_test)\ngpr.save(\"path/to/saved-model\")\nFigure 1: An e",
    "title": "Pymc-learn: Practical Probabilistic Machine Learning in Python",
    "abstract": "$\\textit{Pymc-learn}$ is a Python package providing a variety of\nstate-of-the-art probabilistic models for supervised and unsupervised machine\nlearning. It is inspired by $\\textit{scikit-learn}$ and focuses on bringing\nprobabilistic machine learning to non-specialists. It uses a general-purpose\nhigh-level language that mimics $\\textit{scikit-learn}$. Emphasis is put on\nease of use, productivity, flexibility, performance, documentation, and an API\nconsistent with $\\textit{scikit-learn}$. It depends on $\\textit{scikit-learn}$\nand $\\textit{pymc3}$ and is distributed under the new BSD-3 license,\nencouraging its use in both academia and industry. Source code, binaries, and\ndocumentation are available on http://github.com/pymc-learn/pymc-learn.",
    "link": "http://arxiv.org/abs/1811.00542v1",
    "published": "2018-10-31T22:54:12Z"
  },
  "207": {
    "pdf_path": "data/pdfs/machine learning_paper_148.pdf",
    "text_excerpt": "The Shapley Value in Machine Learning\nBenedek Rozemberczki1,Lauren Watson2,P´eter Bayer3,Hao-Tsung Yang2,\nOliv ´er Kiss4,Sebastian Nilsson1and Rik Sarkar2\n1Research Data & Analytics, Research & Development IT, AstraZeneca\n2The University of Edinburgh, School of Informatics\n3Toulouse School of Economics & Institute for Advanced Study in Toulouse\n4Central European University, Department of Economics and Business\nbenedek.rozemberczki@astrazeneca.com\nAbstract\nOver the last few years, the Shapley value, a solution\nconcept from cooperative game theory, has found nu-\nmerous applications in machine learning. In this paper,\nwe ﬁrst discuss fundamental concepts of cooperative\ngame theory and axiomatic properties of the Shapley\nvalue. Then, we give an overview of the most important\napplications of the Shapley value in machine learning:\nfeature selection, explainability, multi-agent reinforce-\nment learning, ensemble pruning, and data valuation.\nWe examine the most crucial limitations of the Shapley\nvalue and point out directions for future research.\n1 Introduction\nMeasuring importance and the attribution of various gains is a\ncentral problem in many practical aspects of machine learning\nsuch as explainability [Lundberg et al., 2017 ], feature selection\n[Cohen et al., 2007 ], data valuation [Ghorbani et al. , 2019 ],\nensemble pruning [Rozemberczki et al., 2021 ]and federated\nlearning [Wang et al., 2020; Fan et al., 2021 ]. For example, one\nmight ask: What is the importance of a feature in the decisions of\na machine learning model? How much is an individual data point\nworth? Which models are the most valuable in an ensemble?\nThese questions have been addressed in different domains using\nspeciﬁc approaches. Interestingly, there is also a general and\nuniﬁed approach to these questions as a solution to a transferable\nutility (TU) cooperative game. In contrast with other approaches,\nsolution concepts of TU games are theoretically motivated with\naxiomatic properties. The best known s",
    "title": "The Shapley Value in Machine Learning",
    "abstract": "Over the last few years, the Shapley value, a solution concept from cooperative game theory, has found numerous applications in machine learning. In this paper, we first discuss fundamental concepts of cooperative game theory and axiomatic properties of the Shapley value. Then we give an overview of the most important applications of the Shapley value in machine learning: feature selection, explainability, multi-agent reinforcement learning, ensemble pruning, and data valuation. We examine the most crucial limitations of the Shapley value and point out directions for future research.",
    "link": "https://www.semanticscholar.org/paper/09c72d9d46f6750e487afdb5f7cae7693ffccc10",
    "published": "2022-02-11"
  },
  "208": {
    "pdf_path": "data/pdfs/machine learning_paper_15.pdf",
    "text_excerpt": "The Landscape of Modern Machine Learning: A Review of\nMachine, Distributed and Federated Learning\nOmer Subasi1*, Oceane Bel1, Joseph Manzano1, Kevin Barker1\n1*High Performance Computing Group, Pacific Northwest National Laboratory, 902\nBattelle Blvd, Richland, 99354, WA, USA.\n*Corresponding author(s). E-mail(s): omer.subasi@pnnl.gov;\nContributing authors: obel@pnnl.gov; joseph.manzano@pnnl.gov; kevin.barker@pnnl.gov;\nAbstract\nWith the advance of the powerful heterogeneous, parallel and distributed computing systems and ever\nincreasing immense amount of data, machine learning has become an indispensable part of cutting-\nedge technology, scientific research and consumer products. In this study, we present a review of\nmodern machine and deep learning. We provide a high-level overview for the latest advanced machine\nlearning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed\nlearning, deep learning as well as federated learning. As a result, our work serves as an introductory\ntext to the vast field of modern machine learning.\nKeywords: Machine Learning, Distributed Machine Learning, Deep Learning, Federated Learning, Parallel\nand Distributed Computing.\n1 Introduction\nOver the last decade, Machine Learning (ML) has\nbeen applied to ever increasing immense amount\nof data that is becoming available as more people\nbecome daily users of internet, mobile and wireless\nnetworks. Coupled with the significant advances in\ndeep learning (DL), ML has found more complex\napplications: from medical to machine transla-\ntion and speech recognition, to intelligent object\nrecognition, and to smart cities [1, 2]. Modern\nparallel and heterogeneous computing systems\n[3, 4, 5] have enabled such applications by sup-\nporting highly parallel training. These large-scale\nand distributed systems therefore have become the\nbackbone of modern ML [6, 7, 8].\nFederated Learning (FL), as a sub-field of DL,\nhas emerged as a distributed learning solution toprovide data p",
    "title": "The Landscape of Modern Machine Learning: A Review of Machine,\n  Distributed and Federated Learning",
    "abstract": "With the advance of the powerful heterogeneous, parallel and distributed\ncomputing systems and ever increasing immense amount of data, machine learning\nhas become an indispensable part of cutting-edge technology, scientific\nresearch and consumer products. In this study, we present a review of modern\nmachine and deep learning. We provide a high-level overview for the latest\nadvanced machine learning algorithms, applications, and frameworks. Our\ndiscussion encompasses parallel distributed learning, deep learning as well as\nfederated learning. As a result, our work serves as an introductory text to the\nvast field of modern machine learning.",
    "link": "http://arxiv.org/abs/2312.03120v1",
    "published": "2023-12-05T20:40:05Z"
  },
  "209": {
    "pdf_path": "data/pdfs/machine learning_paper_157.pdf",
    "text_excerpt": "Vol.:(0123456789)Machine Learning (2021) 110:457–506\nhttps://doi.org/10.1007/s10994-021-05946-3\n1 3\nAleatoric and epistemic uncertainty in machine learning: \nan introduction to concepts and methods\nEyke Hüllermeier1  · Willem Waegeman2\nReceived: 26 March 2020 / Revised: 29 September 2020 / Accepted: 2 January 2021 / \nPublished online: 8 March 2021 \n© The Author(s) 2021\nAbstract\nThe notion of uncertainty is of major importance in machine learning and constitutes a key \nelement of machine learning methodology. In line with the statistical tradition, uncertainty \nhas long been perceived as almost synonymous with standard probability and probabilistic \npredictions. Yet, due to the steadily increasing relevance of machine learning for practical \napplications and related issues such as safety requirements, new problems and challenges \nhave recently been identified by machine learning scholars, and these problems may call \nfor new methodological developments. In particular, this includes the importance of dis-\ntinguishing between (at least) two different types of uncertainty, often referred to as alea-\ntoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty \nin machine learning as well as an overview of attempts so far at handling uncertainty in \ngeneral and formalizing this distinction in particular.\nKeywords Uncertainty · Probability · Epistemic uncertainty · Version space learning · \nBayesian inference · Calibration · Ensembles · Gaussian processes · Deep neural networks · \nLikelihood-based methods · Credal sets and classifiers · Conformal prediction · Set-valued \nprediction · Generative models\n1 Introduction\nMachine learning is essentially concerned with extracting models from data, often (though \nnot exclusively) using them for the purpose of prediction. As such, it is inseparably con-\nnected with uncertainty. Indeed, learning in the sense of generalizing beyond the data seen \nEditor: Peter Flach.\n * Eyke Hüllermeier \n eyke@upb.de\n W",
    "title": "Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods",
    "abstract": "The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.",
    "link": "https://www.semanticscholar.org/paper/b631ba962b4403a9c0fd9cce446ef3b1e21ea059",
    "published": "2019-10-21"
  },
  "210": {
    "pdf_path": "data/pdfs/machine learning_paper_364.pdf",
    "text_excerpt": "ARTICLE OPEN\nA general-purpose machine learning framework for predicting\nproperties of inorganic materials\nLogan Ward1, Ankit Agrawal2, Alok Choudhary2and Christopher Wolverton1\nA very active area of materials research is to devise methods that use machine learning to automatically extract predictive models\nfrom existing materials data. While prior examples have demonstrated successful models for some applications, many more\napplications exist where machine learning can make a strong impact. To enable faster development of machine-learning-basedmodels for such applications, we have created a framework capable of being applied to a broad range of materials data. Our\nmethod works by using a chemically diverse list of attributes, which we demonstrate are suitable for describing a wide variety of\nproperties, and a novel method for partitioning the data set into groups of similar materials to boost the predictive accuracy. In thismanuscript, we demonstrate how this new method can be used to predict diverse properties of crystalline and amorphous\nmaterials, such as band gap energy and glass-forming ability.\nnpj Computational Materials (2016) 2,16028; doi:10.1038/npjcompumats.2016.28; published online 26 August 2016\nINTRODUCTION\nRational design of materials is the ultimate goal of modern\nmaterials science and engineering. As part of achieving that goal,\nthere has been a large effort in the materials science communityto compile extensive data sets of materials properties to providescientists and engineers with ready access to the properties of\nknown materials. Today, there are databases of crystal structures,\n1\nsuperconducting critical temperatures (http://supercon.nims.go.\njp/), physical properties of crystalline compounds2–5and many\nother repositories containing useful materials data. Recently, it has\nbeen shown that these databases can also serve as resources forcreating predictive models and design rules —the key tools of\nrational materials design.\n6–12These databases h",
    "title": "A General-Purpose Machine Learning Framework for Predicting Properties of Inorganic Materials",
    "abstract": "A very active area of materials research is to devise methods that use machine learning to automatically extract predictive models from existing materials data. While prior examples have demonstrated successful models for some applications, many more applications exist where machine learning can make a strong impact. To enable faster development of machine-learning-based models for such applications, we have created a framework capable of being applied to a broad range of materials data. Our method works by using a chemically diverse list of attributes, which we demonstrate are suitable for describing a wide variety of properties, and a novel method for partitioning the data set into groups of similar materials in order to boost the predictive accuracy. In this manuscript, we demonstrate how this new method can be used to predict diverse properties of crystalline and amorphous materials, such as band gap energy and glass-forming ability.",
    "link": "https://www.semanticscholar.org/paper/a10bc90b3c97a4abe86c73cfb2a8490a9b44373f",
    "published": "2016-06-30"
  },
  "211": {
    "pdf_path": "data/pdfs/machine learning_paper_50.pdf",
    "text_excerpt": "AnO(N)Sorting Algorithm: Machine Learning Sort\nHanqing Zhao1, 2and Yuehan Luo3\n1Department of Modern Physics, University of Science and Technology of China, Hefei 230026, China\n2School of Physical Science and Technology, and Key Laboratory for Magnetism and Magnetic Materials of MOE,\nLanzhou University, Lanzhou, Gansu 730000, China\u0003\n3Department of Applied Mathematics and Statistics,\nState University of New York at Stony Brook, Stony Brook, NY, 11794, USA\nWe propose an O(N \u0001M) sorting algorithm by Machine Learning method, which shows a huge\npotential sorting big data. This sorting algorithm can be applied to parallel sorting and is suitable\nfor GPU or TPU acceleration. Furthermore, we discuss the application of this algorithm to sparse\nhash table.\nINTRODUCTION\nSorting, as a fundamental operation on data, has at-\ntracted intensive interests from the beginning of comput-\ning [1]. Lots of classic algorithms have been designed\nand applied, such as Bubble Sort, Selection Sort, In-\nsertion Sort, etc. However, it's been proven that sort-\ning algorithms based on comparison have a fundamen-\ntal requirement of \n( NlogN) comparisons[2, 3], which\nimplies the time complexity is at least O(NlogN)[4{6].\nThe non-comparison sorting algorithms, such as Bucket\nBort, Counting Sort and Radix Sort, are not restricted\nby the \n(NlogN) boundary, and can reach O(N) com-\nplexity [7], but these algorithms have very limited ap-\nplications. Most of the state-of-art sorting algorithms\nemploy parallel computing to handle big datasets and\nhave accomplished outstanding achievements [8{12]. For\nexample [13], in 2015, FuxiSort [14], developed by Al-\nibaba Group, is a distributed sort implementation on top\nof Apsara. FuxiSort is able to complete the 100TB Day-\ntona GraySort benchmark in 377 seconds on random non-\nskewed dataset and 510 seconds on skewed dataset, and\nIndy GraySort benchmark in 329 seconds. Then, in 2016,\nTencent Sort [15] has achieved a speed of 60.7 TB/min in\nsorting 100 TB data for the",
    "title": "An $O(N)$ Sorting Algorithm: Machine Learning Sort",
    "abstract": "We propose an $O(N\\cdot M)$ sorting algorithm by Machine Learning method,\nwhich shows a huge potential sorting big data. This sorting algorithm can be\napplied to parallel sorting and is suitable for GPU or TPU acceleration.\nFurthermore, we discuss the application of this algorithm to sparse hash table.",
    "link": "http://arxiv.org/abs/1805.04272v2",
    "published": "2018-05-11T08:28:55Z"
  },
  "212": {
    "pdf_path": "data/pdfs/machine learning_paper_68.pdf",
    "text_excerpt": "Adversarial Machine Learning Attacks on Condition -Based Maintenance Capabilities  \nHamidreza Habibollahi Najaf Abadi  \nCenter for Advanced Life Cycle Engineering (CALCE)  \nUniversity of Maryland  \nCollege Park, MD, 20742 USA  \nAbstract  \nCondition -based maintenance  (CBM)  strategies  exploit machine learning models  to assess  the health status of systems \nbased on the collected data from the physical environment , while machine learning  models are vulnerable to \nadversarial attacks.  A malicious adversary  can manipulate  the collected data to deceive the machine learning model \nand affect the CBM system’s performance . Adversarial machine learning  techniques  introduced  in the computer vision \ndomain can be used to make stealthy attacks on CBM systems by adding perturbation to data  to confuse trained \nmodels . The stealthy nature cause s difficulty and  delay in detection  of the attacks . In this paper, adversarial machine \nlearning in the domain of CBM is introduced . A case study shows  how adversarial machine learning can be used to \nattack CBM  capabilities.  Adversarial samples are crafted using the Fast Gradient Sign method , and the performance \nof a CBM  system under attack  is investigated.  The obtained results reveal  that CBM  systems are vulnerable to \nadversarial  machine learning  attack s and defense strategies need to be considered.   \n1. Introduction  \nMaintenance strategies can be classified into corrective maintenance and preventive maintenance. Corrective \nmaintenance is a strategy that is used to repair (or replace) the component after it fails , while  preventive maintenance \nis carried out before break downs . Condition -based maintenance  (CBM) , a type of preventive maintenance, \nrecommends maintenance decisions based on the information collected through c ondition monitoring process  [1]. \nCondition monitoring equipment  includes sensor  nodes  that measure different parameter s, such as  pressure, \ntemperature, speed, vibration,",
    "title": "Adversarial Machine Learning Attacks on Condition-Based Maintenance\n  Capabilities",
    "abstract": "Condition-based maintenance (CBM) strategies exploit machine learning models\nto assess the health status of systems based on the collected data from the\nphysical environment, while machine learning models are vulnerable to\nadversarial attacks. A malicious adversary can manipulate the collected data to\ndeceive the machine learning model and affect the CBM system's performance.\nAdversarial machine learning techniques introduced in the computer vision\ndomain can be used to make stealthy attacks on CBM systems by adding\nperturbation to data to confuse trained models. The stealthy nature causes\ndifficulty and delay in detection of the attacks. In this paper, adversarial\nmachine learning in the domain of CBM is introduced. A case study shows how\nadversarial machine learning can be used to attack CBM capabilities.\nAdversarial samples are crafted using the Fast Gradient Sign method, and the\nperformance of a CBM system under attack is investigated. The obtained results\nreveal that CBM systems are vulnerable to adversarial machine learning attacks\nand defense strategies need to be considered.",
    "link": "http://arxiv.org/abs/2101.12097v1",
    "published": "2021-01-28T16:34:04Z"
  },
  "213": {
    "pdf_path": "data/pdfs/machine learning_paper_604.pdf",
    "text_excerpt": " \n \n \n*Corresponding author. Email:  aka104@live.aul.edu.lb  \n                      \n \n \n \nResea rch Article  \nIntrusion Detection System Based on  Machine Learning Algorithms: ( SVM and \nGenetic Algorithm)  \nAbdulazeez Khlaif Shathir Alsajri1,*, Amani Steiti  2,  \n \n1 Computer Science Department , University Arts,  Sciences and Technology , Beirut , Lebanon . \n2 University Tishreen, faculty of information engineering, Department of Computer Systems and Networks, Latakia, Syria.  \n \n \nA R T I C L E  I N F O  \n \nArticle  Histo ry \nReceived  01 Nov 2023  \nRevised 02 Dec 202 3 \nAccepted   20 Dec 2023  \nPublished 18 Jan 2024  \n \nKeywords  \nCybersecurity  \nIDS  \nMachine Learning  \n SVM Algorithm  \nGenetic Algorithm  \nNetworks  \n \n A B S T R A C T   \n \nThe widespread utilization of the internet and computer systems has resulted in notable security concerns, \ncharacterized by a surge in intrusions and vulnerabilities. Malicious users manipulate internal systems, \nresulting in the exploitation of software flaws and default setups.   With the integration of the internet \ninto society, there is an emergence of new risks such as viruses and worms, which highlights the \nimportance of implement ing robust security measures.   Intrusion detection systems (IDS) are security \ntechnologies utilized to monitor and analyze network traffic or system activity with the purpose of \nidentifying hostile behavior.   This article presents a proposed method for d etecting intrusion in network \ntraffic using a hybrid approach, which combines a genetic algorithm and an SVM algorithm.   The model \nunderwent training and testing on the KDDCup99 dataset, with a reduction in features from 42 to 29 \nusing the hybrid approach .   The results demonstrated that throughout the system testing, it exhibited a \nremarkable accuracy of 0.999. Additionally, it achieved a true positive value of 0.9987 and a false \nnegative rate of 0.012.  \n1. INTRODUCTION  \nIn recent decades, internet and computer systems have ",
    "title": "Intrusion Detection System Based on Machine Learning Algorithms:( SVM and Genetic Algorithm)",
    "abstract": "<jats:p>The widespread utilization of the internet and computer systems has resulted in notable security concerns, characterized by a surge in intrusions and vulnerabilities. Malicious users manipulate internal systems, resulting in the exploitation of software flaws and default setups.   With the integration of the internet into society, there is an emergence of new risks such as viruses and worms, which highlights the importance of implementing robust security measures.   Intrusion detection systems (IDS) are security technologies utilized to monitor and analyze network traffic or system activity with the purpose of identifying hostile behavior.   This article presents a proposed method for detecting intrusion in network traffic using a hybrid approach, which combines a genetic algorithm and an SVM algorithm.   The model underwent training and testing on the KDDCup99 dataset, with a reduction in features from 42 to 29 using the hybrid approach.   The results demonstrated that throughout the system testing, it exhibited a remarkable accuracy of 0.999. Additionally, it achieved a true positive value of 0.9987 and a false negative rate of 0.012.</jats:p>",
    "link": "https://doi.org/10.58496/bjml/2024/002",
    "published": "2024-03-22T10:44:06Z"
  },
  "214": {
    "pdf_path": "data/pdfs/machine learning_paper_203.pdf",
    "text_excerpt": "4.3.3SomeStudiesinMachine Learning UsingtheGameofCheckers\nSomeStudies inMachine Learning\nUsingtheGameofCheckers\nArthurL.Samuel\nAbstract: Twomachine-learning procedures havebeeninvestigated insomedetailusi!Jgthegameof\ncheckers. Enough workhasbeendonetoverifythefactthatacomputer canbeprogrammed sothatitwill\nlearntoplayabettergameofcheckers thancanbeplayedbythepersonwhowrotetheprogram. Further­\nmore,itcanlearntodothisinaremarkably shortperiodoftime(8or10hoursofmachine-playing time)\nwhengivenonlytherulesofthegame,asenseofdirection, andaredundant andincomplete listof\nparameters whicharethought tohavesomething todowiththegame,butwhosecorrectsignsandrelative\nweights areunknown andunspecified. Theprinciples ofmachine learning verified bythese'experiments\nare,ofcourse,applicable tomanyothersituations.535\nIntroduction\nThestudiesreported herehavebeenconcerned withthe\nprogr;Jmming ofauigitalcomputer tobehaveinaway\nwhich.ifdonebyhumanbeingsoranimals, wouldbe\ndescrihcu asinvolving theprocess oflearning. While\nthisisnotthcplacetouwellontheimportance ofma­\nchinc-learning proceuures. ortouiscourse onthephilo­\nsophical aspccts, Ithereisobviously averylargeamount\nnf\\\\'L)rk.nowdonehypeople.whichisquitetrivialinits\nuemanus ontheintellect hutdoes,nevertheless, involve\nsomelearning. Wehaveatourcommand computers with\nadequate data-handling ahilityandwithsufficient ,com­\nputational specdtomakeuseofmachine-learning tech­\nniqucs.hutourknowledge ofthebasicprinciplcs ofthese\ntcchniqucs isstillrudimentary. Lacking suchknowledge,\nitisneccssary tospccifymethods ofproblcm solution in\nminuteanu~xactuetaiLatime-consuming andcostly\npr0cedurc. Programming computers tolearnfromex­\nperienec shoulueventual Iveliminatc theneedformuch\nofthisdctailed programm'ing erfort.\n•Ge/ll'l'lll /llelhodl' ofllpproach\nAtthcoutsetitmighthewelltodistinguish sharply be­\ntwccntwogcncralapproaches tothcproblem ofmachine\nlearning. Onemethod. whichmightbecalledtheNellral­\nXelApproliCh. uealswiththepossibility ofinducing\nlearnc",
    "title": "Some Studies in Machine Learning Using the Game of Checkers",
    "abstract": null,
    "link": "https://www.semanticscholar.org/paper/e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772",
    "published": "1995-10-31"
  },
  "215": {
    "pdf_path": "data/pdfs/Machine Learning for Clinical Predictive Analytics_paper_4.pdf",
    "text_excerpt": "This is the author’s version of the article that has been accepted to PaciﬁcVis 2020 Visualization Meets AI Workshop.\nA Visual Analytics System for Multi-model Comparison on\nClinical Data Predictions\nYiran Li *†, Takanori Fujiwara†, Y ong K. Choi‡, Katherine K. Kim‡§, and Kwan-Liu Ma†\nAbstract — There is a growing trend of applying machine learning methods to medical datasets in order to predict patients’ future\nstatus. Although some of these methods achieve high performance, challenges still exist in comparing and evaluating different models\nthrough their interpretable information. Such analytics can help clinicians improve evidence-based medical decision making. In this\nwork, we develop a visual analytics system that compares multiple models’ prediction criteria and evaluates their consistency. With our\nsystem, users can generate knowledge on different models’ inner criteria and how conﬁdently we can rely on each model’s prediction\nfor a certain patient. Through a case study of a publicly available clinical dataset, we demonstrate the effectiveness of our visual\nanalytics system to assist clinicians and researchers in comparing and quantitatively evaluating different machine learning methods.\nIndex Terms —Clinical data, XAI, tree-based machine learning models, model consistency, measures of dependence, visual analytics\n1 I NTRODUCTION\nComprehensive health data plays an important role in the provision\nof high-quality medical care and decision making. For example, pre-\ndictive analysis on medical datasets can help clinicians understand the\npotential risks of an operation for patients with particular characteris-\ntics [21] or identify potential deterioration of health status over various\ntreatment periods [25]. To develop useful predictions, machine learning\n(ML) methods have been used on medical datasets. For clinical predic-\ntion tasks, ML methods must satisfy high accuracy when clinicians rely\non the predicted results for their decision-making process [45].\nHoweve",
    "title": null,
    "abstract": null,
    "link": null,
    "published": null
  },
  "216": {
    "pdf_path": "data/pdfs/machine learning_paper_91.pdf",
    "text_excerpt": "Practical Black-Box Attacks against Machine Learning\nNicolas Papernot\nPennsylvania State University\nngp5056@cse.psu.eduPatrick McDaniel\nPennsylvania State University\nmcdaniel@cse.psu.eduIan Goodfellow\u0003\nOpenAI\nian@openai.com\nSomesh Jha\nUniversity of Wisconsin\njha@cs.wisc.eduZ. Berkay Celik\nPennsylvania State University\nzbc102@cse.psu.eduAnanthram Swami\nUS Army Research Laboratory\nananthram.swami.civ@mail.mil\nABSTRACT\nMachine learning (ML) models, e.g., deep neural networks\n(DNNs), are vulnerable to adversarial examples: malicious\ninputs modi\fed to yield erroneous model outputs, while ap-\npearing unmodi\fed to human observers. Potential attacks\ninclude having malicious content like malware identi\fed as\nlegitimate or controlling vehicle behavior. Yet, all existing\nadversarial example attacks require knowledge of either the\nmodel internals or its training data. We introduce the \frst\npractical demonstration of an attacker controlling a remotely\nhosted DNN with no such knowledge. Indeed, the only capa-\nbility of our black-box adversary is to observe labels given\nby the DNN to chosen inputs. Our attack strategy consists\nin training a local model to substitute for the target DNN,\nusing inputs synthetically generated by an adversary and\nlabeled by the target DNN. We use the local substitute to\ncraft adversarial examples, and \fnd that they are misclas-\nsi\fed by the targeted DNN. To perform a real-world and\nproperly-blinded evaluation, we attack a DNN hosted by\nMetaMind, an online deep learning API. We \fnd that their\nDNN misclassi\fes 84.24% of the adversarial examples crafted\nwith our substitute. We demonstrate the general applicabil-\nity of our strategy to many ML techniques by conducting the\nsame attack against models hosted by Amazon and Google,\nusing logistic regression substitutes. They yield adversarial\nexamples misclassi\fed by Amazon and Google at rates of\n96.19% and 88.94%. We also \fnd that this black-box attack\nstrategy is capable of evading defense strategies previous",
    "title": "Practical Black-Box Attacks against Machine Learning",
    "abstract": "Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.",
    "link": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024",
    "published": "2016-02-08"
  },
  "217": {
    "pdf_path": "data/pdfs/machine learning_paper_46.pdf",
    "text_excerpt": "arXiv:1703.10121v1  [cs.LG]  29 Mar 2017The Top 10 Topics in Machine Learning\nRevisited: A Quantitative Meta-Study\nPatrick Glauner1, Manxing Du1, Victor Paraschiv2, Andrey Boytsov1,\nIsabel L´ opez Andrade3, Jorge Augusto Meira1, Petko Valtchev14and Radu State1\n1- Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg\n4 rue Alphonse Weicker, 2721 Luxembourg, Luxembourg\n2- Numbers of others\nLondon, United Kingdom\n3- American Express\nSussex House, Civic Way, Burgess Hill RH15 9AQ, United Kingd om\n4- Department of Computer Science, University of Quebec in M ontreal\n201, av. President Kennedy, Montreal H2X 3Y7, Canada\nAbstract . Which topics of machine learning are most commonly ad-\ndressed in research? This question was initially answered i n 2007 by doing\na qualitative survey among distinguished researchers. In o ur study, we\nrevisit this question from a quantitative perspective. Con cretely, we col-\nlect 54K abstracts of papers published between 2007 and 2016 in leading\nmachine learning journals and conferences. We then use mach ine learning\nin order to determine the top 10 topics in machine learning. W e not only\ninclude models, but provide a holistic view across optimiza tion, data, fea-\ntures, etc. This quantitative approach allows reducing the bias of surveys.\nIt reveals new and up-to-date insights into what the 10 most p roliﬁc top-\nics in machine learning research are. This allows researche rs to identify\npopular topics as well as new and rising topics for their rese arch.\n1 Introduction\nIn 2007, a paper named “Top 10 algorithms in data mining” identiﬁed an d pre-\nsented the top 10 most inﬂuential data mining algorithms within the re search\ncommunity [1]. The selection criteria were created by consolidating dir ect nom-\ninations from award winning researchers, the research community opinions and\nthe number of citations in Google Scholar. The top 10 algorithms in tha t prior\nwork are: C4.5, k-means, support vector machine, Apriori,",
    "title": "The Top 10 Topics in Machine Learning Revisited: A Quantitative\n  Meta-Study",
    "abstract": "Which topics of machine learning are most commonly addressed in research?\nThis question was initially answered in 2007 by doing a qualitative survey\namong distinguished researchers. In our study, we revisit this question from a\nquantitative perspective. Concretely, we collect 54K abstracts of papers\npublished between 2007 and 2016 in leading machine learning journals and\nconferences. We then use machine learning in order to determine the top 10\ntopics in machine learning. We not only include models, but provide a holistic\nview across optimization, data, features, etc. This quantitative approach\nallows reducing the bias of surveys. It reveals new and up-to-date insights\ninto what the 10 most prolific topics in machine learning research are. This\nallows researchers to identify popular topics as well as new and rising topics\nfor their research.",
    "link": "http://arxiv.org/abs/1703.10121v1",
    "published": "2017-03-29T16:29:04Z"
  },
  "218": {
    "pdf_path": "data/pdfs/machine learning_paper_327.pdf",
    "text_excerpt": "METHODS ARTICLE\npublished: 21 February 2014\ndoi: 10.3389/fninf.2014.00014\nMachine learning for neuroimaging with scikit-learn\nAlexandre Abraham1,2*, Fabian Pedregosa1,2, Michael Eickenberg1,2, Philippe Gervais1,2,\nAndreas Mueller3, Jean Kossaiﬁ4, Alexandre Gramfort1,2,5, Bertrand Thirion1,2and Gaël Varoquaux1,2\n1Parietal Team, INRIA Saclay-Île-de-France, Saclay, France\n2Neurospin, I2BM, DSV , CEA, Gif-Sur-Yvette, France\n3Institute of Computer Science VI, University of Bonn, Bonn, Germany\n4Department of Computing, Imperial College London, London, UK\n5Institut Mines-Telecom, Telecom ParisTech, CNRS LTCI, Paris, France\nEdited by:\nSatrajit S. Ghosh, MassachusettsInstitute of Technology, USA\nReviewed by:\nMichael Hanke,\nOtto-von-Guericke-University,\nGermanyEugene Duff, University of Oxford,\nUK\n*Correspondence:\nAlexandre Abraham, CEA\nNeurospin, Bât 145, Point Courrier\n156, 91191 Gif-sur-Yvette, France\ne-mail: alexandre.abraham@inria.frStatistical machine learning methods are increasingly used for neuroimaging data analysis.\nTheir main virtue is their ability to model high-dimensional datasets, e.g., multivariate\nanalysis of activation images or resting-state time series. Supervised learning is typically\nused in decoding orencoding settings to relate brain images to behavioral or clinical\nobservations, while unsupervised learning can uncover hidden structures in sets of\nimages (e.g., resting state functional MRI) or ﬁnd sub-populations in large cohorts. By\nconsidering different functional neuroimaging applications, we illustrate how scikit-learn,a Python machine learning library, can be used to perform some key analysis steps.\nScikit-learn contains a very large set of statistical learning algorithms, both supervised\nand unsupervised, and its application to neuroimaging data provides a versatile tool tostudy the brain.\nKeywords: machine learning, statistical learning, neuroimaging, scikit-learn, Python\n1. INTRODUCTION\nInterest in applying statistical machine learning to neur",
    "title": "Machine learning for neuroimaging with scikit-learn",
    "abstract": "Statistical machine learning methods are increasingly used for neuroimaging data analysis. Their main virtue is their ability to model high-dimensional datasets, e.g., multivariate analysis of activation images or resting-state time series. Supervised learning is typically used in decoding or encoding settings to relate brain images to behavioral or clinical observations, while unsupervised learning can uncover hidden structures in sets of images (e.g., resting state functional MRI) or find sub-populations in large cohorts. By considering different functional neuroimaging applications, we illustrate how scikit-learn, a Python machine learning library, can be used to perform some key analysis steps. Scikit-learn contains a very large set of statistical learning algorithms, both supervised and unsupervised, and its application to neuroimaging data provides a versatile tool to study the brain.",
    "link": "https://www.semanticscholar.org/paper/f15367ed93c3505b1d62d802f3f4b769ae0f4ba5",
    "published": "2014-02-21"
  },
  "219": {
    "pdf_path": "data/pdfs/machine learning_paper_41.pdf",
    "text_excerpt": "arXiv:1504.03874v1  [cs.AI]  15 Apr 2015Bridging belief function theory to modern machine learning\nThomas Burger∗\niRTSV-BGE (Universit´ e Grenoble-Alpes, CNRS, CEA, INSERM)\nGrenoble, France\nthomas.burger@cea.fr\nNovember 23, 2021\nAbstract\nMachine learning is a quickly evolving ﬁeld which now looks r eally diﬀerent from what it was 15 years ago,\nwhen classiﬁcation and clustering were major issues. This d ocument proposes several trends to explore the\nnew questions of modern machine learning, with the strong af terthought that the belief function framework\nhas a major role to play.\nKeywords: belief functions; machine learning;\n1 Introduction\nIn an age of user generated web-contents and of portable devi ces with embedded computer vision capabilities,\nmachine learning (ML) and big data mining questions are fund amental. As a result, these questions naturally\npenetrate neighboring research ﬁelds, including belief fu nction theory (BFT), so that it is now usual to attend\na “Classiﬁcation” session [26] or a “Machine Learning” sess ion [16] in a conference devoted to belief functions.\nHowever, it is hard to accept that among the various proposed approaches based on BF, very few have\nbecome state-of-the-art ML methods, the knowledge of which has spread beyond the BF community. Without\nany doubt, this can be partly explained by the relative size o f the scientiﬁc communities under consideration:\nalthough quickly growing, the BF one is relatively small wit h respect to that of statistics, Bayesian networks,\nneural networks, etc. However, this reason alone is not suﬃc ient: There are indeed other topics, such as for\ninstance, information fusion, where BF-based methods are n ow as well recognized as are methods based on\nmore classical formalisms, such as probabilities, or ontol ogies.\nIn this report, I assume an additional reason: that some rese archers focused on BFT (especially the\nyoungest), who have progressively turned their interests t owards ML problems, may not capture th",
    "title": "Bridging belief function theory to modern machine learning",
    "abstract": "Machine learning is a quickly evolving field which now looks really different\nfrom what it was 15 years ago, when classification and clustering were major\nissues. This document proposes several trends to explore the new questions of\nmodern machine learning, with the strong afterthought that the belief function\nframework has a major role to play.",
    "link": "http://arxiv.org/abs/1504.03874v1",
    "published": "2015-04-15T12:04:58Z"
  },
  "220": {
    "pdf_path": "data/pdfs/machine learning_paper_257.pdf",
    "text_excerpt": "Quantum machine learning in feature Hilbert spaces\nMaria Schuld\u0003and Nathan Killoran\nXanadu, 372 Richmond St W, Toronto, M5V 2L7, Canada\n(Dated: March 21, 2018)\nThe basic idea of quantum computing is surprisingly similar to that of kernel methods in machine\nlearning, namely to e\u000eciently perform computations in an intractably large Hilbert space. In this\npaper we explore some theoretical foundations of this link and show how it opens up a new avenue\nfor the design of quantum machine learning algorithms. We interpret the process of encoding inputs\nin a quantum state as a nonlinear feature map that maps data to quantum Hilbert space. A\nquantum computer can now analyse the input data in this feature space. Based on this link, we\ndiscuss two approaches for building a quantum model for classi\fcation. In the \frst approach, the\nquantum device estimates inner products of quantum states to compute a classically intractable\nkernel. This kernel can be fed into any classical kernel method such as a support vector machine. In\nthe second approach, we can use a variational quantum circuit as a linear model that classi\fes data\nexplicitly in Hilbert space. We illustrate these ideas with a feature map based on squeezing in a\ncontinuous-variable system, and visualise the working principle with 2-dimensional mini-benchmark\ndatasets.\nI. INTRODUCTION\nThe goal of many quantum algorithms is to perform\ne\u000ecient computations in a Hilbert space that grows\nrapidly with the size of a quantum system. `E\u000ecient'\nmeans that the number of operations applied to the\nsystem grows at most polynomially with the system size.\nAn illustration is the famous quantum Fourier transform\napplied to an n-qubit system, which uses O(poly(n))\noperations to perform a discrete Fourier transform on\n2namplitudes. In continuous-variable systems this is\npushed to the extreme, as a single operation { for exam-\nple, squeezing { applied to a mode formally manipulates\na quantum state in an in\fnite-dimensional Hilbert space.\nIn th",
    "title": "Quantum Machine Learning in Feature Hilbert Spaces.",
    "abstract": "A basic idea of quantum computing is surprisingly similar to that of kernel methods in machine learning, namely, to efficiently perform computations in an intractably large Hilbert space. In this Letter we explore some theoretical foundations of this link and show how it opens up a new avenue for the design of quantum machine learning algorithms. We interpret the process of encoding inputs in a quantum state as a nonlinear feature map that maps data to quantum Hilbert space. A quantum computer can now analyze the input data in this feature space. Based on this link, we discuss two approaches for building a quantum model for classification. In the first approach, the quantum device estimates inner products of quantum states to compute a classically intractable kernel. The kernel can be fed into any classical kernel method such as a support vector machine. In the second approach, we use a variational quantum circuit as a linear model that classifies data explicitly in Hilbert space. We illustrate these ideas with a feature map based on squeezing in a continuous-variable system, and visualize the working principle with two-dimensional minibenchmark datasets.",
    "link": "https://www.semanticscholar.org/paper/7e7eb0f93c9550d7336f4bbfad5fe89604295705",
    "published": "2018-03-19"
  },
  "221": {
    "pdf_path": "data/pdfs/machine learning_paper_344.pdf",
    "text_excerpt": "fgene-09-00515 November 2, 2018 Time: 17:6 # 1\nORIGINAL RESEARCH\npublished: 06 November 2018\ndoi: 10.3389/fgene.2018.00515\nEdited by:\nTao Huang,\nShanghai Institutes for Biological\nSciences (CAS), China\nReviewed by:\nJianbo Pan,\nJohns Hopkins Medicine,\nUnited States\nZhu-Hong You,\nXinjiang Technical Institute of Physics\n& Chemistry (CAS), China\nChao Pang,\nColumbia University Medical Center,\nUnited States\n*Correspondence:\nQuan Zou\nzouquan@nclab.net\nHua Tang\nhuatang@swmu.edu.cn\nSpecialty section:\nThis article was submitted to\nBioinformatics and Computational\nBiology,\na section of the journal\nFrontiers in Genetics\nReceived: 29 July 2018\nAccepted: 12 October 2018\nPublished: 06 November 2018\nCitation:\nZou Q, Qu K, Luo Y, Yin D, Ju Y\nand Tang H (2018) Predicting\nDiabetes Mellitus With Machine\nLearning Techniques.\nFront. Genet. 9:515.\ndoi: 10.3389/fgene.2018.00515\nPredicting Diabetes Mellitus With\nMachine Learning Techniques\nQuan Zou1,2*, Kaiyang Qu1, Yamei Luo3, Dehui Yin3, Ying Ju4and Hua Tang5*\n1School of Computer Science and Technology, Tianjin University, Tianjin, China,2Institute of Fundamental and Frontier\nSciences, University of Electronic Science and Technology of China, Chengdu, China,3School of Medical Information\nand Engineering, Southwest Medical University, Luzhou, China,4School of Information Science and Technology, Xiamen\nUniversity, Xiamen, China,5Department of Pathophysiology, School of Basic Medicine, Southwest Medical University,\nLuzhou, China\nDiabetes mellitus is a chronic disease characterized by hyperglycemia. It may cause\nmany complications. According to the growing morbidity in recent years, in 2040, the\nworld’s diabetic patients will reach 642 million, which means that one of the ten adults\nin the future is suffering from diabetes. There is no doubt that this alarming ﬁgure needs\ngreat attention. With the rapid development of machine learning, machine learning\nhas been applied to many aspects of medical health. In this study, we used decision\ntree, r",
    "title": "Predicting Diabetes Mellitus With Machine Learning Techniques",
    "abstract": "Diabetes mellitus is a chronic disease characterized by hyperglycemia. It may cause many complications. According to the growing morbidity in recent years, in 2040, the world’s diabetic patients will reach 642 million, which means that one of the ten adults in the future is suffering from diabetes. There is no doubt that this alarming figure needs great attention. With the rapid development of machine learning, machine learning has been applied to many aspects of medical health. In this study, we used decision tree, random forest and neural network to predict diabetes mellitus. The dataset is the hospital physical examination data in Luzhou, China. It contains 14 attributes. In this study, five-fold cross validation was used to examine the models. In order to verity the universal applicability of the methods, we chose some methods that have the better performance to conduct independent test experiments. We randomly selected 68994 healthy people and diabetic patients’ data, respectively as training set. Due to the data unbalance, we randomly extracted 5 times data. And the result is the average of these five experiments. In this study, we used principal component analysis (PCA) and minimum redundancy maximum relevance (mRMR) to reduce the dimensionality. The results showed that prediction with random forest could reach the highest accuracy (ACC = 0.8084) when all the attributes were used.",
    "link": "https://www.semanticscholar.org/paper/5327bb691a1c63791a06de2d3f0478e47785add5",
    "published": "2018-11-06"
  },
  "222": {
    "pdf_path": "data/pdfs/machine learning_paper_7.pdf",
    "text_excerpt": "The Tribes of Machine Learning and the Realm of Computer\nArchitecture\nAYAZ AKRAM, University of California, Davis\nJASON LOWE-POWER, University of California, Davis\nMachine learning techniques have influenced the field of computer architecture like many other fields. This\npaper studies how the fundamental machine learning techniques can be applied towards computer architecture\nproblems. We also provide a detailed survey of computer architecture research that employs different machine\nlearning methods. Finally, we present some future opportunities and the outstanding challenges that need to\nbe overcome to exploit full potential of machine learning for computer architecture.\nAdditional Key Words and Phrases: machine learning, computer architecture\n1 INTRODUCTION\nMachine learning (ML) refers to the process in which computers learn to make decisions based on\nthe given data set without being explicitly programmed to do so [ 8]. There are various classifications\nof the ML algorithms. One of the more insightful classifications has been done by Pedro Domingos\nin his book The Master Algorithm [39]. Domingos presents five fundamental tribes of ML: the\nsymbolists, the connectionists, the evolutionaries, the bayesians and the analogizers. Each of these\nbelieve in a different strategy to go through the learning process. These tribes or schools of thought\nof ML along-with their primary algorithms and origins are shown in Table 1. There are existing\nproofs that given the enough amount of data, each of these algorithms can fundamentally learn\nanything. Most of the well known ML techniques/algorithms1belong to one of these tribes of ML.\nTable 1. Five Tribes of ML (taken from [39])\nTribe Origins Master Algorithms\nSymbolists Logic, philosophy Inverse deduction\nConnectionists Neuroscience Backpropagation\nEvolutionaries Evolutionary biology Genetic programming\nBayesians Statistics Probabilistic infernce\nAnalogizers Psychology Kernel machines\nIn this paper, we look at these five school of",
    "title": "The Tribes of Machine Learning and the Realm of Computer Architecture",
    "abstract": "Machine learning techniques have influenced the field of computer\narchitecture like many other fields. This paper studies how the fundamental\nmachine learning techniques can be applied towards computer architecture\nproblems. We also provide a detailed survey of computer architecture research\nthat employs different machine learning methods. Finally, we present some\nfuture opportunities and the outstanding challenges that need to be overcome to\nexploit full potential of machine learning for computer architecture.",
    "link": "http://arxiv.org/abs/2012.04105v1",
    "published": "2020-12-07T23:10:51Z"
  },
  "223": {
    "pdf_path": "data/pdfs/machine learning_paper_389.pdf",
    "text_excerpt": "Machin e Learnin g 3: 5-8, 198 8\n© 1988 Kluwe r Academi c Publishers , Bosto n - Manufacture d in The Netherland s\nEDITORIA L\nMachin e Learnin g as an Experimenta l Scienc e\nThe role of experiment s in machin e learnin g\nMachin e learnin g is a scientifi c disciplin e and, like the field s of AI and com -\nputer science , has both theoretica l and empirica l aspects . Althoug h recen t\nprogres s has occurre d on the theoretica l fron t (see Machine  Learning,  volum e\n2, numbe r 4), mos t learnin g algorithm s are too comple x for forma l analysis .\nThus , the field promise s to hav e a significan t empirica l componen t for the\nforeseeabl e future . An d unlik e some empirica l sciences , machin e learnin g is\nfortunat e enoug h to have experimenta l contro l over a wide rang e of factors ,\nmakin g it mor e akin to physic s and chemistr y than astronom y or sociology .\nIn any science , the goal of experimentatio n is to bette r understan d a class of\nbehavior s and the condition s unde r whic h they occur . Ideally , this will lead to\nempirica l laws that can aid the proces s of theor y formation . In our field , the\ncentra l behavio r is learning , and the condition s involv e the algorith m employed ,\nthe domai n knowledge , and the environmen t in whic h learnin g occurs . An\nimplemente d learnin g algorith m is necessar y but not sufficient ; one shoul d\nalso attemp t to specify  whe n it operate s well and the reason s for that behavior.\nLackin g theoretica l evidence , experimentatio n is the natura l alternative .\nAs normall y defined , an experiment  involve s systematicall y varyin g one or\nmore independent  variable s and examinin g their effec t on some dependent  vari -\nables . Thus , a machin e learnin g experimen t require s mor e than a singl e learn -\ning run; it require s a numbe r of runs carrie d out unde r differen t conditions . In\neach case , one mus t measur e som e aspec t of the system' s behavio r for compar -\nison acros s the differen t co",
    "title": "Machine Learning as an Experimental Science",
    "abstract": "",
    "link": "https://doi.org/10.1023/a:1022623814640",
    "published": "2003-04-04T16:55:36Z"
  },
  "224": {
    "pdf_path": "data/pdfs/machine learning_paper_22.pdf",
    "text_excerpt": "arXiv:1906.06821v2  [cs.LG]  23 Oct 20191\nA Survey of Optimization Methods from\na Machine Learning Perspective\nShiliang Sun, Zehui Cao, Han Zhu, and Jing Zhao\nAbstract —Machine learning develops rapidly, which has made\nmany theoretical breakthroughs and is widely applied in var ious\nﬁelds. Optimization, as an important part of machine learni ng,\nhas attracted much attention of researchers. With the expon ential\ngrowth of data amount and the increase of model complexity,\noptimization methods in machine learning face more and more\nchallenges. A lot of work on solving optimization problems o r\nimproving optimization methods in machine learning has bee n\nproposed successively. The systematic retrospect and summ ary\nof the optimization methods from the perspective of machine\nlearning are of great signiﬁcance, which can offer guidance\nfor both developments of optimization and machine learning\nresearch. In this paper, we ﬁrst describe the optimization\nproblems in machine learning. Then, we introduce the princi ples\nand progresses of commonly used optimization methods. Next ,\nwe summarize the applications and developments of optimiza tion\nmethods in some popular machine learning ﬁelds. Finally, we\nexplore and give some challenges and open problems for the\noptimization in machine learning.\nIndex Terms —Machine learning, optimization method, deep\nneural network, reinforcement learning, approximate Baye sian\ninference.\nI. I NTRODUCTION\nRECENTLY, machine learning has grown at a remarkable\nrate, attracting a great number of researchers and\npractitioners. It has become one of the most popular researc h\ndirections and plays a signiﬁcant role in many ﬁelds, such\nas machine translation, speech recognition, image recogni tion,\nrecommendation system, etc. Optimization is one of the core\ncomponents of machine learning. The essence of most machine\nlearning algorithms is to build an optimization model and le arn\nthe parameters in the objective function from the given data .\nIn the era of",
    "title": "A Survey of Optimization Methods from a Machine Learning Perspective",
    "abstract": "Machine learning develops rapidly, which has made many theoretical\nbreakthroughs and is widely applied in various fields. Optimization, as an\nimportant part of machine learning, has attracted much attention of\nresearchers. With the exponential growth of data amount and the increase of\nmodel complexity, optimization methods in machine learning face more and more\nchallenges. A lot of work on solving optimization problems or improving\noptimization methods in machine learning has been proposed successively. The\nsystematic retrospect and summary of the optimization methods from the\nperspective of machine learning are of great significance, which can offer\nguidance for both developments of optimization and machine learning research.\nIn this paper, we first describe the optimization problems in machine learning.\nThen, we introduce the principles and progresses of commonly used optimization\nmethods. Next, we summarize the applications and developments of optimization\nmethods in some popular machine learning fields. Finally, we explore and give\nsome challenges and open problems for the optimization in machine learning.",
    "link": "http://arxiv.org/abs/1906.06821v2",
    "published": "2019-06-17T02:54:51Z"
  }
}